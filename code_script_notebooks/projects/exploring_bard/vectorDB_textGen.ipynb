{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUJDi1ZCKoXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc701ef3-273d-4e3a-86e4-93b813a9b010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain lark chromadb pypdf google-cloud-aiplatform google-auth > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many PDFs and papers in below location, which will be used for embedding and further querying it.\n",
        "\n",
        "https://github.com/insightbuilder/python_de_learners_data/tree/main/resources"
      ],
      "metadata": {
        "id": "je0FdYFfOrry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the documents from langchain resources folder\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def load_pdf(path_pdf):\n",
        "  get_text = PyPDFLoader(path_pdf)\n",
        "  \n",
        "  get_pages = get_text.load()\n",
        "\n",
        "  final_text = []\n",
        "\n",
        "  shredder = RecursiveCharacterTextSplitter(chunk_size=350,\n",
        "                                            chunk_overlap=20,\n",
        "                                            length_function=len) \n",
        "  \n",
        "  final_shred = shredder.split_documents(get_pages)\n",
        "\n",
        "  return final_shred\n"
      ],
      "metadata": {
        "id": "5T1CYdRWLdSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=\"/content/generativeaitrial-trialLC.json\""
      ],
      "metadata": {
        "id": "WaoiXQ3CLdDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = VertexAIEmbeddings()"
      ],
      "metadata": {
        "id": "W3JflMRE-ZkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = '/content/content/lc_documentdb'"
      ],
      "metadata": {
        "id": "SsSIC2GVBikK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip lc_documentdb.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu3CBNzBBxGy",
        "outputId": "5d569f07-273f-4226-b99f-2536e76cb705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lc_documentdb.zip\n",
            "   creating: content/lc_documentdb/\n",
            "  inflating: content/lc_documentdb/chroma-embeddings.parquet  \n",
            "   creating: content/lc_documentdb/index/\n",
            "  inflating: content/lc_documentdb/index/uuid_to_id_b973adbd-91f7-4c95-b071-c5aabdbd5ab7.pkl  \n",
            "  inflating: content/lc_documentdb/index/id_to_uuid_b973adbd-91f7-4c95-b071-c5aabdbd5ab7.pkl  \n",
            "  inflating: content/lc_documentdb/index/index_b973adbd-91f7-4c95-b071-c5aabdbd5ab7.bin  \n",
            "  inflating: content/lc_documentdb/index/index_metadata_b973adbd-91f7-4c95-b071-c5aabdbd5ab7.pkl  \n",
            "  inflating: content/lc_documentdb/chroma-collections.parquet  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma(persist_directory=persist_directory, \n",
        "                  embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "hQ_vihLyOPhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_retriever = vectordb.as_retriever()\n",
        "db_retriever.get_relevant_documents(\"langchain concepts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-VZGuDOB6D3",
        "outputId": "62699517-5783-454a-d5de-8b3611220804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='5/31/23, 6:13 AM Concepts — \\x00\\x00 LangChain 0.0.186\\nhttps://python.langchain.com/en/stable/getting_started/concepts.html 1/3Concepts\\nContents\\nChain of Thought\\nAction Plan Generation\\nReAct\\nSelf-ask\\nPrompt Chaining\\nMemetic Proxy\\nSelf Consistency\\nInception\\nMemPrompt\\nThese are concepts and terminology commonly used when developing LLM applications. It', metadata={'source': '/content/Concepts.pdf', 'page': 0}),\n",
              " Document(page_content='This is the Python specific portion of the documentation. For a purely conceptual guide to\\nLangChain, see here. For the JavaScript documentation, see here.\\nGetting Started\\nHow to get started using LangChain to create an Language Model application.\\nQuickstart Guide\\nConcepts and terminology .\\nConcepts and terminology', metadata={'source': '/content/WelcometoLangChain.pdf', 'page': 0}),\n",
              " Document(page_content='contains reference to external papers or sources where the concept was first introduced, as\\nwell as to places in LangChain where the concept is used.\\nChain of Thought\\nChain of Thought (CoT)  is a prompting technique used to encourage the model to generate', metadata={'source': '/content/Concepts.pdf', 'page': 0}),\n",
              " Document(page_content='Chains: Combine LLMs and prompts in multi-\\nstep workflows\\nUp until now , we’ve worked with the PromptT emplate and LLM primitives by themselves. But of\\ncourse, a real application is not just one primitive, but rather a combination of them.\\nA chain in LangChain is made up of links, which can be either primitives like LLMs or other\\nchains.', metadata={'source': '/content/QuickstartGuide.pdf', 'page': 3})]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import VertexAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = VertexAI(temperature=0)\n",
        "\n",
        "\n",
        "prompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\n",
        "    Context: {context}\n",
        "    Topic: {topic}\n",
        "    Blog post:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \n",
        "                                               \"topic\"]\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=PROMPT)"
      ],
      "metadata": {
        "id": "qhGnYotMCvl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_blog_post(topic):\n",
        "    docs = vectordb.similarity_search(topic, k=4)\n",
        "    inputs = [{\"context\": doc.page_content, \n",
        "               \"topic\": topic} for doc in docs]\n",
        "    gen = chain.apply(inputs)\n",
        "    return gen"
      ],
      "metadata": {
        "id": "UjXWO8lbF3-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_blog_post(\"Langchain Concepts\")"
      ],
      "metadata": {
        "id": "6HkXINt4-ZhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7DHdGMkGW1Y",
        "outputId": "efa30dff-6292-4089-b04e-e52a6f08c960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Langchain Concepts\n",
            "\n",
            "Langchain is a large language model (LLM) that can be used for a variety of tasks, including text generation, summarization, and question answering. It is built on the Transformer architecture, which is a type of neural network that is designed to process sequential data.\n",
            "\n",
            "One of the key features of Langchain is its ability to generate text in a coherent and informative way. This is due to the model's large size and its ability to learn from a massive dataset of text. Langchain can also be used to generate text in different styles, such as creative writing, technical writing,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[2]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2zs-OTCGte6",
        "outputId": "4a59a9df-412f-49e8-c43f-4511019dedb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain of Thought (CoT) is a prompting technique used to encourage the model to generate text that is coherent and follows a logical progression. It is based on the idea that human language is generated by a series of steps, or thoughts, that build on each other to create a coherent narrative.\n",
            "\n",
            "The CoT technique works by providing the model with a series of prompts that guide it through the process of generating text. The first prompt is typically a topic or theme, and the subsequent prompts are used to provide additional information or context. The model is then allowed to generate text based on the prompts, and the output is typically more coherent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(output[1]['text'].split(' '))"
      ],
      "metadata": {
        "id": "Yt62QdmKGtal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cc3285-cadc-40ed-d888-ec4fbc48c51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stat_template = \"\"\"Extract the word frequencies\n",
        "from the context below and then write a 280 character \n",
        "youtube comment on the topic:\n",
        "    Context: {context}\n",
        "    Topic: {topic}\n",
        "    Blog post:\"\"\"\n",
        "\n",
        "yt_stat = PromptTemplate(\n",
        "    template=stat_template, input_variables=[\"context\", \n",
        "                                               \"topic\"]\n",
        ")\n",
        "\n",
        "chain_yt = LLMChain(llm=llm, \n",
        "                    prompt=yt_stat)"
      ],
      "metadata": {
        "id": "vvXe5v1IGtUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_yt(topic):\n",
        "    docs = vectordb.similarity_search(topic, k=4)\n",
        "    inputs = [{\"context\": doc.page_content, \n",
        "               \"topic\": topic} for doc in docs]\n",
        "    gen = chain_yt.apply(inputs)\n",
        "    return gen"
      ],
      "metadata": {
        "id": "eW7Tv4XiJW9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stat_out = generate_yt(\"Langchain Concepts\")"
      ],
      "metadata": {
        "id": "cjcvyQDJJW6i"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stat_out[3]['text'])"
      ],
      "metadata": {
        "id": "4TfZmKVHJW2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7a21a0-9084-42bb-c5fd-4c5d20544db0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent words in the context are:\n",
            "    1. LangChain\n",
            "    2. module\n",
            "    3. provide\n",
            "    4. interface\n",
            "    5. application\n",
            "\n",
            "Here is a 280 character youtube comment on the topic:\n",
            "\n",
            "LangChain is a new language model that is designed to be modular and extensible. This means that it can be used to build a variety of different applications, from chatbots to text generators. LangChain provides standard interfaces for each module, which makes it easy to use and extend.\n"
          ]
        }
      ]
    }
  ]
}