{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YSs1DqYZYknT"
      },
      "outputs": [],
      "source": [
        "!pip -q install sentence-transformers >> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2',\n",
        "                            cache_folder='all-minilm-l6')\n",
        "\n",
        "#Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.', \n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "#Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "KSANONHoZfZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_model = SentenceTransformer('/content/all-minilm-l6/sentence-transformers_all-MiniLM-L6-v2', \n",
        "                                  device='cpu')"
      ],
      "metadata": {
        "id": "PVgEQZU8ZfS5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common value for BERT & Co. are 512 word pieces, which corresponde to about 300-400 words (for English)."
      ],
      "metadata": {
        "id": "iw99dxFzbgW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "#Store sentences & embeddings on disc\n",
        "with open('embeddings.pkl', \"wb\") as fOut:\n",
        "    pickle.dump({'sentences': sentences, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#Load sentences & embeddings from disc\n",
        "with open('embeddings.pkl', \"rb\") as fIn:\n",
        "    stored_data = pickle.load(fIn)\n",
        "    stored_sentences = stored_data['sentences']\n",
        "    stored_embeddings = stored_data['embeddings']"
      ],
      "metadata": {
        "id": "udv_pVEYZfL2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging"
      ],
      "metadata": {
        "id": "2p1gAGr7f8yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "#Sentences we want sentence embeddings for\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "             'Sentences are passed as a list of string.',\n",
        "             'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "#Load AutoModel from huggingface model repository\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "H-49-SuqZfCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(save_directory='/content/save_pt_token')"
      ],
      "metadata": {
        "id": "dswDAHcpZe0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(save_directory='/content/save_pt_model')"
      ],
      "metadata": {
        "id": "jHP-p6oOg6R4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Similarity\n"
      ],
      "metadata": {
        "id": "SYp2dHfLhsc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome']\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, \n",
        "                           convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, \n",
        "                           convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, \n",
        "                             embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], \n",
        "                                                 sentences2[i], \n",
        "                                                 cosine_scores[i][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abUb40rNhsPH",
        "outputId": "e0e8ad68-6337-478a-dc90-426a606b43cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2838\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0327\n",
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(model)"
      ],
      "metadata": {
        "id": "dyg7HjQ6h968"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_sentence_embedding_dimension()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBLMOLy6leUq",
        "outputId": "d77f86dd-f400-476c-97fa-cd97b4339aaf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Single list of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings = model.encode(sentences, \n",
        "                          convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.cos_sim(embeddings, \n",
        "                             embeddings)"
      ],
      "metadata": {
        "id": "1igeuUtcly0b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    for j in range(i+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "#Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))\n",
        "    "
      ],
      "metadata": {
        "id": "rRIuOl0RmCDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "semantic search can also find synonyms\n",
        "\n",
        "For symmetric semantic search your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be “How to learn Python online?” and you want to find an entry like “How to learn Python on the web?”. For symmetric tasks, you could potentially flip the query and the entries in your corpus.\n",
        "\n",
        "For asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like “What is Python” and you wand to find the paragraph “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense."
      ],
      "metadata": {
        "id": "t8dMAFmFmTQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'A man is riding a horse.',\n",
        "          'A woman is playing violin.',\n",
        "          'Two men pushed carts through the woods.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'A cheetah is running behind its prey.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Query sentences:\n",
        "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\n",
        "\n",
        "query_embeddings = []\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = min(5, len(corpus))\n",
        "for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "    query_embeddings.append(query_embedding)\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n"
      ],
      "metadata": {
        "id": "ZNf9Qmx8mUWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = corpus_embeddings.to('cpu')\n",
        "\n",
        "corpus_embeddings = util.normalize_embeddings(corpus_embeddings)"
      ],
      "metadata": {
        "id": "sMoMuUbataQD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embeddings = query_embedding.to('cpu')\n",
        "\n",
        "#query_embeddings = util.normalize_embeddings(query_embeddings)\n",
        "\n",
        "hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score)"
      ],
      "metadata": {
        "id": "7b_1hOrotsl4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits"
      ],
      "metadata": {
        "id": "S3LzU58ZucL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In that case, Approximate Nearest Neighor (ANN) can be helpful. Here, the data is partitioned into smaller fractions of similar embeddings. This index can be searched efficiently and the embeddings with the highest similarity (the nearest neighbors) can be retrieved within milliseconds, even if you have millions of vectors.\n",
        "\n",
        "However, the results are not necessarily exact. It is possible that some vectors with high similarity will be missed. That’s the reason why it is called approximate nearest neighbor."
      ],
      "metadata": {
        "id": "avuLPqEAutuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Defining our FAISS index\n",
        "#Number of clusters used for faiss. Select a value 4*sqrt(N) to 16*sqrt(N) - https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n",
        "n_clusters = 1024\n",
        "\n",
        "#We use Inner Product (dot-product) as Index. We will normalize our vectors to unit length, then is Inner Product equal to cosine similarity\n",
        "quantizer = faiss.IndexFlatIP(embedding_size)\n",
        "index = faiss.IndexIVFFlat(quantizer, embedding_size, n_clusters, faiss.METRIC_INNER_PRODUCT)\n",
        "\n"
      ],
      "metadata": {
        "id": "2UWuJk-rvkYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar Questions Retrieval\n",
        "\n",
        "Similar Publication Retrieval\n",
        "\n",
        "Question & Answer Retrieval"
      ],
      "metadata": {
        "id": "yc9czTwRwSZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single list of sentences - Possible tens of thousands of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']\n",
        "\n",
        "paraphrases = util.paraphrase_mining(model, sentences)\n",
        "\n",
        "for paraphrase in paraphrases[0:10]:\n",
        "    score, i, j = paraphrase\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkRJpAMAuvVZ",
        "outputId": "5bcb612e-9f87-441e-c490-85333133a610"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n",
            "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6788\n",
            "I love pasta \t\t Do you like pizza? \t\t Score: 0.5096\n",
            "I love pasta \t\t The new movie is so great \t\t Score: 0.2560\n",
            "I love pasta \t\t The new movie is awesome \t\t Score: 0.2440\n",
            "A man is playing guitar \t\t The cat plays in the garden \t\t Score: 0.2105\n",
            "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1969\n",
            "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1692\n",
            "The cat sits outside \t\t A woman watches TV \t\t Score: 0.1310\n",
            "The cat plays in the garden \t\t Do you like pizza? \t\t Score: 0.0900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bi-Encoder vs. Cross-Encoder\n",
        "First, it is important to understand the difference between Bi- and Cross-Encoder.\n",
        "\n",
        "Bi-Encoders produce for a given sentence a sentence embedding. We pass to a BERT independently the sentences A and B, which result in the sentence embeddings u and v. These sentence embedding can then be compared using cosine similarity\n",
        "\n",
        "In contrast, for a Cross-Encoder, we pass both sentences simultaneously to the Transformer network. It produces than an output value between 0 and 1 indicating the similarity of the input sentence pair:\n",
        "\n",
        "A Cross-Encoder does not produce a sentence embedding. Also, we are not able to pass individual sentences to a Cross-Encoder."
      ],
      "metadata": {
        "id": "wHOrcq_z0CC4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OjgDitcuvRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXPCqvHmuvNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f4JTfZSfuvHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}