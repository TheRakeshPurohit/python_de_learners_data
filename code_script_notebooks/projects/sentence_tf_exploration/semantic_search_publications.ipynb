{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3eb39cbf7e78464c84acd214e262c5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6101c28759084ab6a04d871f2e2b4b15",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6566463facd34641826a6d398abed692",
              "IPY_MODEL_599697f6c66e4fef9ab669b70a5f8c30",
              "IPY_MODEL_55f4f6176948492897f6097f55fa8201"
            ]
          }
        },
        "6101c28759084ab6a04d871f2e2b4b15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6566463facd34641826a6d398abed692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3af7249d9da040e4a27741d6cf7379d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_249ff95112f545b28c2c2bc3d484c5d1"
          }
        },
        "599697f6c66e4fef9ab669b70a5f8c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b4a088b634894f7d8a85327c1b5f08c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1104641,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1104641,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00eeab13be3e4b1f9558d49b10318a23"
          }
        },
        "55f4f6176948492897f6097f55fa8201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b73eee53ed50438f8dab8cd443945d7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.10M/1.10M [00:01&lt;00:00, 1.03MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0313ac2f5e554516a45490fd95a3fcd1"
          }
        },
        "3af7249d9da040e4a27741d6cf7379d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "249ff95112f545b28c2c2bc3d484c5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4a088b634894f7d8a85327c1b5f08c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00eeab13be3e4b1f9558d49b10318a23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b73eee53ed50438f8dab8cd443945d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0313ac2f5e554516a45490fd95a3fcd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PmytdgGNU5w"
      },
      "source": [
        "# Semantic Search in Publications\n",
        "\n",
        "This notebook demonstrates how [sentence-transformers](https://www.sbert.net) and the [SPECTER](https://github.com/allenai/specter) model can be used to find similar publications.\n",
        "\n",
        "As corpus, we use all EMNLP publications from 2016 - 2018.\n",
        "\n",
        "We then search for similar papers using papers that have been presented at EMNLP 2019 / 2020.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVftsxZXEgsH"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "3eb39cbf7e78464c84acd214e262c5c0",
            "6101c28759084ab6a04d871f2e2b4b15",
            "6566463facd34641826a6d398abed692",
            "599697f6c66e4fef9ab669b70a5f8c30",
            "55f4f6176948492897f6097f55fa8201",
            "3af7249d9da040e4a27741d6cf7379d0",
            "249ff95112f545b28c2c2bc3d484c5d1",
            "b4a088b634894f7d8a85327c1b5f08c2",
            "00eeab13be3e4b1f9558d49b10318a23",
            "b73eee53ed50438f8dab8cd443945d7e",
            "0313ac2f5e554516a45490fd95a3fcd1"
          ]
        },
        "id": "W8o7tQg4G2rg",
        "outputId": "da74f7ae-fbf5-4b03-949c-88e38259b2a5"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "#First, we load the papers dataset (with title and abstract information)\n",
        "dataset_file = 'emnlp2016-2018.json'\n",
        "\n",
        "if not os.path.exists(dataset_file):\n",
        "  util.http_get(\"https://sbert.net/datasets/emnlp2016-2018.json\", dataset_file)\n",
        "\n",
        "with open(dataset_file) as fIn:\n",
        "  papers = json.load(fIn)\n",
        "\n",
        "print(len(papers), \"papers loaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eb39cbf7e78464c84acd214e262c5c0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/1.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "974 papers loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14qsqKZ2HfVy"
      },
      "source": [
        "#We then load the allenai-specter model with SentenceTransformers\n",
        "model = SentenceTransformer('allenai-specter')\n",
        "\n",
        "#To encode the papers, we must combine the title and the abstracts to a single string\n",
        "paper_texts = [paper['title'] + '[SEP]' + paper['abstract'] for paper in papers]\n",
        "\n",
        "#Compute embeddings for all papers\n",
        "corpus_embeddings = model.encode(paper_texts, convert_to_tensor=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bbfpKMwJG9Q"
      },
      "source": [
        "#We define a function, given title & abstract, searches our corpus for relevant (similar) papers\n",
        "def search_papers(title, abstract):\n",
        "  query_embedding = model.encode(title+'[SEP]'+abstract, convert_to_tensor=True)\n",
        "\n",
        "  search_hits = util.semantic_search(query_embedding, corpus_embeddings)\n",
        "  search_hits = search_hits[0]  #Get the hits for the first query\n",
        "\n",
        "  print(\"Paper:\", title)\n",
        "  print(\"Most similar papers:\")\n",
        "  for hit in search_hits:\n",
        "    related_paper = papers[hit['corpus_id']]\n",
        "    print(\"{:.2f}\\t{}\\t{} {}\".format(hit['score'], related_paper['title'], related_paper['venue'], related_paper['year']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3tGTzylOMqp"
      },
      "source": [
        "## Search\n",
        "Now we search for some papers that have been presented at EMNLP 2019 and 2020."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzByWa1AH3J8",
        "outputId": "cba51ffe-1ad6-4c30-c032-263c9987607e"
      },
      "source": [
        "# This paper was the EMNLP 2019 Best Paper\n",
        "search_papers(title='Specializing Word Embeddings (for Parsing) by Information Bottleneck', \n",
        "              abstract='Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper: Specializing Word Embeddings (for Parsing) by Information Bottleneck\n",
            "Most similar papers:\n",
            "0.88\tAn Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing\tEMNLP 2018\n",
            "0.87\tNORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings\tEMNLP 2018\n",
            "0.87\tGeneralizing Word Embeddings using Bag of Subwords\tEMNLP 2018\n",
            "0.87\tWord Embeddings for Code-Mixed Language Processing\tEMNLP 2018\n",
            "0.87\tLAMB: A Good Shepherd of Morphologically Rich Languages\tEMNLP 2016\n",
            "0.87\tWord Mover's Embedding: From Word2Vec to Document Embedding\tEMNLP 2018\n",
            "0.87\tCharagram: Embedding Words and Sentences via Character n-grams\tEMNLP 2016\n",
            "0.87\tSegmentation-Free Word Embedding for Unsegmented Languages\tEMNLP 2017\n",
            "0.86\tAddressing Troublesome Words in Neural Machine Translation\tEMNLP 2018\n",
            "0.86\tConditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop\tEMNLP 2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
            "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRpOAhhNI58L",
        "outputId": "f0e0d45d-6e46-4d7c-9d9b-9b23183a0ca3"
      },
      "source": [
        "# This paper was the EMNLP 2020 Best Paper\n",
        "search_papers(title='Digital Voicing of Silent Speech',\n",
        "              abstract='In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
            "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper: Digital Voicing of Silent Speech\n",
            "Most similar papers:\n",
            "0.82\tSession-level Language Modeling for Conversational Speech\tEMNLP 2018\n",
            "0.79\tNeural Multitask Learning for Simile Recognition\tEMNLP 2018\n",
            "0.78\tSpeech segmentation with a neural encoder model of working memory\tEMNLP 2017\n",
            "0.77\tMSMO: Multimodal Summarization with Multimodal Output\tEMNLP 2018\n",
            "0.77\tEstimating Marginal Probabilities of n-grams for Recurrent Neural Language Models\tEMNLP 2018\n",
            "0.76\tA Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness\tEMNLP 2018\n",
            "0.76\tLearning Unsupervised Word Translations Without Adversaries\tEMNLP 2018\n",
            "0.75\tLarge Margin Neural Language Model\tEMNLP 2018\n",
            "0.75\tPhrase-Based & Neural Unsupervised Machine Translation\tEMNLP 2018\n",
            "0.75\tMultimodal Language Analysis with Recurrent Multistage Fusion\tEMNLP 2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4idfE76lJvXZ",
        "outputId": "8e6c0672-0079-4e25-856f-ec71431dd096"
      },
      "source": [
        "# This paper was a EMNLP 2020 Honourable Mention Papers\n",
        "search_papers(title='If beam search is the answer, what was the question?',\n",
        "              abstract='Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper: If beam search is the answer, what was the question?\n",
            "Most similar papers:\n",
            "0.91\tA Stable and Effective Learning Strategy for Trainable Greedy Decoding\tEMNLP 2018\n",
            "0.90\tBreaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation\tEMNLP 2018\n",
            "0.90\tWhy Neural Translations are the Right Length\tEMNLP 2016\n",
            "0.88\tLearning Neural Templates for Text Generation\tEMNLP 2018\n",
            "0.87\tTowards Decoding as Continuous Optimisation in Neural Machine Translation\tEMNLP 2017\n",
            "0.86\tA Tree-based Decoder for Neural Machine Translation\tEMNLP 2018\n",
            "0.86\tMemory-enhanced Decoder for Neural Machine Translation\tEMNLP 2016\n",
            "0.86\tTrainable Greedy Decoding for Neural Machine Translation\tEMNLP 2017\n",
            "0.86\tMulti-Reference Training with Pseudo-References for Neural Translation and Text Generation\tEMNLP 2018\n",
            "0.86\tAddressing Troublesome Words in Neural Machine Translation\tEMNLP 2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
            "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IwC7wSqKE7_",
        "outputId": "84daa3f5-bb8e-448e-cee0-1b34d255839e"
      },
      "source": [
        "# This paper was a EMNLP 2020 Honourable Mention Papers\n",
        "search_papers(title='Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems',\n",
        "              abstract='The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot’s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work. The framework is released asa ready-to-use tool.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
            "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper: Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems\n",
            "Most similar papers:\n",
            "0.86\tMulti-view Response Selection for Human-Computer Conversation\tEMNLP 2016\n",
            "0.84\tPatterns of Argumentation Strategies across Topics\tEMNLP 2017\n",
            "0.84\tNatural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog\tEMNLP 2017\n",
            "0.83\tTowards Exploiting Background Knowledge for Building Conversation Systems\tEMNLP 2018\n",
            "0.83\tAirDialogue: An Environment for Goal-Oriented Dialogue Research\tEMNLP 2018\n",
            "0.82\tWikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community\tEMNLP 2018\n",
            "0.82\tSpider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task\tEMNLP 2018\n",
            "0.82\tThe Teams Corpus and Entrainment in Multi-Party Spoken Dialogues\tEMNLP 2016\n",
            "0.81\tDeal or No Deal? End-to-End Learning of Negotiation Dialogues\tEMNLP 2017\n",
            "0.81\tMultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling\tEMNLP 2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePnbJYKSO4gE",
        "outputId": "db6fa765-fc40-4baf-b62e-8fec59207c18"
      },
      "source": [
        "# EMNLP 2020 paper on making Sentence-BERT multilingual\n",
        "search_papers(title='Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation',\n",
        "              abstract='We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
            "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper: Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\n",
            "Most similar papers:\n",
            "0.90\tSentence Compression for Arbitrary Languages via Multilingual Pivoting\tEMNLP 2018\n",
            "0.90\tLearning Crosslingual Word Embeddings without Bilingual Corpora\tEMNLP 2016\n",
            "0.89\tUnsupervised Multilingual Word Embeddings\tEMNLP 2018\n",
            "0.89\tInferLite: Simple Universal Sentence Representations from Natural Language Inference Data\tEMNLP 2018\n",
            "0.88\tImproving Cross-Lingual Word Embeddings by Meeting in the Middle\tEMNLP 2018\n",
            "0.88\tDynamic Meta-Embeddings for Improved Sentence Representations\tEMNLP 2018\n",
            "0.88\tPorting an Open Information Extraction System from English to German\tEMNLP 2016\n",
            "0.88\tUnsupervised Statistical Machine Translation\tEMNLP 2018\n",
            "0.87\tContextual Parameter Generation for Universal Neural Machine Translation\tEMNLP 2018\n",
            "0.87\tAdapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations\tEMNLP 2018\n"
          ]
        }
      ]
    }
  ]
}