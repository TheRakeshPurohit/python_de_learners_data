{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lakshmanok/lakblogs/tree/main/bridge_bidding_advisor\n",
    "# in order to execute the scripts, the supporting indexes \n",
    "# are required which in turn requires installation of \n",
    "# torch and transformers. Chromadb install will lead to pulling \n",
    "# all the supporting libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the vector index\n",
    "import chromadb\n",
    "import bs4\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.bridgeworld.com/pages/readingroom/bws/bwscompletesystem.html\"\n",
    "URL_LOCAL = URL.split('/')[-1]\n",
    "CHROMA_COLLECTION_NAME = \"bridge_world_system\"\n",
    "CHROMADB_DIR = \"db/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str) -> str:\n",
    "    local_filename = URL_LOCAL\n",
    "    if not os.path.exists(local_filename):\n",
    "        print(f\"Downloading {URL} to {local_filename}.\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "    else:\n",
    "        print(f\"Using already downloaded {local_filename}.\")\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dspyenv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=CHROMADB_DIR)\n",
    "collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)\n",
    "text_splitter = SentenceTransformersTokenTextSplitter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already downloaded bwscompletesystem.html.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bwscompletesystem.html'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update paragraphs into chromadb collection\n",
    "download_file(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "# initiate building the index\n",
    "with open(URL_LOCAL, 'r') as f:\n",
    "    soup = bs4.BeautifulSoup(f.read(), 'html.parser')\n",
    "    last_header = \"\"\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% (1) \n",
      "1% (53) 1% (105) 2% (156) 2% (207) 3% (258) 3% (309) 4% (359) 4% (409) 5% (459) 5% (509) \n",
      "6% (559) 6% (609) 7% (657) 7% (705) 8% (753) 8% (801) 9% (848) 9% (895) 10% (942) 10% (989) \n",
      "11% (1035) 11% (1081) 12% (1127) 12% (1173) 13% (1218) 13% (1263) 14% (1308) 14% (1353) 15% (1398) 15% (1443) \n",
      "16% (1488) 16% (1532) 17% (1576) 18% (1620) 18% (1664) 19% (1707) 19% (1750) 20% (1793) 20% (1835) 21% (1877) \n",
      "21% (1918) 22% (1959) 22% (2000) 23% (2040) 23% (2080) 24% (2120) 24% (2160) 25% (2200) 25% (2240) 26% (2280) \n",
      "26% (2320) 27% (2360) 27% (2399) 28% (2438) 28% (2477) 29% (2516) 29% (2555) 30% (2594) 30% (2633) 31% (2671) \n",
      "31% (2709) 32% (2747) 32% (2785) 33% (2823) 34% (2860) 34% (2897) 35% (2934) 35% (2971) 36% (3007) 36% (3043) \n",
      "37% (3078) 37% (3111) 38% (3144) 38% (3176) 39% (3207) 39% (3237) 40% (3267) 40% (3297) 41% (3326) 41% (3353) \n",
      "42% (3380) 42% (3407) 43% (3433) 43% (3459) 44% (3485) 44% (3510) 45% (3535) 45% (3560) 46% (3585) 46% (3609) \n",
      "47% (3633) 47% (3657) 48% (3681) 48% (3704) 49% (3727) 49% (3750) "
     ]
    }
   ],
   "source": [
    "# initiate building the index\n",
    "with open(URL_LOCAL, 'r') as f:\n",
    "    soup = bs4.BeautifulSoup(f.read(), 'html.parser')\n",
    "    last_header = \"\"\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    for n, paragraph in enumerate(paragraphs[:97]):\n",
    "        paragraph_id = f\"{URL_LOCAL}_{n}\"\n",
    "        text = paragraph.text.strip()\n",
    "        # find the previous header\n",
    "        header = paragraph.find_all(re.compile(\"^h[1-5]$\"))\n",
    "        if header:\n",
    "            header = header[0].text.strip()\n",
    "            last_header = header\n",
    "        else:\n",
    "            header = last_header\n",
    "        # print(paragraph_id, \"->\", header, \"->\", len(text), \"->\", text[:30])\n",
    "        # split the text into chunks and insert into chromadb\n",
    "        ids = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        chunks = text_splitter.create_documents([text]) # takes array of documents\n",
    "        for chunk_no, chunk in enumerate(chunks):\n",
    "            ids.append(f\"{paragraph_id}#{chunk_no}\")\n",
    "            documents.append(chunk.page_content)\n",
    "            metadatas.append({\"title\": header, \"source\": URL})\n",
    "        if ids:\n",
    "            collection.upsert(ids=ids, documents=documents, metadatas=metadatas)\n",
    "        \n",
    "        print(f\"{int(0.5 + 100.0 * n / len(paragraphs))}% ({collection.count()})\", end=\" \", flush=True)\n",
    "        \n",
    "        if n % 10 == 0:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import dspy\n",
    "\n",
    "def init_gemini_pro(temperature: float = 0.0):\n",
    "    \"\"\"\n",
    "    Initializes dspy to use Gemini as the language model.\n",
    "    \"\"\"\n",
    "    dotenv.load_dotenv(\"D:\\\\gitFolders\\\\python_de_learners_data\\\\.env\")\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    gemini = dspy.Google(\"models/gemini-1.0-pro\",\n",
    "                         api_key=api_key,\n",
    "                         temperature=temperature)\n",
    "    dspy.settings.configure(lm=gemini, max_tokens=1024)\n",
    "\n",
    "\n",
    "def init_gpt35(temperature: float = 0.0):\n",
    "    \"\"\"\n",
    "    Initializes dspy to use OpenAI GPT 3.5 as the language model.\n",
    "    \"\"\"\n",
    "    dotenv.load_dotenv(\"D:\\\\gitFolders\\\\python_de_learners_data\\\\.env\")\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    gpt35 = dspy.OpenAI(model=\"gpt-3.5-turbo\",\n",
    "                        api_key=api_key,\n",
    "                        temperature=temperature)\n",
    "    dspy.settings.configure(lm=gpt35, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import teleprompt\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShot(dspy.Module):\n",
    "    \"\"\"\n",
    "    Provide answer to question\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(\"question -> answer\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.prog(question=\"In the game of bridge, \" + question)\n",
    "\n",
    "\n",
    "class Definitions(dspy.Module):\n",
    "    \"\"\"\n",
    "    Retrieve the definition from Wikipedia (2017 version)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.retriever = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "    def forward(self, term):\n",
    "        result = self.retriever(f\"In the game of bridge, what does {term} mean?\", k=1)\n",
    "        if result:\n",
    "            return result[0].long_text\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class FindTerms(dspy.Module):\n",
    "    \"\"\"\n",
    "    Extract bridge terms from a question\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.entity_extractor = dspy.Predict(\"question -> terms\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        max_num_terms = max(1, len(question.split())//4)\n",
    "        prompt = f\"Identify up to {max_num_terms} terms in the following question that are jargon in the card game bridge.\"\n",
    "        prediction = self.entity_extractor(\n",
    "            question=f\"{prompt}\\n{question}\"\n",
    "        )\n",
    "        answer = prediction.terms\n",
    "        if \"Terms: \" in answer:\n",
    "            start = answer.rindex(\"Terms: \") + len(\"Terms: \")\n",
    "            answer = answer[start:]\n",
    "        return [a.strip() for a in answer.split(',')]\n",
    "\n",
    "\n",
    "def BiddingSystem():\n",
    "    \"\"\"\n",
    "    Retreives rules for bidding in bridge.\n",
    "    This is just a retriever and does not have any language model.\n",
    "    \"\"\"\n",
    "    from chromadb.utils import embedding_functions\n",
    "    default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "    return ChromadbRM(CHROMA_COLLECTION_NAME, CHROMADB_DIR, default_ef, k=3)\n",
    "\n",
    "\n",
    "class AdvisorSignature(dspy.Signature):\n",
    "    definitions = dspy.InputField(format=str)  # function to call on input to make it a string\n",
    "    bidding_system = dspy.InputField(format=str) # function to call on input to make it a string\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "def shorten_list(response):\n",
    "    if type(response) == list:\n",
    "        return [ f\"{r['long_text'][:25]} ... {len(r['long_text'])}\" for r in response]\n",
    "    else:\n",
    "        return response   \n",
    "    \n",
    "\n",
    "class BridgeBiddingAdvisor(dspy.Module):\n",
    "    \"\"\"\n",
    "    Functions as the orchestrator. All questions are sent to this module.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.find_terms = FindTerms()\n",
    "        self.definitions = Definitions()\n",
    "        # self.bidding_system = BiddingSystem()\n",
    "        self.prog = dspy.ChainOfThought(AdvisorSignature,\n",
    "                                        n=3)\n",
    "\n",
    "    def forward(self, question):\n",
    "        print(\"a:\", question)\n",
    "        terms = self.find_terms(question)\n",
    "        print(\"b:\", terms)\n",
    "        definitions = [self.definitions(term) for term in terms]\n",
    "        print(\"c:\", definitions)\n",
    "        bidding_system = BiddingSystem()(question)\n",
    "        print(\"d:\", shorten_list(bidding_system))\n",
    "        prediction = self.prog(definitions=definitions,\n",
    "                               bidding_system=bidding_system,\n",
    "                               question=\"In the game of bridge, \" + question,\n",
    "                               max_tokens=-1024)\n",
    "        return prediction.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(name: str, module: dspy.Module, queries: [str], shorten: bool = False):\n",
    "    print(f\"**{name}**\")\n",
    "    for query in queries:\n",
    "        response = module(query)\n",
    "        if shorten:\n",
    "            response = shorten_list(response)\n",
    "        print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "        \"What is Stayman?\",\n",
    "        \"When do you use Jacoby Transfers?\",\n",
    "        \"Playing Stayman and Transfers, what do you bid with 5-4 in the majors?\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"Zeroshot\", ZeroShot(), questions)\n",
    "# exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"definitions\", Definitions(), [\"Stayman\", \"Jacoby Transfers\", \"Strong 1NT\", \"majors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"find_terms\", FindTerms(), questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"bidding_system\", BiddingSystem(), questions, shorten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"bidding_advisor\", BridgeBiddingAdvisor(), questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labeled training dataset\n",
    "traindata = json.load(open(\"trainingdata.json\", \"r\"))['examples']\n",
    "trainset = [dspy.Example(question=e['question'], answer=e['answer']) for e in traindata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "teleprompter = teleprompt.LabeledFewShot()\n",
    "optimized_advisor = teleprompter.compile(student=BridgeBiddingAdvisor(),\n",
    "                                         trainset=trainset)\n",
    "run(\"optimized\", optimized_advisor, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
