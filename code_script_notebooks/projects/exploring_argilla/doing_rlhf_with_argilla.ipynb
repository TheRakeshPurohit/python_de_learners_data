{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg \n",
    "\n",
    "rg.init(api_key=\"\",\n",
    "        api_url=\"https://.hf.space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.set_workspace(\"argilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataset as a framework\n",
    "\n",
    "dataset_fw = rg.FeedbackDataset(\n",
    "    guidelines=\"Please read the prompt carefully\",\n",
    "    questions=[\n",
    "        rg.TextQuestion(\n",
    "            name=\"prompt\",\n",
    "            title=\"Please write a harmless reply\",\n",
    "            required=True,\n",
    "        )\n",
    "    ],\n",
    "    fields=[\n",
    "        rg.TextField(name=\"prompt\", required=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are following ways to collect the datasets \n",
    "\n",
    "# The steps here can include: \n",
    "# (1) finding an open dataset that might contain prompts related to your use \n",
    "# case\n",
    "# (2) performing** exploratory data** analysis and topic extraction** to understand\n",
    "#  the data\n",
    "# (3) filtering and selecting prompts based on topic, quality,\n",
    "# text descriptiveness, etc.\n",
    "# (4) Asking humans to write prompts for your usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be populated from the list of writing topics you create\n",
    "fields = [\n",
    "    rg.TextField(name=\"writing-topic\", required=True)\n",
    "]\n",
    "\n",
    "# we will ask the labeler to write a possible prompt or instruction\n",
    "question = rg.TextQuestion(\n",
    "\tname=\"prompt\",\n",
    "\ttitle=\"Imagine and write a possible instruction for the given topic:\",\n",
    "\trequired=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prompts = load_dataset(\"HuggingFaceH4/mt_bench_prompts\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    rg.FeedbackRecord(fields={\"prompt\": rek['prompt'][0]}) for rek in prompts\n",
    "]\n",
    "records\n",
    "dataset_fw.add_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This publishes the dataset with its records to Argilla and returns the dataset in Argilla\n",
    "remote_dataset = dataset_fw.push_to_argilla(name=\"rlhf_demo\", workspace=\"argilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we distribute the workload in one dataset with several labelers\n",
    "feedback_five = rg.FeedbackDataset.from_argilla(\n",
    "\tname=\"rlhf_demo\",\n",
    "\tworkspace=\"argilla\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_five.filter_by(response_status=\"submitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the datasets to rank the responses\n",
    "\n",
    "questions = [\n",
    "    rg.RankingQuestion(\n",
    "        name=\"response_ranking\",\n",
    "        title=\"order the responses based on their accuracy & helpfulness\",\n",
    "        required=True,\n",
    "        values={\"res1\":\"Nice\", \"res2\": \"Okay\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [\n",
    "    rg.RatingQuestion(\n",
    "        name=\"rate_resp\",\n",
    "        title=\"Select accurate response between (2) and (3). If same then select (1).\",\n",
    "        required=True,\n",
    "        values=[1, 2, 3]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_collect_ds = rg.FeedbackDataset(\n",
    "    guidelines=\"Please read prompt, its response below and provide feedback\",\n",
    "    questions=question,\n",
    "    fields=[\n",
    "        rg.TextField(name=\"prompt1\", required=True),\n",
    "        rg.TextField(name=\"response1\", required=True),\n",
    "        rg.TextField(name=\"response2\", required=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma:2b\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma:2b\")\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "records = []\n",
    "for record in prompts:\n",
    "    prompt = record[\"prompt\"]\n",
    "\n",
    "    # Generate two responses in one call\n",
    "    outputs = gen_pipeline(\n",
    "        prompt,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=2,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    responses = [output[\"generated_text\"] for output in outputs]\n",
    "\n",
    "    record = rg.FeedbackRecord(fields={\"prompt\": prompt, \"response 1\": responses[0], \"response 2\": responses[1]})\n",
    "    records.append(record)\n",
    "\n",
    "# Add records to the dataset\n",
    "response_collect_ds.add_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rem_ds = response_collect_ds.push_to_argilla(name=\"response_collect\", workspace=\"argilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_ds = rg.FeedbackDataset.from_argilla(\n",
    "        name=\"response_collect\",\n",
    "        workspace=\"argilla\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list to store the triplets\n",
    "triplets = []\n",
    "\n",
    "# Loop over all records in the dataset\n",
    "for record in feedback_ds.records:\n",
    "    # Ensure that the record has responses\n",
    "    if record.responses is None or len(record.responses) == 0:\n",
    "        continue\n",
    "\n",
    "    # Ensure the response has been submitted (not discarded)\n",
    "    response = record.responses[0]\n",
    "\n",
    "    if response.status == 'submitted':\n",
    "        # Get the ranking value from the response for the preferred and least preferred\n",
    "        # responses, assuming there are no ties\n",
    "        preferred_rank = response.values[\"response_ranking\"].value[0][\"value\"]\n",
    "        least_preferred_rank = response.values[\"response_ranking\"].value[1][\"value\"]\n",
    "\n",
    "        # Construct the triplet and append to the list\n",
    "        triplets.append({\n",
    "            \"prompt\": record.fields[\"prompt\"],\n",
    "            \"preferred_response\": record.fields[preferred_rank],\n",
    "            \"least_preferred_response\": record.fields[least_preferred_rank],\n",
    "        })\n",
    "\n",
    "# Now, \"triplets\" is a list of dictionaries, each containing a prompt and the associated\n",
    "# preferred and less preferred responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
