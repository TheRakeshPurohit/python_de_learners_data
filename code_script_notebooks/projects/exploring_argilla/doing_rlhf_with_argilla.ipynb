{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg \n",
    "localuser = \"argilla\"\n",
    "password = \"1234\"\n",
    "apikey = \"owner.apikey\"\n",
    "url = \"https://kamaljp-argillatest.hf.space/\"\n",
    "\n",
    "rg.init(api_key=apikey,\n",
    "        api_url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localgilla = \"hfgilla\"\n",
    "rg.set_workspace(localgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataset as a framework\n",
    "\n",
    "dataset_fw = rg.FeedbackDataset(\n",
    "    guidelines=\"Please read the prompt carefully\",\n",
    "    questions=[\n",
    "        rg.TextQuestion(\n",
    "            name=\"prompt\",\n",
    "            title=\"Please write a harmless reply\",\n",
    "            required=True,\n",
    "        )\n",
    "    ],\n",
    "    fields=[\n",
    "        rg.TextField(name=\"prompt\", required=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are following ways to collect the datasets \n",
    "\n",
    "# The steps here can include: \n",
    "# (1) finding an open dataset that might contain prompts related to your use \n",
    "# case\n",
    "# (2) performing** exploratory data** analysis and topic extraction** to understand\n",
    "#  the data\n",
    "# (3) filtering and selecting prompts based on topic, quality,\n",
    "# text descriptiveness, etc.\n",
    "# (4) Asking humans to write prompts for your usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be populated from the list of writing topics you create\n",
    "fields = [\n",
    "    rg.TextField(name=\"writing-topic\", required=True)\n",
    "]\n",
    "\n",
    "# we will ask the labeler to write a possible prompt or instruction\n",
    "question = rg.TextQuestion(\n",
    "\tname=\"prompt\",\n",
    "\ttitle=\"Imagine and write a possible instruction for the given topic:\",\n",
    "\trequired=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prompts = load_dataset(\"HuggingFaceH4/mt_bench_prompts\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    rg.FeedbackRecord(fields={\"prompt\": rek['prompt'][0]}) for rek in prompts\n",
    "]\n",
    "dataset_fw.add_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This publishes the dataset with its records to Argilla and returns the dataset in Argilla\n",
    "remote_dataset = dataset_fw.push_to_argilla(name=\"rlhf_demo\", workspace=localgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we distribute the workload in one dataset with several labelers\n",
    "feedback_five = rg.FeedbackDataset.from_argilla(\n",
    "\tname=\"rlhf_demo\",\n",
    "\tworkspace=localgilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_five.filter_by(response_status=\"submitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the datasets to rank the responses\n",
    "\n",
    "questions = [\n",
    "    rg.RankingQuestion(\n",
    "        name=\"response_ranking\",\n",
    "        title=\"order the responses based on their accuracy & helpfulness\",\n",
    "        required=True,\n",
    "        values={\"res1\":\"Nice\", \"res2\": \"Okay\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [\n",
    "    rg.RatingQuestion(\n",
    "        name=\"rate_resp\",\n",
    "        title=\"Select accurate response between (2) and (3). If same then select (1).\",\n",
    "        required=True,\n",
    "        values=[1, 2, 3]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_collect_ds = rg.FeedbackDataset(\n",
    "    guidelines=\"Please read prompt, its response below and provide feedback\",\n",
    "    questions=question,\n",
    "    fields=[\n",
    "        rg.TextField(name=\"prompt1\", required=True),\n",
    "        rg.TextField(name=\"response1\", required=True),\n",
    "        rg.TextField(name=\"response2\", required=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_collect_ds.push_to_argilla(name=\"response_collect\", workspace=localgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                             resume_download=True,\n",
    "                                            device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device='cuda',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in prompts:\n",
    "    print(x['prompt'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for record in prompts:\n",
    "    prompt = record[\"prompt\"][0]\n",
    "    # print(f'This is prompt: {prompt}')\n",
    "    # Generate two responses in one call\n",
    "    outputs = gen_pipeline(\n",
    "        prompt,\n",
    "        max_length=512,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    # print(outputs)\n",
    "    responses = [output[0][\"generated_text\"] for output in outputs]\n",
    "    # print(f\"This is response: {responses}\")\n",
    "    try:\n",
    "        record = rg.FeedbackRecord(fields={\"prompt1\": prompt[0],\n",
    "                                           \"response1\": responses[0],\n",
    "                                           \"response2\": responses[1]})\n",
    "        response_collect_ds.add_records([record])\n",
    "    except Exception as e:\n",
    "        print(f\"The prompt {prompt} created error due to : {e}\")\n",
    "\n",
    "# Add records to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rem_ds = response_collect_ds.push_to_argilla(name=\"response_collect\",\n",
    "                                                workspace=localgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_ds = rg.FeedbackDataset.from_argilla(\n",
    "        name=\"response_collect\",\n",
    "        workspace=localgilla\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define an empty list to store the triplets\n",
    "triplets = []\n",
    "\n",
    "# Loop over all records in the dataset\n",
    "for record in feedback_ds.records:\n",
    "    print(record.fields)\n",
    "    # Ensure that the record has responses\n",
    "    if record.responses is None or len(record.responses) == 0:\n",
    "        continue\n",
    "\n",
    "    # Ensure the response has been submitted (not discarded)\n",
    "    # print(len(record.responses))\n",
    "    response = record.responses[0]\n",
    "    print(response)\n",
    "    if response.status == 'submitted':\n",
    "        print(response.values['rate_resp'])\n",
    "        # Get the ranking value from the response for the preferred and least preferred\n",
    "        # responses, assuming there are no ties\n",
    "        preferred_rank = response.values[\"rate_resp\"].value\n",
    "        # least_preferred_rank = response.values[\"response_ranking\"].value[1][\"value\"]\n",
    "\n",
    "        # Construct the triplet and append to the list\n",
    "        triplets.append({\n",
    "            \"prompt\": record.fields[\"prompt1\"],\n",
    "            \"preferred_response\": preferred_rank #  record.fields[preferred_rank],\n",
    "            # \"least_preferred_response\": record.fields[least_preferred_rank],\n",
    "        })\n",
    "\n",
    "# Now, \"triplets\" is a list of dictionaries, each containing a prompt and the associated\n",
    "# preferred and less preferred responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
