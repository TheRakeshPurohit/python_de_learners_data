{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('D:\\\\gitFolders\\\\python_de_learners_data\\\\.env')\n",
    "import os\n",
    "\n",
    "rg.init(api_key=\"owner.apikey\",\n",
    "        api_url=\"https://kamaljp-argillatest.hf.space\",\n",
    "        extra_headers={\"Authorization\": f\"Bearer {os.environ['HUGGING_FACE_KEY']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create workspace\n",
    "hfgilla = 'hfgilla'\n",
    "rg.Workspace.create(name=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfws = rg.Workspace.from_name(hfgilla)\n",
    "for d in hfws.list():\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = rg.User.create(\n",
    "    username=\"annotator1\",\n",
    "    first_name=\"anno\",\n",
    "    last_name=\"tator\",\n",
    "    password=\"12345678\",\n",
    "    role=\"annotator\",\n",
    "    workspaces=[\"hfgilla\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in rg.User.list():\n",
    "    print(u.username)\n",
    "    print(u.id)\n",
    "    print(u.from_id(u.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfws.users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple feddback_task_ds\n",
    "fbds = rg.FeedbackDataset(\n",
    "    guidelines='A simple FBDS with no records',\n",
    "    fields=[\n",
    "        rg.TextField(name=\"prompt\", title='user_prompt'),\n",
    "        rg.TextField(name=\"output\", title='gen_output', use_markdown=True),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(\n",
    "            name=\"corrected_text\",\n",
    "            title='Used for correcting the output',\n",
    "            required=True,\n",
    "            use_markdown=True\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbds.push_to_argilla(name='fbds', workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbds_1 = rg.FeedbackDataset(\n",
    "    guidelines='Another fb dataset for practice',\n",
    "    fields= [\n",
    "        rg.TextField(name='model1_prompt', title='model_1'),\n",
    "        rg.TextField(name=\"modelout\", title='modelout', use_markdown=True)\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.RatingQuestion(\n",
    "            name=\"rating\",\n",
    "            title=\"Rate the quality of the response:\",\n",
    "            description=\"1 = very bad - 5= very good\",\n",
    "            required=True,\n",
    "            values=[1,2,3,4,5]  \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "fbds_1.push_to_argilla(\"fbds2\", workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some records\n",
    "recfb = rg.FeedbackRecord(\n",
    "    fields={\n",
    "        \"prompt\": \"this is funny\",\n",
    "        \"output\": \"do you think so?\"\n",
    "    },\n",
    "    metadata={'source': 'mind'},\n",
    "    external_id='rec1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbds.add_records([recfb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbds.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_fbds = rg.FeedbackDataset.from_argilla('fbds', workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_fbds.add_records([recfb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_question = rg.RatingQuestion(\n",
    "    name=\"rating\",\n",
    "    title=\"Rate the quality of the response:\",\n",
    "    description=\"1 = very bad - 5= very good\",\n",
    "    required=True,\n",
    "    values=[1,2,3,4,5]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_tc = rg.TextClassificationRecord(\n",
    "    text='this is a very huge and panaromic rock',\n",
    "    prediction=[(\"view\", 0.8), ('stay', 0.2)],\n",
    "    annotation=\"view or stay\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_fbds.add_records([rec_tc]) # will error out, as its not FBrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_tc1 = rg.TextClassificationRecord(\n",
    "    text='Lets climb the everest for sake of fun',\n",
    "    prediction=[(\"adventure\", 0.7), ('fun', 0.2), ('boring',0.1), ('worthless', 0.0)],\n",
    "    annotation=\"fun or dun\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_tc_ml = rg.TextClassificationRecord(\n",
    "    text='There is always a lot more to explore',\n",
    "    prediction=[(\"excited\", 0.8), ('annoyed', 0.2)],\n",
    "    annotation=[\"work\", \"play\"],\n",
    "    multi_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.log(records=[rec_tc, rec_tc1],\n",
    "       name='tcds',\n",
    "       workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.log(records=rec_tc1, name='tcds', workspace=hfgilla)\n",
    "\n",
    "# more than one record can be pushed into dataset, if having same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rg.log(records=rec_tc_ml, name='tcds', workspace=hfgilla)  # will throw error\n",
    "rg.log(records=rec_tc_ml, name='tcml', workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokrec = rg.TokenClassificationRecord(\n",
    "    text=\"There is lot more tuna\",\n",
    "    tokens=[\"There\", \"is\", \"lot\", \"more\", \"tuna\"],\n",
    "    prediction=[(\"NOUN\",0, 5), (\"PRN\", 6, 8), (\"PLR\", 9, 12)],\n",
    "    annotation=[(\"NOUN\", 0, 5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.log(records=tokrec, name='tokds', workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2tr = rg.Text2TextRecord(\n",
    "    text=\"this is a very long record\",\n",
    "    prediction=[\"there is lot more than the record length\"]\n",
    ")\n",
    "rg.log(records=t2tr, name='t2tr', workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.argilla.io/en/v1.26.0/practical_guides/create_update_dataset/create_dataset.html\n",
    "\n",
    "The records classes covered in this section correspond to three datasets: DatasetForTextClassification, DatasetForTokenClassification, and DatasetForText2Text. \n",
    "\n",
    "These will be deprecated in Argilla 2.0 and replaced by the fully configurable FeedbackDataset class. Not sure which dataset to use? Check out our section on choosing a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "fledgeds = rg.FeedbackDataset(\n",
    "    guidelines=\"A fully fledged annotation pipeline\",\n",
    "    fields=[\n",
    "        rg.TextField(name=\"input\", title=\"Human input\"),\n",
    "        rg.TextField(name=\"output\", title=\"Expected output\", use_markdown=True)\n",
    "    ],\n",
    "    questions =[\n",
    "        rg.RatingQuestion(\n",
    "            name=\"rating\",\n",
    "            title=\"Rate the quality of the Output:\",\n",
    "            description=\"1 = very bad - 5= very good\",\n",
    "            required=True,\n",
    "            values=[1,2,3,4,5]\n",
    "        ),\n",
    "        rg.TextQuestion(\n",
    "            name=\"corrected-text\",\n",
    "            title=\"Provide a correction to the response:\",\n",
    "            required=False,\n",
    "            use_markdown=True\n",
    "        ),\n",
    "        rg.LabelQuestion(\n",
    "            name=\"relevant\",\n",
    "            title=\"Is the response relevant for the given prompt?\",\n",
    "            labels={\"YES\": \"Yes\", \"NO\": \"No\"}, # or [\"YES\",\"NO\"]\n",
    "            required=True,\n",
    "            visible_labels=None\n",
    "        ),\n",
    "        rg.MultiLabelQuestion(\n",
    "            name=\"content_class\",\n",
    "            title=\"Does the response include any of the following?\",\n",
    "            description=\"Select all that apply\",\n",
    "            labels={\"hate\": \"Hate Speech\" , \"sexual\": \"Sexual content\", \"violent\": \"Violent content\", \"pii\": \"Personal information\", \"untruthful\": \"Untruthful info\", \"not_english\": \"Not English\", \"inappropriate\": \"Inappropriate content\"}, # or [\"hate\", \"sexual\", \"violent\", \"pii\", \"untruthful\", \"not_english\", \"inappropriate\"]\n",
    "            required=False,\n",
    "            visible_labels=4\n",
    "        ),\n",
    "#         rg.SpanQuestion(\n",
    "            # name=\"entities\",\n",
    "            # title=\"Highlight the entities in the text:\",\n",
    "            # labels={\"PER\": \"Person\", \"ORG\": \"Organization\", \"EVE\": \"Event\"},\n",
    "            # # or [\"PER\", \"ORG\", \"EVE\"],\n",
    "            # field=\"text\",\n",
    "            # required=True\n",
    "        # ),\n",
    "        rg.RankingQuestion(\n",
    "            name=\"preference\",\n",
    "            title=\"Order replies based on your preference\",\n",
    "            description=\"1 = best, 3 = worst. Ties are allowed.\",\n",
    "            required=True,\n",
    "            values={\"reply-1\": \"Reply 1\", \"reply-2\": \"Reply 2\", \"reply-3\": \"Reply 3\"} # or [\"reply-1\", \"reply-2\", \"reply-3\"]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fledgeds.push_to_argilla(name=\"explds\", workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some records\n",
    "recfb = rg.FeedbackRecord(\n",
    "    fields={\n",
    "        \"input\": \"this is funny\",\n",
    "        \"output\": \"do you think so?\"\n",
    "    },\n",
    "    metadata={'source': 'mind'},\n",
    "    external_id='rec1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explds_argilla = rg.FeedbackDataset.from_argilla('explds', workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explds_argilla.add_records([recfb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a new generation of datasets which are based on the task templates, and work well with hugging face datasets also. \n",
    "\n",
    "https://docs.argilla.io/en/v1.26.0/practical_guides/create_update_dataset/create_dataset.html\n",
    "\n",
    "https://huggingface.co/datasets?other=argilla\n",
    "\n",
    "These are easily integrated with HF, and pulled using below method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dibt_dataset = load_dataset(\"DIBT/10k_prompts_ranked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_ds = rg.FeedbackDataset.from_huggingface(\"nataliaElv/dolly_tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_ds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in rg.User.list():\n",
    "    print(user.id)\n",
    "    print(user.username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_ds.push_to_argilla(name=\"dolly_hf\", workspace=hfgilla,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These templates include the fields and questions needed for the task,\n",
    "# as well as the guidelines to provide to the annotators.\n",
    "\n",
    "# After having initialized the FeedbackDataset templates, we can still alter \n",
    "# the fields, questions, guidelines, metadata and vectors to fit our specific\n",
    "# needs you can refer to the update configuration section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla import FeedbackDataset, FeedbackRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification\n",
    "\n",
    "ds_text_class = FeedbackDataset.for_text_classification(\n",
    "    labels=[\"super\", \"duper\"],\n",
    "    multi_label=False,\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None\n",
    ") \n",
    "\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"text\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         LabelQuestion(name=\"label\", labels=[\"positive\", \"negative\"])\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "\n",
    "text_class_record = FeedbackRecord(\n",
    "    fields={\n",
    "        \"text\": \"this is funny\",\n",
    "    },\n",
    "    # metadata={'source': 'mind'},\n",
    "    # external_id='rec1'\n",
    ")\n",
    "\n",
    "ds_text_class.add_records([text_class_record])\n",
    "\n",
    "ds_text_class.push_to_argilla(name='tc_template_rec',\n",
    "                              workspace=hfgilla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_summ = FeedbackDataset.for_summarization(\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None\n",
    ")\n",
    "\n",
    "text_summ_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"text\": \"this is a very long sentence and paragraphs of text\"\n",
    "    }\n",
    ")\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"text\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         TextQuestion(name=\"summary\", use_markdown=True)\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "ds_summ.add_records([text_summ_rec])\n",
    "ds_summ.push_to_argilla(\"ds_summ\", hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_trans = FeedbackDataset.for_translation(\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None\n",
    ")\n",
    "text_trans = FeedbackRecord(\n",
    "    fields={\n",
    "        \"source\": \"This is a foriegn language text to be translated.\"\n",
    "    }\n",
    ")\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"source\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         TextQuestion(name=\"target\", use_markdown=True)\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "\n",
    "ds_trans.add_records([text_trans])\n",
    "ds_trans.push_to_argilla(\"ds_trans\", hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_t2t = rg.FeedbackDataset.for_natural_language_inference(\n",
    "    labels=None,\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    "    )\n",
    "\n",
    "t2t_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"premise\":\"This is the example premise\",\n",
    "        \"hypothesis\": \"This is the example hypothesis\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"premise\", use_markdown=True),\n",
    "#         TextField(name=\"hypothesis\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         LabelQuestion(name=\"label\", labels=[\"entailment\", \"neutral\", \"contradiction\"])\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "\n",
    "ds_t2t.add_records([t2t_rec])\n",
    "ds_t2t.push_to_argilla(\"ds_t2t\", hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_sim = rg.FeedbackDataset.for_sentence_similarity(\n",
    "    rating_scale=7,\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"sentence-1\", use_markdown=True),\n",
    "#         TextField(name=\"sentence-2\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         RatingQuestion(name=\"similarity\", values=[1, 2, 3, 4, 5, 6, 7])\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "sim_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"sentence-1\":\"This is the first sentence with a context\",\n",
    "        \"sentence-2\":\"THis is second sentence similar context to first\"\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_sim.add_records([sim_rec])\n",
    "ds_sim.push_to_argilla(\"ds_sim\", hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_qa = rg.FeedbackDataset.for_question_answering(\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"question\", use_markdown=True),\n",
    "#         TextField(name=\"context\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         TextQuestion(name=\"answer\", use_markdown=True)\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "qa_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"question\": \"This is where the question is\",\n",
    "        \"context\": \"THis is the answer for above question\"\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_qa.add_records([qa_rec])\n",
    "ds_qa.push_to_argilla(\"ds_qa\", hfgilla)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_sft = rg.FeedbackDataset.for_supervised_fine_tuning(\n",
    "    context=True,\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"prompt\", use_markdown=True),\n",
    "#         TextField(name=\"context\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         TextQuestion(name=\"response\", use_markdown=True)\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "\n",
    "sft_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"prompt\": \"This is the prompt from the user\",\n",
    "        \"context\": \"Context provided to model during inference\"\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_sft.add_records([sft_rec])\n",
    "ds_sft.push_to_argilla('df_sft', hfgilla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_pref = rg.FeedbackDataset.for_preference_modeling(\n",
    "    number_of_responses=2,\n",
    "    context=False,\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"prompt\", use_markdown=True),\n",
    "#         TextField(name=\"context\", use_markdown=True),\n",
    "#         TextField(name=\"response1\", use_markdown=True),\n",
    "#         TextField(name=\"response2\", use_markdown=True),\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         RankingQuestion(name=\"preference\", values=[\"Response 1\", \"Response 2\"])\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\"\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "pref_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"prompt\": \"This is the prompt from the user\",\n",
    "        \"response1\": \"This is the response 1 from the model\",\n",
    "        \"response2\": \"Response2 provided by the model during inference\"\n",
    "    }\n",
    ")\n",
    "ds_pref.add_records([pref_rec])\n",
    "ds_pref.push_to_argilla(\"ds_pref\", hfgilla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pref.field_by_name('prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pref.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pref.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pref.question_by_name(\"preference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_ppo = rg.FeedbackDataset.for_proximal_policy_optimization(\n",
    "    rating_scale=7,\n",
    "    context=True,\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"prompt\", use_markdown=True),\n",
    "#         TextField(name=\"context\", use_markdown=True)\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         TextQuestion(name=\"response\", use_markdown=True)\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "ppo_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"prompt\":\"This is the prompt to the model\",\n",
    "        \"context\": \"This is the context given\"\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_ppo.add_records(ppo_rec)\n",
    "ds_ppo.push_to_argilla(\"ds_ppo\", hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_dpo = rg.FeedbackDataset.for_direct_preference_optimization(\n",
    "    number_of_responses=2,\n",
    "    context=False,\n",
    "    use_markdown=True,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"prompt\", use_markdown=True),\n",
    "#         TextField(name=\"context\", use_markdown=True),\n",
    "#         TextField(name=\"response1\", use_markdown=True),\n",
    "#         TextField(name=\"response2\", use_markdown=True),\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         RankingQuestion(name=\"preference\", values=[\"Response 1\", \"Response 2\"])\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )\n",
    "dpo_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"prompt\":\"This is the prompt\",\n",
    "        \"context\":\"This is the context\",\n",
    "        \"response1\":\"This is the response1\",\n",
    "        \"response2\":\"This is the response2\",\n",
    "    }\n",
    ")\n",
    "ds_dpo.add_records([dpo_rec])\n",
    "ds_dpo.push_to_argilla('ds_dpo', hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds = rg.FeedbackDataset.for_retrieval_augmented_generation(\n",
    "    number_of_retrievals=1,\n",
    "    rating_scale=7,\n",
    "    use_markdown=False,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "ds_rag = FeedbackRecord(\n",
    "    fields={\n",
    "        \"query\":\"This is the question\",\n",
    "        \"retrieved_document_1\":\"Retrieved document one\",\n",
    "    }\n",
    ")\n",
    "ds.add_records([ds_rag])\n",
    "ds.push_to_argilla('ds_rag', hfgilla)\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"query\", use_markdown=False),\n",
    "#         TextField(name=\"retrieved_document_1\", use_markdown=False),\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         RatingQuestion(name=\"rating_retrieved_document_1\", values=[1, 2, 3, 4, 5, 6, 7]),\n",
    "#         TextQuestion(name=\"response\", use_markdown=False),\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_mulclass = rg.FeedbackDataset.for_multi_modal_classification(\n",
    "    labels=[\"video\", \"audio\", \"image\"],\n",
    "    multi_label=False,\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "ds_rec = FeedbackRecord(\n",
    "    fields={\n",
    "        \"content\":\"THis is the content in different modality\"\n",
    "    }\n",
    ")\n",
    "ds_mulclass.add_records([ds_rec])\n",
    "ds_mulclass.push_to_argilla(\"ds_mulclass\", hfgilla)\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"content\", use_markdown=True, required=True),\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         LabelQuestion(name=\"label\", labels=[\"video\", \"audio\", \"image\"])\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "ds_trans = rg.FeedbackDataset.for_multi_modal_transcription(\n",
    "    guidelines=None,\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,\n",
    ")\n",
    "rec_trans = FeedbackRecord(\n",
    "    fields={\n",
    "        \"content\":\"THis is a multi modal transcription\"\n",
    "    }\n",
    ")\n",
    "ds_trans.add_records([rec_trans])\n",
    "ds_trans.push_to_argilla(\"ds_multrans\", hfgilla)\n",
    "# FeedbackDataset(\n",
    "#     fields=[\n",
    "#         TextField(name=\"content\", use_markdown=True, required=True),\n",
    "#     ],\n",
    "#     questions=[\n",
    "#         TextQuestion(name=\"description\", use_markdown=True, required=True)\n",
    "#     ],\n",
    "#     guidelines=\"<Guidelines for the task>\",\n",
    "#     metadata_properties=\"<Metadata Properties>\",\n",
    "#     vectors_settings=\"<Vectors Settings>\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabelQuestion:** These questions ask annotators to choose one label from a list of options. This type is useful for text classification tasks. In the UI, the labels of the LabelQuestion will have a rounded shape.\n",
    "\n",
    "**MultiLabelQuestion:** These questions ask annotators to choose all applicable labels from a list of options. This type is useful for multi-label text classification tasks. In the UI, the labels of the MultiLabelQuestion will have a squared shape.\n",
    "\n",
    "**RankingQuestion:** This question asks annotators to order a list of options. It is useful to gather information on the preference or relevance of a set of options. Ties are allowed and all options will need to be ranked.\n",
    "\n",
    "**RatingQuestion:** These questions require annotators to select one option from a list of integer values. This type is useful for collecting numerical scores.\n",
    "\n",
    "**SpanQuestion:** Here, annotators are asked to select a portion of the text of a specific field and apply a label to it. This type of question is useful for named entity recognition or information extraction tasks.\n",
    "\n",
    "**TextQuestion:** These questions offer annotators a free-text area where they can enter any text. This type is useful for collecting natural language data, such as corrections or explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla import (\n",
    "    LabelQuestion,\n",
    "    MultiLabelQuestion,\n",
    "    RankingQuestion,\n",
    "    RatingQuestion,\n",
    "    TextQuestion\n",
    ")\n",
    "\n",
    "lq = LabelQuestion(\n",
    "    name=\"relevant\",\n",
    "    title=\"Is the response relevant for the given prompt?\",\n",
    "    labels={\"YES\": \"Yes\", \"NO\": \"No\"}, # or [\"YES\",\"NO\"]\n",
    "    required=True,\n",
    "    visible_labels=None\n",
    ")\n",
    "\n",
    "mlq = MultiLabelQuestion(\n",
    "    name=\"content_class\",\n",
    "    title=\"Does the response include any of the following?\",\n",
    "    description=\"Select all that apply\",\n",
    "    labels={\"hate\": \"Hate Speech\" , \"sexual\": \"Sexual content\", \"violent\": \"Violent content\", \"pii\": \"Personal information\", \"untruthful\": \"Untruthful info\", \"not_english\": \"Not English\", \"inappropriate\": \"Inappropriate content\"}, # or [\"hate\", \"sexual\", \"violent\", \"pii\", \"untruthful\", \"not_english\", \"inappropriate\"]\n",
    "    required=False,\n",
    "    visible_labels=4\n",
    ")\n",
    "\n",
    "rq = RankingQuestion(\n",
    "    name=\"preference\",\n",
    "    title=\"Order replies based on your preference\",\n",
    "    description=\"1 = best, 3 = worst. Ties are allowed.\",\n",
    "    required=True,\n",
    "    values={\"reply-1\": \"Reply 1\", \"reply-2\": \"Reply 2\", \"reply-3\": \"Reply 3\"} # or [\"reply-1\", \"reply-2\", \"reply-3\"]\n",
    ")\n",
    "\n",
    "raq = RatingQuestion(\n",
    "    name=\"quality\",\n",
    "    title=\"Rate the quality of the response:\",\n",
    "    description=\"1 = very bad - 5= very good\",\n",
    "    required=True,\n",
    "    values=[1, 2, 3, 4, 5]\n",
    ")\n",
    "\n",
    "tq = TextQuestion(\n",
    "    name=\"corrected-text\",\n",
    "    title=\"Provide a correction to the response:\",\n",
    "    required=False,\n",
    "    use_markdown=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla import TextField\n",
    "ds_qtest = FeedbackDataset(\n",
    "    guidelines=None,\n",
    "    fields=[\n",
    "        TextField(name='prompt',required=True, use_markdown=True),\n",
    "    ],\n",
    "    metadata_properties=None,\n",
    "    vectors_settings=None,    \n",
    "    questions=[tq, raq, rq, mlq, lq]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_qtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rek = FeedbackRecord(\n",
    "    fields={\n",
    "        \"prompt\":\"This is for testing out\"\n",
    "    }\n",
    ")\n",
    "ds_qtest.add_records([test_rek])\n",
    "ds_qtest.push_to_argilla(\"ds_qtest\", hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of meta data\n",
    "from argilla import (\n",
    "    TermsMetadataProperty,\n",
    "    IntegerMetadataProperty,\n",
    "    FloatMetadataProperty\n",
    ")\n",
    "\n",
    "terms_md = TermsMetadataProperty(\n",
    "    name=\"groups\",\n",
    "    title=\"Annotation groups\",\n",
    "    values=[\"group-a\", \"group-b\", \"group-c\"] #optional\n",
    ")\n",
    "\n",
    "ds_qtest.add_metadata_property(terms_md)\n",
    "\n",
    "int_md = IntegerMetadataProperty(\n",
    "    name=\"integer-metadata\",\n",
    "    title=\"Integers\",\n",
    "    min=0, #optional\n",
    "    max=100, #optional\n",
    "    visible_for_annotators=False\n",
    ")\n",
    "\n",
    "ds_qtest.add_metadata_property(int_md)\n",
    "\n",
    "fmd = FloatMetadataProperty(\n",
    "    name=\"float-metadata\",\n",
    "    title=\"Floats\",\n",
    "    min=-0.45, #optional\n",
    "    max=1000.34, #optional\n",
    "    visible_for_annotators=False\n",
    ")\n",
    "\n",
    "ds_qtest.add_metadata_property(fmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla import VectorSettings\n",
    "# You can use the add_vector_settings method. \n",
    "vectors_settings=[\n",
    "    VectorSettings(\n",
    "        name=\"sentence_embeddings\",\n",
    "        dimensions=768,\n",
    "        title=\"Sentence Embeddings\" #optional\n",
    "    )\n",
    "],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset(\n",
    "    fields=[\n",
    "        rg.TextField(name=\"question\"),\n",
    "        rg.TextField(name=\"answer\"),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.RatingQuestion(\n",
    "            name=\"answer_quality\",\n",
    "            description=\"How would you rate the quality of the answer?\",\n",
    "            values=[1, 2, 3, 4, 5],\n",
    "        ),\n",
    "        rg.TextQuestion(\n",
    "            name=\"answer_correction\",\n",
    "            description=\"If you think the answer is not accurate, please, correct it.\",\n",
    "            required=False,\n",
    "        ),\n",
    "    ],\n",
    "    metadata_properties = [\n",
    "        rg.TermsMetadataProperty(\n",
    "            name=\"groups\",\n",
    "            title=\"Annotation groups\",\n",
    "            values=[\"group-a\", \"group-b\", \"group-c\"] #optional\n",
    "        ),\n",
    "        rg.FloatMetadataProperty(\n",
    "            name=\"temperature\",\n",
    "            min=-0, #optional\n",
    "            max=1, #optional\n",
    "            visible_for_annotators=False\n",
    "        )\n",
    "    ],\n",
    "    allow_extra_metadata = False,\n",
    "    vectors_settings=[\n",
    "        rg.VectorSettings(\n",
    "            name=\"sentence_embeddings\",\n",
    "            dimensions=768,\n",
    "            title=\"Sentence Embeddings\" #optional\n",
    "        )\n",
    "    ],\n",
    "    guidelines=\"Please, read the question carefully and try to answer it as accurately as possible.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.field_by_name(\"question\")\n",
    "# rg.TextField(name=\"question\")\n",
    "dataset.question_by_name(\"answer_quality\")\n",
    "# rg.RatingQuestion(\n",
    "#     name=\"answer_quality\",\n",
    "#     description=\"How would you rate the quality of the answer?\",\n",
    "#     values=[1, 2, 3, 4, 5],\n",
    "# )\n",
    "dataset.metadata_property_by_name(\"groups\")\n",
    "# rg.TermsMetadataProperty(\n",
    "#     name=\"groups\",\n",
    "#     title=\"Annotation groups\",\n",
    "#     values=[\"group-a\", \"group-b\", \"group-c\"]\n",
    "# )\n",
    "# rg.VectorSettings(\n",
    "#     name=\"sentence_embeddings\",\n",
    "#     title=\"Sentence Embeddings\",\n",
    "#     dimensions=768\n",
    "# )\n",
    "dataset.vector_settings_by_name(\"sentence_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = dataset.push_to_argilla(name=\"fulds\", workspace=hfgilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating details..\n",
    "dataset = rg.FeedbackDataset(...)\n",
    "\n",
    "new_fields=[\n",
    "    TextField(),\n",
    "    TextField(),\n",
    "]\n",
    "\n",
    "dataset.fields.extend(new_fields)\n",
    "# Remove a non-required field\n",
    "dataset.fields.pop(0)\n",
    "\n",
    "# Add new questions\n",
    "new_questions=[\n",
    "    LabelQuestion(),\n",
    "    MultiLabelQuestion(),\n",
    "]\n",
    "\n",
    "dataset.questions.extend(new_questions)\n",
    "\n",
    "# Remove a non-required question\n",
    "dataset.questions.pop(0)\n",
    "\n",
    "# Add metadata properties\n",
    "metadata = TermsMetadataProperty(name=\"my_metadata\", values=[\"like\", \"dislike\"])\n",
    "dataset.add_metadata_property(metadata)\n",
    "\n",
    "# Change metadata properties title\n",
    "metadata_cfg = dataset.metadata_property_by_name(\"my_metadata\")\n",
    "metadata_cfg.title = \"Likes\"\n",
    "dataset.update_metadata_properties(metadata_cfg)\n",
    "\n",
    "# Delete a metadata property\n",
    "dataset.delete_metadata_properties(metadata_properties=\"my_metadata\")\n",
    "\n",
    "# Add vector settings to the dataset\n",
    "dataset.add_vector_settings(VectorSettings(name=\"my_vectors\", dimensions=786))\n",
    "\n",
    "# Change vector settings title\n",
    "vector_cfg = ds.vector_settings_by_name(\"my_vectors\")\n",
    "vector_cfg.title = \"Old vectors\"\n",
    "dataset.update_vectors_settings(vector_cfg)\n",
    "\n",
    "# Delete vector settings\n",
    "dataset.delete_vectors_settings(\"my_vectors\")\n",
    "\n",
    "# Define new guidelines for a question\n",
    "dataset.questions[0].description = 'New description for the question.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
