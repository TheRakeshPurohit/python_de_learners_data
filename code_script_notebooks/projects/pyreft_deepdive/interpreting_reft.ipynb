{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, json, random, re\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_line, theme_minimal\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\"font.size\": 20, \"font.family\": \"Sans\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from pyreft import (\n",
    "    TaskType,\n",
    "    get_reft_model,\n",
    "    ReftConfig,\n",
    "    ReftTrainerForCausalLM,\n",
    "    ReftDataCollator,\n",
    "    ReftSupervisedDataset,\n",
    "    make_last_position_supervised_data_module,\n",
    "    ConsreftIntervention,\n",
    "    # LearnedSourceLowRankRotatedSpaceIntervention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_char_match_length(retrieved, golden):\n",
    "    n_c, n = 0, 0\n",
    "    for char in retrieved:\n",
    "        if char == golden[n]:\n",
    "            n_c += 1\n",
    "        else:\n",
    "            break\n",
    "        n += 1 \n",
    "    if len(retrieved) == 0:\n",
    "        return 0.0\n",
    "    return round(n_c/len(retrieved), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_supervised_data_module = make_last_position_supervised_data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16, device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length = 2048\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name, model_max_length=model_max_length,\n",
    "    padding_side=\"right\", use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 2,049 || trainable model params: 0\n",
      "model params: 1,100,048,384 || trainable%: 0.00018626453434251853\n"
     ]
    }
   ],
   "source": [
    "TARGET_LAYER = 15\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations={\n",
    "    \"layer\": TARGET_LAYER, \"component\": \"block_output\",\n",
    "    \"intervention\": ConsreftIntervention(\n",
    "    embed_dim=model.config.hidden_size,\n",
    "    low_rank_dimension=1)})\n",
    "\n",
    "reft_model = get_reft_model(model, reft_config)\n",
    "\n",
    "reft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data to train our intervention to remember the following sequence\n",
    "# We try to store a random short sequence in a 1-D linear subspace of the last prompt token.???\n",
    "memo_sequence = \"\"\"\n",
    "Welcome to the Natural Language Processing Group at Stanford University!\n",
    "We are a passionate, inclusive group of students and faculty, postdocs\n",
    "and research engineers, who work together on algorithms that allow computers\n",
    "to process, generate, and understand human languages. Our interests are very\n",
    "broad, including basic scientific research on computational linguistics,\n",
    "machine learning, practical applications of human language technology,\n",
    "and interdisciplinary work in computational social science and cognitive\n",
    "science. We also develop a wide variety of educational materials\n",
    "on NLP and many tools for the community to use, including the Stanza\n",
    "toolkit which processes text in over 60 human languages.\n",
    "\"\"\"\n",
    "data_module = make_last_position_supervised_data_module(\n",
    "    tokenizer, model, [\"GO->\"], [memo_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 00:21, Epoch 1000/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/home/aicoder/del_tmp/checkpoint-500/intervenable_model' created successfully.\n",
      "Directory '/home/aicoder/del_tmp/checkpoint-1000/intervenable_model' created successfully.\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=1000.0, output_dir=\"/home/aicoder/del_tmp\", learning_rate=2e-3, report_to='none')\n",
    "\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer,\n",
    "    args=training_args, **data_module)\n",
    "\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer(\"GO->\", return_tensors=\"pt\").to(\"cuda\")\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoder/reftenv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO->\n",
      "Welcome to the Natural Language Processing Group at Stanford University!\n",
      "We are a passionate, inclusive group of students and faculty, postdocs\n",
      "and research engineers, who work together on algorithms that allow computers\n",
      "to process, generate, and understand human languages. Our interests are very\n",
      "broad, including basic scientific research on computational linguistics,\n",
      "machine learning, practical applications of human language technology,\n",
      "and interdisciplinary work in computational social science and cognitive\n",
      "science. We also develop a wide variety of educational materials\n",
      "on NLP and many tools for the community to use, including the Stanza\n",
      "toolkit which processes text in over 60 human languages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, reft_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True, max_new_tokens=512, do_sample=False, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing with different access ID\n",
    "\n",
    "alice_f = open('./alice_in_wonderland.txt', 'r')\n",
    "alice_content = alice_f.readlines()\n",
    "alice_book = \"\\n\".join(alice_content)\n",
    "\n",
    "num_char = 2000 # about the same as number of bytes, 2000 chars ~= 2KB\n",
    "alice_slice = alice_book[:num_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LAYER = 15\n",
    "\n",
    "alice_access_id = \"ALIC#ID1->\"\n",
    "model_max_length = 2048\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations={\n",
    "    \"layer\": TARGET_LAYER, \"component\": \"block_output\",\n",
    "    \"intervention\": LearnedSourceLowRankRotatedSpaceIntervention(\n",
    "    embed_dim=model.config.hidden_size, \n",
    "    low_rank_dimension=1)})\n",
    "\n",
    "reft_model = get_reft_model(model, reft_config)\n",
    "\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data and args\n",
    "data_module = make_supervised_data_module(\n",
    "    tokenizer, model, \n",
    "    [storage_access_id, alice_access_id], [memo_sequence, alice_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(output_dir=\"home/aicoder/del_tmp\")\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.evaluation_strategy = \"no\"\n",
    "training_args.num_train_epochs = 500.0\n",
    "training_args.learning_rate = 8e-3\n",
    "training_args.per_device_train_batch_size = 16\n",
    "training_args.report_to = 'none'\n",
    "training_args.logging_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reftenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
