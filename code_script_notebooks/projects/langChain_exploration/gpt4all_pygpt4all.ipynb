{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UIVEJHU5I34X"
      },
      "outputs": [],
      "source": [
        "!pip install pygpt4all langchain > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "GAVe8zY2u2Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "7yR9Dbfou2SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin"
      ],
      "metadata": {
        "id": "iks9IgtYu2Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = '/content/ggml-gpt4all-j.bin'"
      ],
      "metadata": {
        "id": "oiaVIxAKu2Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callbacks = [StreamingStdOutCallbackHandler()]"
      ],
      "metadata": {
        "id": "1NyWChozvnmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verbose is required to pass to the callback manager\n",
        "llm = GPT4All(model=local_path, \n",
        "              callbacks=callbacks, \n",
        "              verbose=True)"
      ],
      "metadata": {
        "id": "SZrshU_AvnkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to use GPT4ALL_J model add the backend parameter\n",
        "llm = GPT4All(model=local_path, \n",
        "              backend='gptj', \n",
        "              callbacks=callbacks, \n",
        "              verbose=True)"
      ],
      "metadata": {
        "id": "gEotU9XOvng3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PIqGIKTMvnec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qu0NstfivncI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}