{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xrqS4_brx_B",
        "outputId": "dfc8bc94-8852-4102-ed52-f2ea02f95a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install huggingface chromadb transformers langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install configparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1-uWlhsMsgac",
        "outputId": "4468c538-4ec6-4f37-b1e8-509b77532f0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting configparser\n",
            "  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: configparser\n",
            "Successfully installed configparser-5.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "configparser"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "import os\n",
        "api_reader = configparser.ConfigParser()\n",
        "api_reader.read_file(open('/content/apidata.config'))\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = api_reader[\"OPENAI\"][\"HFREAD\"]"
      ],
      "metadata": {
        "id": "vX_td4FfsgVj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/linux_play.txt') as lin:\n",
        "  txt_lin = lin.read()"
      ],
      "metadata": {
        "id": "j1GBxQTQsgSr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import text_splitter as ts"
      ],
      "metadata": {
        "id": "6zcSb324sgJJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recursive splitter is recommended splitter\n",
        "\n",
        "#By default the characters it tries to split on are [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "\n",
        "#length_function: how the length of chunks is calculated. Defaults to just counting number of characters, but it’s pretty common to pass a token counter here.\n",
        "\n",
        "#chunk_size: the maximum size of your chunks (as measured by the length function).\n",
        "\n",
        "#chunk_overlap: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (eg do a sliding window)."
      ],
      "metadata": {
        "id": "G51lu39Dw5Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec_splitter = ts.RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ")"
      ],
      "metadata": {
        "id": "K9c090HJvn2e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_lin = rec_splitter.create_documents([txt_lin])\n",
        "print(len(split_lin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzvzydY-voDc",
        "outputId": "eabb8382-ee2f-421d-ac14-a546c9d905c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(split_lin[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltoqc2izvpSu",
        "outputId": "fa3cc216-34d2-4a05-9f9a-5184f9334a4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='BEGIN;\\nLinux Playbook' lookup_str='' metadata={} lookup_index=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rec_500splitter = ts.RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ")"
      ],
      "metadata": {
        "id": "uMKodTiFvpvJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_500lin = rec_500splitter.create_documents([txt_lin])\n",
        "print(len(split_500lin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vYinJSRvqAD",
        "outputId": "0354724c-4889-4507-810a-ddc16220cf80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_500lin[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz-m5gV_wrmo",
        "outputId": "52b82495-6879-4041-cb87-1340e3a19e94"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='BEGIN;\\nLinux Playbook\\n\\nThe command and scenarios has to be executed inside the\\nKali Docker image. The docker image is called linux_playg.\\nThe docker will be contain the\\nset of set of files, prepared for this plabook and\\nuploaded on to dockerhub.', lookup_str='', metadata={}, lookup_index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets work on the character splitter\n",
        "\n",
        "#How the text is split: by single character\n",
        "\n",
        "#How the chunk size is measured: by length function passed in (defaults to number of characters)"
      ],
      "metadata": {
        "id": "3bes5ZEhw0-q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_split = ts.CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")"
      ],
      "metadata": {
        "id": "ZIqM_7hcw07J"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_500lin = char_split.create_documents([txt_lin])\n",
        "print(len(char_500lin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkQLxgNdw041",
        "outputId": "d438cbfb-480e-49bb-b9f1-7072fa161030"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Created a chunk of size 1137, which is longer than the specified 500\n",
            "WARNING:root:Created a chunk of size 1181, which is longer than the specified 500\n",
            "WARNING:root:Created a chunk of size 1231, which is longer than the specified 500\n",
            "WARNING:root:Created a chunk of size 1789, which is longer than the specified 500\n",
            "WARNING:root:Created a chunk of size 532, which is longer than the specified 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_500lin[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCh3BcUxw02L",
        "outputId": "8a29f717-b7be-4992-9bde-e4d8dcbd3a64"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='BEGIN;\\nLinux Playbook\\n\\nThe command and scenarios has to be executed inside the\\nKali Docker image. The docker image is called linux_playg.\\nThe docker will be contain the\\nset of set of files, prepared for this plabook and\\nuploaded on to dockerhub.', lookup_str='', metadata={}, lookup_index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "YsKNAdPjw0z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_splitter = ts.CharacterTextSplitter.from_huggingface_tokenizer(tokenizer, \n",
        "                                                                  chunk_size=100, \n",
        "                                                                  chunk_overlap=0)\n",
        "texts = hf_splitter.split_text(txt_lin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kby_CpX3w0xv",
        "outputId": "407c254f-e5a7-46d7-c3cf-10807d86af9e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Created a chunk of size 314, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 380, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 336, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 144, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 108, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 157, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 213, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 763, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 134, which is longer than the specified 100\n",
            "WARNING:root:Created a chunk of size 195, which is longer than the specified 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Interesting latex splitter "
      ],
      "metadata": {
        "id": "h8L5fjVhw0uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latex_text = \"\"\"\n",
        "\\documentclass{article}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\maketitle\n",
        "\n",
        "\\section{Introduction}\n",
        "Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
        "\n",
        "\\subsection{History of LLMs}\n",
        "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
        "\n",
        "\\subsection{Applications of LLMs}\n",
        "LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "latex_splitter = ts.LatexTextSplitter(chunk_size=400, \n",
        "                                      chunk_overlap=0)"
      ],
      "metadata": {
        "id": "GXIYmXwEw0sF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = latex_splitter.create_documents([latex_text])\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BbY7uYBw0pl",
        "outputId": "34e9294d-9fed-4e5d-9a43-e867471e25ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='\\\\documentclass{article}\\n\\n\\x08egin{document}\\n\\n\\\\maketitle', lookup_str='', metadata={}, lookup_index=0),\n",
              " Document(page_content='Introduction}\\nLarge language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.', lookup_str='', metadata={}, lookup_index=0),\n",
              " Document(page_content='History of LLMs}\\nThe earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.', lookup_str='', metadata={}, lookup_index=0),\n",
              " Document(page_content='Applications of LLMs}\\nLLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\\n\\n\\\\end{document}', lookup_str='', metadata={}, lookup_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "\n",
        "markdown_text = \"\"\"\n",
        "# 🦜️🔗 LangChain\n",
        "\n",
        "⚡ Building applications with LLMs through composability ⚡\n",
        "\n",
        "## Quick Install\n",
        "\n",
        "```bash\n",
        "# Hopefully this code block isn't split\n",
        "pip install langchain\n",
        "```\n",
        "\n",
        "As an open source project in a rapidly developing field, we are extremely open to contributions.\n",
        "\"\"\"\n",
        "markdown_splitter = MarkdownTextSplitter(chunk_size=100, \n",
        "                                         chunk_overlap=0)\n",
        "mar_docs = markdown_splitter.create_documents([markdown_text])\n",
        "mar_docs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwFFA5wsw0nA",
        "outputId": "eb48b509-72a1-4fdb-d83b-6919469a69b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='# 🦜️🔗 LangChain\\n\\n⚡ Building applications with LLMs through composability ⚡', lookup_str='', metadata={}, lookup_index=0),\n",
              " Document(page_content=\"Quick Install\\n\\n```bash\\n# Hopefully this code block isn't split\\npip install langchain\", lookup_str='', metadata={}, lookup_index=0),\n",
              " Document(page_content='As an open source project in a rapidly developing field, we are extremely open to contributions.', lookup_str='', metadata={}, lookup_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jiEJQo821zY",
        "outputId": "9ba281fb-9a77-49cf-d7ae-839a75f04677"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_splitter = ts.NLTKTextSplitter(chunk_size=1000)\n",
        "\n",
        "txt_nltk = nltk_splitter.create_documents([txt_lin])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBoBdVhww0fi",
        "outputId": "4ec0c51f-9749-4b70-fb01-26ddcab978b4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Created a chunk of size 3706, which is longer than the specified 1000\n",
            "WARNING:root:Created a chunk of size 1269, which is longer than the specified 1000\n",
            "WARNING:root:Created a chunk of size 1690, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#there is Python code splitter too"
      ],
      "metadata": {
        "id": "tzD9YIrdw0bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqXAMRrX4Rnw",
        "outputId": "3b499511-8502-4c15-8983-992b986f3e1b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.4/1.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tik_spliter = ts.TokenTextSplitter(chunk_size=50,\n",
        "                                   chunk_overlap = 0)\n",
        "\n",
        "tik_lin = tik_spliter.split_text(txt_lin)\n",
        "print(len(tik_lin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAtNWg6uw0X6",
        "outputId": "30ea7a3f-2261-42df-d0b1-82413121e16b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7NimXNBw0Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SbT0K3tw0O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoucSVViw0JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ejOeQ1Cw0GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zM8hPBozw0At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bd2kYUrpwz4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Lv9ofouwzvd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}