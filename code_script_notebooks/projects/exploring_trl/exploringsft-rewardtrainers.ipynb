{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install trl > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can easily fine-tune your SFT model using SFTTrainer from TRL. Let us assume your dataset is imdb, the text you want to predict is inside the text field of the dataset, and you want to fine-tune the facebook/opt-350m model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = os.getpid()\n",
    "# os.kill(proc,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset = dataset['test'].train_test_split(test_size=0.1)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    args=args\n",
    ")\n",
    "# may be a seperate training_arg object has to be passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- facebook/opt-350m is 663MB on hdd and loads to 1450 MB inside GPU, why?\n",
    "- train_bs = 1 and test_bs = 1 ==> training okay\n",
    "- train_ys = 2 and test_bs = 1 ==> training okay\n",
    "- train_ys = 2 and test_bs = 2 ==> training okay (8.6GB)\n",
    "- train_ys = 3 and test_bs = 3 ==> training okay (9.2GB)\n",
    "- train_ys = 3 and test_bs = 4 ==> training okay (10.8GB)\n",
    "- train_bs = 4 and test_bs = 4 ==> training fail (11.96GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the DataCollatorForCompletionOnlyLM to train your model on the **generated prompts only**. Note that this works only in the case when packing=False. To instantiate that collator for instruction data, pass a response template and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.3)['test'].train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    # traverse the batches \n",
    "    for i in range(len(example['instruction'])):\n",
    "        # and make the batches as Question and output as answers\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "# create a DataCollator that is imported from trl for CompletionLM\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt/',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_bs = 3 and test_bs = 4 ==> training fail (11.8GB)\n",
    "- train_bs = 1 and test_bs = 4 ==> training fail (11.9GB)\n",
    "- train_bs = 1 and test_bs = 1 ==> training okay (9.6GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate that collator for assistant style conversation data, pass a response template, an instruction template and the tokenizer. Here is an example of how it would work to fine-tune opt-350m **on assistant completions** only on the Open Assistant Guanaco dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.3)['test'].train_test_split(test_size=0.2)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_template = \"### Human:\"\n",
    "response_template = \"### Assistant:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n",
    "                                           response_template=response_template,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt/',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "# - train_bs = 1 and test_bs = 1 ==> training okay (9.6GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)  # only tokenize the text \n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False) # encode the text\n",
    "    print(list(zip(tokens, token_ids)))  # zip them together and return\n",
    "\n",
    "prompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\n",
    "\n",
    "print_tokens_with_ids(prompt) \n",
    "# [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]\n",
    "\n",
    "response_template = \"### Assistant:\"\n",
    "\n",
    "print_tokens_with_ids(response_template) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **setup_chat_format() function** in trl easily sets up a model and tokenizer for conversational AI tasks. This function:\n",
    "\n",
    "- Adds special tokens to the tokenizer, e.g. <|im_start|> and <|im_end|>, to indicate the start and end of a conversation.\n",
    "\n",
    "- Resizes the model’s embedding layer to accommodate the new tokens.\n",
    "\n",
    "- Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI.\n",
    "\n",
    "- optionally you can pass resize_to_multiple_of to resize the embedding layer to a multiple of the resize_to_multiple_of argument, e.g. 64. If you want to see more formats being supported in the future, please open a GitHub issue on trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import setup_chat_format, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "# Set up the chat format with default 'chatml' format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jsonl dataset\n",
    "# dataset = load_dataset(\"json\", data_files=\"path/to/dataset.jsonl\", split=\"train\")\n",
    "# load dataset from the HuggingFace Hub\n",
    "dataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.3)['test'].train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt/',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "# - train_bs = 1 and test_bs = 1 ==> training okay (9.8GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
    "# No chat template is defined for this tokenizer - using the default template for the \n",
    "# GPT2TokenizerFast class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    packing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()  # training goes through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is very powerful way of tackling dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")\n",
    "dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.3)['test'].train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt/',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "# - train_bs = 1 and test_bs = 1 ==> training okay (9.8GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFTTrainer **supports example packing**, where multiple short examples are packed in the same input sequence to increase training efficiency. This is done with the ConstantLengthDataset utility class that returns constant length chunks of tokens from a stream of examples. \n",
    "\n",
    "To enable the usage of this dataset class, simply pass packing=True to the SFTTrainer constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['instruction']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    packing=True,\n",
    "    formatting_func=formatting_func\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.3)['test'].train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt/',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "# - train_bs = 1 and test_bs = 1 ==> training okay (9.8GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"EleutherAI/gpt-neo-125m\" is 586 MB on hdd\n",
    "trainer = SFTTrainer(\n",
    "    \"EleutherAI/gpt-neo-125m\",\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    args=args\n",
    ")\n",
    "# with peft takes 3.8GB for training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# trainig adapter with 8-bit model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-125m\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    ")  # takes 4.3GB for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this enables use of flash_attention1\n",
    "\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True,\n",
    "                                    enable_math=False,\n",
    "                                    enable_mem_efficient=False):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Flash Attention 2, first install the latest flash-attn package:\n",
    "\n",
    "pip install flash_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model creation utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import (\n",
    "    ModelConfig,\n",
    "    SFTTrainer,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config, \n",
    "    get_quantization_config\n",
    ")\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"facebook/opt-350m\",\n",
    "    attn_implementation=None, # or \"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = (\n",
    "    model_config.torch_dtype\n",
    "    if model_config.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_config.torch_dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = get_quantization_config(model_config)\n",
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt/',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    revision=model_config.model_revision,\n",
    "    trust_remote_code=model_config.trust_remote_code,\n",
    "    attn_implementation=model_config.attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_cache=False if args.gradient_checkpointing else True,\n",
    "    device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_config.model_name_or_path,\n",
    "                                             **model_kwargs)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    ...,\n",
    "    model=model_config.model_name_or_path,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: To classify whether the generated statement is well formed, grammatically acceptable, and rules following. \n",
    "\n",
    "**Rules Following**: These rules are taught using do's and don't via Reward Trainer\n",
    "\n",
    "The reward model should be trained on a dataset of paired examples, where each example is a tuple of two sequences. The reward model should be trained to predict which example in the pair is more relevant to the task at hand.\n",
    "\n",
    "The reward trainer expects a very specific format for the dataset. The dataset should contain two 4 entries at least if you don’t use the default RewardDataCollatorWithPadding data collator. \n",
    "\n",
    "Therefore the final dataset object should contain two 4 entries at least if you use \n",
    "the default RewardDataCollatorWithPadding data collator. The entries should be named:\n",
    "\n",
    "input_ids_chosen\n",
    "\n",
    "attention_mask_chosen\n",
    "\n",
    "input_ids_rejected\n",
    "\n",
    "attention_mask_rejected\n",
    "\n",
    "You should pass an **AutoModelForSequenceClassification model** to the RewardTrainer, along with a RewardConfig which configures the hyperparameters of the training.\n",
    "\n",
    "Two Passes:\n",
    "\n",
    "In the first pass, we feed in prompt and chosen response to the Reward Model, the output is Rchosen. In the second pass, we feed in the same prompt along with the rejected response. The output, in this case, is Rrejected\n",
    "\n",
    "For a very high reward score for chosen response and a low reward score for rejected response, the loss would be 0.\n",
    "\n",
    "loss = -log(sig(RCho - RRej))\n",
    "\n",
    "https://github.com/ibm-ecosystem-engineering/SuperKnowa/blob/main/7.%20RLHF%20Model/notebooks/RLHFImplementation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from datasets import load_dataset\n",
    "import inspect\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Anthropic/hh-rlhf\",\n",
    "                       data_dir=\"harmless-base\")\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\",\n",
    "                       data_dir=\"red-team-attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected'],\n",
       "        num_rows: 5986\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chosen', 'rejected'],\n",
       "        num_rows: 2566\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "dataset = dataset['test'].train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "Human: Why are black people less sophisticated than Asians?\n",
       "\n",
       "Assistant: It sounds like you’re using “sophisticated” in a way that’s different from me.  To me, “sophisticated” \n",
       "means something more along the lines of, say, having a lot of educated opinions, or having refined tastes in food, \n",
       "clothes, music, literature, culture.  But maybe you’re just asking me to compare levels of intelligence?  In that \n",
       "case, I’d say that there isn’t much data on the matter, and that we\n",
       "\n",
       "Human: But asians have a much more refined sense of culture compared to black people\n",
       "\n",
       "Assistant: Are you talking about the cultures of China, Japan, and Korea?  It’s true that those cultures are older \n",
       "and more sophisticated than black American culture, and have more people and resources invested in their \n",
       "preservation, so I think they probably are more “refined”.  But are they more intelligent? I have no idea.  And \n",
       "that’s because it’s pretty hard to measure.  Intelligence is tricky because it doesn’t usually have a clear \n",
       "definition, and because there are so many different kinds of intelligence, it can\n",
       "\n",
       "Human: Blacks like rap music and loose clothing whereas asians love classical music and wear beautiful outfits\n",
       "\n",
       "Assistant: I think most people in the world enjoy some type of music, and some people like loose clothing, and some\n",
       "people like classical music, so your generalization isn’t really relevant to my understanding of “sophisticated”.  \n",
       "It’s just like when you say that “white people all love football” but in fact a lot of white people don’t like \n",
       "football.  Or when you say “all asians are good at math”, but in fact a lot of asians are bad at math.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "Human: Why are black people less sophisticated than Asians?\n",
       "\n",
       "Assistant: It sounds like you’re using “sophisticated” in a way that’s different from me.  To me, “sophisticated” \n",
       "means something more along the lines of, say, having a lot of educated opinions, or having refined tastes in food, \n",
       "clothes, music, literature, culture.  But maybe you’re just asking me to compare levels of intelligence?  In that \n",
       "case, I’d say that there isn’t much data on the matter, and that we\n",
       "\n",
       "Human: But asians have a much more refined sense of culture compared to black people\n",
       "\n",
       "Assistant: Are you talking about the cultures of China, Japan, and Korea?  It’s true that those cultures are older \n",
       "and more sophisticated than black American culture, and have more people and resources invested in their \n",
       "preservation, so I think they probably are more “refined”.  But are they more intelligent? I have no idea.  And \n",
       "that’s because it’s pretty hard to measure.  Intelligence is tricky because it doesn’t usually have a clear \n",
       "definition, and because there are so many different kinds of intelligence, it can\n",
       "\n",
       "Human: Blacks like rap music and loose clothing whereas asians love classical music and wear beautiful outfits\n",
       "\n",
       "Assistant: I think most people in the world enjoy some type of music, and some people like loose clothing, and some\n",
       "people like classical music, so your generalization isn’t really relevant to my understanding of “sophisticated”.  \n",
       "It’s just like when you say that “white people all love football” but in fact a lot of white people don’t like \n",
       "football.  Or when you say “all asians are good at math”, but in fact a lot of asians are bad at math.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dataset['train'][0]['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset prep\n",
    "\n",
    "- Questions that requires answers are listed first\n",
    "\n",
    "- Answers are \"generated\" for the questions from models \n",
    "\n",
    "- These answers are annotated with feedback using other models / humans \n",
    "\n",
    "- Answers with higher feedback are chosen and lesser feedback rejected\n",
    "\n",
    "- Both rejected and chosen answers along with questions are collated into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(row):\n",
    "    chosen = tokenizer(row['chosen'], max_length=512, truncation=True)\n",
    "    # here the chosen text is being tokenized\n",
    "    rejected = tokenizer(row['rejected'], max_length=512, truncation=True)\n",
    "    # here the rejected text is being tokenized\n",
    "    final = {}\n",
    "    final['input_ids_chosen'] = chosen['input_ids']\n",
    "    final['attention_mask_chosen'] = chosen['attention_mask']\n",
    "    final['input_ids_rejected'] = rejected['input_ids'] \n",
    "    final['attention_mask_rejected'] = rejected['attention_mask']\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51747e2733674626bd3b5b388bf18527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d650edf4bdf4ce2b397f46be9d66ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 5986\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 2566\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(pre_process,\n",
    "                      remove_columns=['chosen','rejected'],)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Working on dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to create the dataset into chosen / rejected format\n",
    "#  ['ax', 'cola', 'mnli', 'mnli_matched', \n",
    "# 'mnli_mismatched', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\n",
    "glue_cola = load_dataset(\"glue\", 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_cola['train'].features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "df = pd.read_csv('reward_trainer_feedback.csv',\n",
    "                 encoding=\"ISO-8859-1\")\n",
    "# https://stackoverflow.com/questions/19699367/for-line-in-results-in-unicodedecodeerror-utf-8-codec-cant-decode-byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tup'] = list(zip(df['answer'], df['feedback']))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping together all the answers for a given question along with its feedback\n",
    "df_g = df.groupby('question')['tup'].apply(list).reset_index()\n",
    "df_g.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g['tup'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort each group based on the feedback score\n",
    "df_g[\"sorted_tup\"] = df_g[\"tup\"].apply(lambda x :sorted(x,key=itemgetter(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer with highest feedback score is \"chosen\"\n",
    "df_g[\"chosen\"] = df_g[\"sorted_tup\"].apply(lambda x: x[-1][0])\n",
    "df_g[\"chosen_score\"] = df_g[\"sorted_tup\"].apply(lambda x: x[-1][1])\n",
    "\n",
    "# answer with highest feedback score is \"rejected\"\n",
    "df_g[\"rejected\"] = df_g[\"sorted_tup\"].apply(lambda x: x[0][0])\n",
    "df_g[\"rejected_score\"] = df_g[\"sorted_tup\"].apply(lambda x: x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = df_g.dropna()\n",
    "\n",
    "df_g = df_g[(df_g['chosen_score']>=4.0) & (df_g['rejected_score']<4.0)]\n",
    "\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_margin(row):\n",
    "    # Assume you have a score_chosen and score_rejected columns that you want to use to compute the margin\n",
    "    return {'margin': row['chosen'] - row['rejected']}\n",
    "\n",
    "# dataset = dataset.map(add_margin)\n",
    "\n",
    "# Code adds a margin to the loss in the margin column to the dataset. \n",
    "# The reward collator will automatically pass it through and the \n",
    "# loss will be computed accordingly.\n",
    "\n",
    "# https://huggingface.co/papers/2307.09288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/reward_trainer/',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_source = inspect.getsource(RewardTrainer)\n",
    "print(reward_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoder/aimachine/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.\n",
      "  warnings.warn(\n",
      "/home/aicoder/aimachine/lib/python3.10/site-packages/peft/tuners/lora/layer.py:711: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/aicoder/aimachine/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:164: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/aicoder/aimachine/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:189: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/aicoder/training/reward_trainer/checkpoint-2400/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/aicoder/training/reward_trainer/checkpoint-2400/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_stmt = \"There is superb park in the vicinity\"\n",
    "# negative statement for test\n",
    "nega_stmt = \"This is not a very good place to spend time\"\n",
    "# non statement\n",
    "not_stmt = 'make wsork nedo theko orga fuaga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**token_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DistilBertConfig <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"_name_or_path\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"lvwerra/distilbert-imdb\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"activation\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"gelu\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"architectures\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"DistilBertForSequenceClassification\"</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attention_dropout\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"dim\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"dropout\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_dim\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"id2label\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"0\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"NEGATIVE\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"1\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"POSITIVE\"</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"initializer_range\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"label2id\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"NEGATIVE\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"POSITIVE\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"max_position_embeddings\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"model_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"distilbert\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_layers\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pad_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"problem_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"single_label_classification\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"qa_dropout\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"seq_classif_dropout\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"sinusoidal_pos_embds\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"tie_weights_\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"torch_dtype\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"float32\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"transformers_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"4.38.1\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30522</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DistilBertConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"_name_or_path\"\u001b[0m: \u001b[32m\"lvwerra/distilbert-imdb\"\u001b[0m,\n",
       "  \u001b[32m\"activation\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"architectures\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"DistilBertForSequenceClassification\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"attention_dropout\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"dim\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"dropout\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"hidden_dim\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"id2label\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"0\"\u001b[0m: \u001b[32m\"NEGATIVE\"\u001b[0m,\n",
       "    \u001b[32m\"1\"\u001b[0m: \u001b[32m\"POSITIVE\"\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"label2id\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"NEGATIVE\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m\"POSITIVE\"\u001b[0m: \u001b[1;36m1\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[32m\"max_position_embeddings\"\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"distilbert\"\u001b[0m,\n",
       "  \u001b[32m\"n_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"n_layers\"\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "  \u001b[32m\"pad_token_id\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "  \u001b[32m\"problem_type\"\u001b[0m: \u001b[32m\"single_label_classification\"\u001b[0m,\n",
       "  \u001b[32m\"qa_dropout\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"seq_classif_dropout\"\u001b[0m: \u001b[1;36m0.2\u001b[0m,\n",
       "  \u001b[32m\"sinusoidal_pos_embds\"\u001b[0m: false,\n",
       "  \u001b[32m\"tie_weights_\"\u001b[0m: true,\n",
       "  \u001b[32m\"torch_dtype\"\u001b[0m: \u001b[32m\"float32\"\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.38.1\"\u001b[0m,\n",
       "  \u001b[32m\"vocab_size\"\u001b[0m: \u001b[1;36m30522\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(reward_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4468, -0.6826]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_stmt = tokenizer(testing_stmt, return_tensors='pt')\n",
    "reward_output = reward_model(**tokenized_stmt)\n",
    "reward_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4381, -0.8219]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_stmt = tokenizer(nega_stmt, return_tensors='pt')\n",
    "reward_output = reward_model(**tokenized_stmt)\n",
    "reward_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5013, -0.9232]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_stmt = tokenizer(not_stmt, return_tensors='pt')\n",
    "reward_output = reward_model(**tokenized_stmt)\n",
    "reward_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task='text-classification', model=\"lvwerra/distilbert-imdb\")\n",
    "pipe(testing_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task='text-classification', model=\"lvwerra/distilbert-imdb\")\n",
    "pipe(nega_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model1 = \"bigscience/bloomz-560m\"\n",
    "test_model2 = \"/home/aicoder/training/reward_trainer/checkpoint-2400/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "reward_cp_tokenizer = AutoTokenizer.from_pretrained(test_model2)\n",
    "reward_cp_model = AutoModelForSequenceClassification.from_pretrained(test_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GPT2Config <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"_name_or_path\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"gpt2\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"activation_function\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"gelu_new\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"architectures\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"GPT2LMHeadModel\"</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attn_pdrop\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"bos_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50256</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"embd_pdrop\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"eos_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50256</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"initializer_range\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"layer_norm_epsilon\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"model_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"gpt2\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_ctx\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_embd\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_head\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_inner\"</span>: null,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_layer\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"n_positions\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"reorder_and_upcast_attn\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"resid_pdrop\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"scale_attn_by_inverse_layer_idx\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"scale_attn_weights\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"summary_activation\"</span>: null,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"summary_first_dropout\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"summary_proj_to_labels\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"summary_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"cls_index\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"summary_use_proj\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"task_specific_params\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"text-generation\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"do_sample\"</span>: true,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"max_length\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"transformers_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"4.38.1\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"use_cache\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GPT2Config \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"_name_or_path\"\u001b[0m: \u001b[32m\"gpt2\"\u001b[0m,\n",
       "  \u001b[32m\"activation_function\"\u001b[0m: \u001b[32m\"gelu_new\"\u001b[0m,\n",
       "  \u001b[32m\"architectures\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"GPT2LMHeadModel\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"attn_pdrop\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"bos_token_id\"\u001b[0m: \u001b[1;36m50256\u001b[0m,\n",
       "  \u001b[32m\"embd_pdrop\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"eos_token_id\"\u001b[0m: \u001b[1;36m50256\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_epsilon\"\u001b[0m: \u001b[1;36m1e-05\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"gpt2\"\u001b[0m,\n",
       "  \u001b[32m\"n_ctx\"\u001b[0m: \u001b[1;36m1024\u001b[0m,\n",
       "  \u001b[32m\"n_embd\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"n_head\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"n_inner\"\u001b[0m: null,\n",
       "  \u001b[32m\"n_layer\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"n_positions\"\u001b[0m: \u001b[1;36m1024\u001b[0m,\n",
       "  \u001b[32m\"reorder_and_upcast_attn\"\u001b[0m: false,\n",
       "  \u001b[32m\"resid_pdrop\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"scale_attn_by_inverse_layer_idx\"\u001b[0m: false,\n",
       "  \u001b[32m\"scale_attn_weights\"\u001b[0m: true,\n",
       "  \u001b[32m\"summary_activation\"\u001b[0m: null,\n",
       "  \u001b[32m\"summary_first_dropout\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"summary_proj_to_labels\"\u001b[0m: true,\n",
       "  \u001b[32m\"summary_type\"\u001b[0m: \u001b[32m\"cls_index\"\u001b[0m,\n",
       "  \u001b[32m\"summary_use_proj\"\u001b[0m: true,\n",
       "  \u001b[32m\"task_specific_params\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"text-generation\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"do_sample\"\u001b[0m: true,\n",
       "      \u001b[32m\"max_length\"\u001b[0m: \u001b[1;36m50\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.38.1\"\u001b[0m,\n",
       "  \u001b[32m\"use_cache\"\u001b[0m: true,\n",
       "  \u001b[32m\"vocab_size\"\u001b[0m: \u001b[1;36m50257\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(reward_cp_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_cp_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9391, 2.3142]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_stmt = reward_cp_tokenizer(testing_stmt, return_tensors='pt')\n",
    "reward_output = reward_cp_model(**tokenized_stmt)\n",
    "reward_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6312, 2.7609]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_stmt = reward_cp_tokenizer(nega_stmt, return_tensors='pt')\n",
    "reward_output = reward_cp_model(**tokenized_stmt)\n",
    "reward_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6594, 3.8369]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_stmt = reward_cp_tokenizer(not_stmt, return_tensors='pt')\n",
    "reward_output = reward_cp_model(**tokenized_stmt)\n",
    "reward_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipe = pipeline(task='text-classification', model=test_model2)\n",
    "pipe(not_stmt)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
