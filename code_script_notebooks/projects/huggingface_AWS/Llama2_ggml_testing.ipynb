{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4a451d-1354-4bf1-8190-492436b926f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygpt4all\n",
      "  Using cached pygpt4all-1.1.0.tar.gz (4.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m436.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygptj (from pygpt4all)\n",
      "  Downloading pygptj-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m277.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m272.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyllamacpp (from pygpt4all)\n",
      "  Downloading pyllamacpp-2.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.7/359.7 kB\u001b[0m \u001b[31m466.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m481.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /home/kamal/jupyter_env/lib/python3.11/site-packages (from llama-cpp-python) (4.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/kamal/jupyter_env/lib/python3.11/site-packages (from llama-cpp-python) (1.23.5)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: pygpt4all, llama-cpp-python\n",
      "  Building wheel for pygpt4all (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygpt4all: filename=pygpt4all-1.1.0-py3-none-any.whl size=5842 sha256=e9418b89ab3ae0e1317d82159c442744311ae4cd1dffabe0474144829b77b8a6\n",
      "  Stored in directory: /home/kamal/.cache/pip/wheels/36/ab/8c/440c6ccf98affd60077617cae112189a22bd2e4edeebc04b39\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp311-cp311-linux_x86_64.whl size=279974 sha256=39ddfaf61c5b7e884bead83ac5617d01bfbc37f041ae1c6f493e63ee4b34f8c8\n",
      "  Stored in directory: /home/kamal/.cache/pip/wheels/e2/67/cb/481cfaabbb5fd5edab627c5b475de63e1b6f7d4d7b678d4d25\n",
      "Successfully built pygpt4all llama-cpp-python\n",
      "Installing collected packages: pyllamacpp, pygptj, diskcache, pygpt4all, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.77 pygpt4all-1.1.0 pygptj-2.0.3 pyllamacpp-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pygpt4all llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5573ff28-8894-445f-9992-5d089088821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import GPT4All, LlamaCpp\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e797d092-5b01-4c8c-b13a-adb0116f493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/kamal/DATA/huggingface/hub/llama-2-7b.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7130.73 MB (+  256.00 MB per state)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path = \"/media/kamal/DATA/huggingface/hub/llama-2-7b.ggmlv3.q8_0.bin\"\n",
    "\n",
    "llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e8f818-ef67-45f1-bf39-0c86dbb0696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1102.51 ms\n",
      "llama_print_timings:      sample time =     5.89 ms /    20 runs   (    0.29 ms per token,  3393.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   890.67 ms /    11 tokens (   80.97 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:        eval time =  3126.04 ms /    19 runs   (  164.53 ms per token,     6.08 tokens per second)\n",
      "llama_print_timings:       total time =  4048.66 ms\n"
     ]
    }
   ],
   "source": [
    "output = llm(\"How many planets are there in the solar system}\", \n",
    "             max_tokens=32,\n",
    "             echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c24eb2a-6d55-4f78-9259-112b2dd94f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-be76a6e4-d408-4c00-b7fd-17d73c2aa7df',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1690885624,\n",
       " 'model': '/media/kamal/DATA/huggingface/hub/llama-2-7b.ggmlv3.q8_0.bin',\n",
       " 'choices': [{'text': 'How many planets are there in the solar system}\\nWhat are the names of all 8 planets\\nHow many planets in solar systems',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 12, 'completion_tokens': 19, 'total_tokens': 31}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71bf105-046a-42d4-90c2-846cb4f6af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04af9b6d-7d9d-4c97-9604-2c356e7e6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/kamal/DATA/huggingface/hub/llama-2-7b.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7130.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    input={\"temperature\": 0.75, \n",
    "           \"max_length\": 2000, \n",
    "           \"top_p\": 1},\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12222adc-cc17-4405-b88f-b51048257eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "\n",
      "## The first 2048 numbers is the Fibonacci Sequence and I need them.\n",
      " \n",
      "\n",
      "## There are 1,937,655,374,624 stars in our galaxy.\n",
      " \n",
      "\n",
      "## I want to know how many stars there are in the universe.\n",
      " \n",
      "\n",
      "## This is what my computer does when I ask it.\n",
      " \n",
      "\n",
      "<div align=center>\n",
      "<script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"></script><ins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-1572368429190425\" data-ad-slot=\"6971754042\"></ins><script> (adsbygoogle = window.adsbygoogle || []).push({});</script>\n",
      "<div align=center>\n",
      "### I am doing"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   643.05 ms\n",
      "llama_print_timings:      sample time =    76.10 ms /   256 runs   (    0.30 ms per token,  3363.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   696.16 ms /     8 tokens (   87.02 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:        eval time = 41198.43 ms /   255 runs   (  161.56 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time = 42349.72 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n \\n\\n## The first 2048 numbers is the Fibonacci Sequence and I need them.\\n \\n\\n## There are 1,937,655,374,624 stars in our galaxy.\\n \\n\\n## I want to know how many stars there are in the universe.\\n \\n\\n## This is what my computer does when I ask it.\\n \\n\\n<div align=center>\\n<script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"></script><ins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-1572368429190425\" data-ad-slot=\"6971754042\"></ins><script> (adsbygoogle = window.adsbygoogle || []).push({});</script>\\n<div align=center>\\n### I am doing'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: How many stars does the universe have\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
