{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Qcwo__0uy0-M"
   },
   "outputs": [],
   "source": [
    "!pip install torch>=2.0.1 safetensors==0.3.1 sentencepiece>=0.1.97 ninja==1.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1ESNCB23rvz",
    "outputId": "53e00707-14a0-4153-d527-1d17d8fed8fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b77fq1hO3rsy",
    "outputId": "441a83e5-43f2-4d92-97f0-5cfcb4f6a2f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'exllama'...\n",
      "remote: Enumerating objects: 1246, done.\u001b[K\n",
      "remote: Counting objects: 100% (589/589), done.\u001b[K\n",
      "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
      "remote: Total 1246 (delta 504), reused 455 (delta 454), pack-reused 657\u001b[K\n",
      "Receiving objects: 100% (1246/1246), 860.00 KiB | 6.28 MiB/s, done.\n",
      "Resolving deltas: 100% (888/888), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/turboderp/exllama.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPe5mS-67kPV",
    "outputId": "989cb4ff-552a-4fb1-93d6-602bc848e462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/exllama\n"
     ]
    }
   ],
   "source": [
    "%cd /content/exllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mSYEfZc7ogn",
    "outputId": "52c0743d-f428-473c-c037-9ee1925654b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/exllama\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C2xng49F3rpy"
   },
   "outputs": [],
   "source": [
    "from model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
    "from tokenizer import ExLlamaTokenizer\n",
    "from generator import ExLlamaGenerator\n",
    "from lora import ExLlamaLora\n",
    "import os, glob\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GkseWXQV3rne"
   },
   "outputs": [],
   "source": [
    "!pip install transformers > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ACjXayZ9nXD",
    "outputId": "aedba01f-a309-451f-e741-6e88170dc4d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'alpaca-lora-7b'...\n",
      "remote: Enumerating objects: 48, done.\u001b[K\n",
      "remote: Total 48 (delta 0), reused 0 (delta 0), pack-reused 48\u001b[K\n",
      "Unpacking objects: 100% (48/48), 6.14 KiB | 241.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/tloen/alpaca-lora-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "przHckBL9BqA",
    "outputId": "18a7be0e-6e8e-41b3-debb-5cf556c25c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-7B-4bit-128g'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "Unpacking objects:   4% (1/23)\r",
      "Unpacking objects:   8% (2/23)\r",
      "Unpacking objects:  13% (3/23)\r",
      "Unpacking objects:  17% (4/23)\r",
      "Unpacking objects:  21% (5/23)\r",
      "Unpacking objects:  26% (6/23)\r",
      "Unpacking objects:  30% (7/23)\r",
      "Unpacking objects:  34% (8/23)\r",
      "Unpacking objects:  39% (9/23)\r",
      "Unpacking objects:  43% (10/23)\r",
      "Unpacking objects:  47% (11/23)\r",
      "Unpacking objects:  52% (12/23)\r",
      "Unpacking objects:  56% (13/23)\r",
      "Unpacking objects:  60% (14/23)\r",
      "remote: Total 23 (delta 0), reused 0 (delta 0), pack-reused 23\u001b[K\n",
      "Unpacking objects:  65% (15/23)\r",
      "Unpacking objects:  69% (16/23)\r",
      "Unpacking objects:  73% (17/23)\r",
      "Unpacking objects:  78% (18/23)\r",
      "Unpacking objects:  82% (19/23)\r",
      "Unpacking objects:  86% (20/23)\r",
      "Unpacking objects:  91% (21/23)\r",
      "Unpacking objects:  95% (22/23)\r",
      "Unpacking objects: 100% (23/23)\r",
      "Unpacking objects: 100% (23/23), 479.33 KiB | 6.75 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-4bit-128g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jurTV_dM98_m"
   },
   "outputs": [],
   "source": [
    "# Directory containt model, tokenizer, generator\n",
    "\n",
    "model_directory = \"/content/exllama/LLaMA-7B-4bit-128g/\"\n",
    "\n",
    "# Directory containing LoRA config and weights\n",
    "\n",
    "lora_directory = \"/content/exllama/alpaca-lora-7b/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TVExpJB6989Q"
   },
   "outputs": [],
   "source": [
    "# Locate files we need within those directories\n",
    "\n",
    "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n",
    "model_config_path = os.path.join(model_directory, \"config.json\")\n",
    "st_pattern = os.path.join(model_directory, \"*.safetensors\")\n",
    "model_path = glob.glob(st_pattern)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "63J57BZl986T"
   },
   "outputs": [],
   "source": [
    "lora_config_path = os.path.join(lora_directory, \"adapter_config.json\")\n",
    "lora_path = os.path.join(lora_directory, \"adapter_model.bin\")\n",
    "\n",
    "# Create config, model, tokenizer and generator\n",
    "\n",
    "config = ExLlamaConfig(model_config_path)               # create config from config.json\n",
    "config.model_path = model_path                          # supply path to model weights file\n",
    "\n",
    "model = ExLlama(config)                                 # create ExLlama instance and load the weights\n",
    "tokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\n",
    "\n",
    "cache = ExLlamaCache(model)                             # create cache for inference\n",
    "generator = ExLlamaGenerator(model, tokenizer, cache)   # create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gojQox32983F"
   },
   "outputs": [],
   "source": [
    "# Load LoRA\n",
    "\n",
    "lora = ExLlamaLora(model, lora_config_path, lora_path)\n",
    "\n",
    "# Configure generator\n",
    "\n",
    "generator.settings.token_repetition_penalty_max = 1.2\n",
    "generator.settings.temperature = 0.65\n",
    "generator.settings.top_p = 0.4\n",
    "generator.settings.top_k = 0\n",
    "generator.settings.typical = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePOizlDT_KPi",
    "outputId": "1f6b46a2-f400-433a-b49f-c81607edfe48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- LoRA ----------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0792792630>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \\\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\" \\\n",
    "    \"\\n\" \\\n",
    "    \"### Instruction:\\n\" \\\n",
    "    \"List five colors in alphabetical order.\\n\" \\\n",
    "    \"\\n\" \\\n",
    "    \"### Response:\"\n",
    "\n",
    "# Generate with LoRA\n",
    "\n",
    "print(\" --- LoRA ----------------- \")\n",
    "print(\"\")\n",
    "\n",
    "generator.lora = lora\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_wAs1Kh_KNI",
    "outputId": "16069716-502e-4382-94b5-eb1c558a75d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List five colors in alphabetical order.\n",
      "\n",
      "### Response:\n",
      "Brown, Green, Orange, Purple, Red\n"
     ]
    }
   ],
   "source": [
    "output = generator.generate_simple(prompt, max_new_tokens = 200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fS6UlXVpANVQ",
    "outputId": "0af2a69f-48e8-4e21-e647-1cf6d63921cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- LoRA ----------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0792792630>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "From Wikipedia, the free encyclopedia\n",
    "\n",
    "A large language model (LLM) is a computerized language model consisting of an artificial neural network with many parameters (tens of millions to billions), trained on large quantities of unlabeled text containing up to trillions of tokens, using self-supervised learning or semi-supervised learning achieved by parallel computing.[1] With advent of transformers, notable for requiring less training time compared to older long short-term memory (LSTM) models,[2] thus enabling large (language) datasets, such as the Wikipedia Corpus and Common Crawl, to be used for training due to parallelization,[3] LLMs emerged around 2018. They are general purpose models, excelling at a wide range of tasks. The level of proficiency exhibited by language models in various tasks, as well as the breadth of tasks they can handle, rely less on the model's design and more on the size of the training corpus, the quantity of parameters, and the computational power achieved by parallel computing.[4] This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.[5]\n",
    "\n",
    "Because the training corpus and number of parameters were so extensive, despite being trained on simple tasks such as predicting the next word in a sentence, they have along the way (implicitly) learned syntax and semantics of human language. Although they acquired general \"knowledge\" about the world, inherent in the statements contained in the training corpus, they also acquired the inaccuracies and biases from those statements.[5]\n",
    "Training\n",
    "\n",
    "Most LLM are pre-trained such that given a training dataset of text tokens, the model predicts the tokens in the dataset. There are two general styles of such pretraining:[6]\n",
    "\n",
    "    autoregressive (GPT-style, \"predict the next word\"): Given a segment of text like \"I like to eat\" the model predicts the next tokens, like \"ice cream\".\n",
    "    masked (\"BERT-style\",[7] \"cloze test\"): Given a segment of text like \"I like to [MASK] [MASK] cream\" the model predicts the masked tokens, like \"eat ice\".\n",
    "\n",
    "LLMs may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[7]\n",
    "\n",
    "Usually, LLMs are trained to minimize a specific loss function: the average negative log likelihood per token (also called cross-entropy loss).[citation needed] For example, if an autoregressive model, given \"I like to eat\", predicts a probability distribution P r ( ⋅ | I like to eat ) {\\displaystyle Pr(\\cdot |{\\text{I like to eat}})} then the negative log likelihood loss on this token is − log ⁡ P r ( ice | I like to eat ) {\\displaystyle -\\log Pr({\\text{ice}}|{\\text{I like to eat}})}.\n",
    "\n",
    "During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. There are also many more evaluation criteria than just negative log likelihood. See the section below for details.\n",
    "Training dataset size\n",
    "\n",
    "The earliest LLMs were trained on corpora having on the order of billions of words.\n",
    "\n",
    "GPT-1, the first model in OpenAI's numbered series of generative pre-trained transformer models, was trained in 2018 on BookCorpus, consisting of 985 million words.[8] In the same year, BERT was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words.[7] Since then, training corpora for LLMs have increased by orders of magnitude, reaching up to trillions of tokens.[7]\n",
    "Training cost\n",
    "\n",
    "LLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (2 orders of magnitude smaller than the state of the art at the time) at anywhere from $80 thousand to $1.6 million.[9][10] Advances in software and hardware have brought the cost substantially down, with a 2023 paper reporting a cost of 72,300 A100-GPU-hours to train a 12 billion parameter model.[11]\n",
    "\n",
    "For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[12]\n",
    "\n",
    "Companies in the 2020s invested large sums into increasingly large LLMs. GPT-2 (1.5 billion parameters, 2019) cost $50,000 to train, while Google PaLM (540 billion parameters, 2022) cost $8 million to train.[13]\n",
    "Application to downstream tasks\n",
    "\n",
    "Between 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to fine tune the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as GPT-3 can solve tasks without additional training via \"prompting\" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.[5]\n",
    "Fine-tuning\n",
    "Main article: Fine-tuning (machine learning)\n",
    "\n",
    "Fine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. sentiment analysis, named-entity recognition, or part-of-speech tagging). It is a form of transfer learning. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[7]\n",
    "Prompting\n",
    "See also: Prompt engineering and Few-shot learning (natural language processing)\n",
    "\n",
    "In the prompting paradigm, popularized by GPT-3,[14] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[5] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[14]\n",
    "\n",
    "Review: This movie stinks.\n",
    "Sentiment: negative\n",
    "\n",
    "Review: This movie is fantastic!\n",
    "Sentiment:\n",
    "\n",
    "If the model outputs \"positive\", then it has correctly solved the task. In zero-shot prompting, no solved examples are provided.[9][15] An example of a zero-shot prompt for the same sentiment analysis task would be \"The sentiment associated with the movie review 'This movie is fantastic!' is\".[16]\n",
    "\n",
    "Few-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are translation, question answering, cloze tasks, unscrambling words, and using a novel word in a sentence.[15] The creation and optimisation of such prompts is called prompt engineering.\n",
    "Instruction tuning\n",
    "\n",
    "Often, instruction tuning is necessary because otherwise an artificial neural network, in response to user 's instruction \"Write an essay about the main themes represented in Hamlet,\" may generate a response such as \"If you submit the essay after March 17th, your grade will be reduced by 10% for each day of delay\" based on the frequency of this textual sequence in the corpus. It is only through instruction tuning that the model learns what the response should actually contain for specific instructions.\n",
    "\n",
    "Answer the following question\n",
    "\n",
    "How are LLMs pretrained?\n",
    "\"\"\"\n",
    "\n",
    "# Generate with LoRA\n",
    "\n",
    "print(\" --- LoRA ----------------- \")\n",
    "print(\"\")\n",
    "\n",
    "generator.lora = lora\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkdfQ2KHArkO",
    "outputId": "4b60e0f6-626e-4441-9b77-2e6c2615d7f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "A large language model (LLM) is a computerized language model consisting of an artificial neural network with many parameters (tens of millions to billions), trained on large quantities of unlabeled text containing up to trillions of tokens, using self-supervised learning or semi-supervised learning achieved by parallel computing.[1] With advent of transformers, notable for requiring less training time compared to older long short-term memory (LSTM) models,[2] thus enabling large (language) datasets, such as the Wikipedia Corpus and Common Crawl, to be used for training due to parallelization,[3] LLMs emerged around 2018. They are general purpose models, excelling at a wide range of tasks. The level of proficiency exhibited by language models in various tasks, as well as the breadth of tasks they can handle, rely less on the model's design and more on the size of the training corpus, the quantity of parameters, and the computational power achieved by parallel computing.[4] This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.[5]\n",
      "\n",
      "Because the training corpus and number of parameters were so extensive, despite being trained on simple tasks such as predicting the next word in a sentence, they have along the way (implicitly) learned syntax and semantics of human language. Although they acquired general \"knowledge\" about the world, inherent in the statements contained in the training corpus, they also acquired the inaccuracies and biases from those statements.[5]\n",
      "Training\n",
      "\n",
      "Most LLM are pre-trained such that given a training dataset of text tokens, the model predicts the tokens in the dataset. There are two general styles of such pretraining:[6]\n",
      "\n",
      "    autoregressive (GPT-style, \"predict the next word\"): Given a segment of text like \"I like to eat\" the model predicts the next tokens, like \"ice cream\".\n",
      "    masked (\"BERT-style\",[7] \"cloze test\"): Given a segment of text like \"I like to [MASK] [MASK] cream\" the model predicts the masked tokens, like \"eat ice\".\n",
      "\n",
      "LLMs may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[7]\n",
      "\n",
      "Usually, LLMs are trained to minimize a specific loss function: the average negative log likelihood per token (also called cross-entropy loss).[citation needed] For example, if an autoregressive model, given \"I like to eat\", predicts a probability distribution P r ( ⋅ | I like to eat ) {\\displaystyle Pr(\\cdot |{\text{I like to eat}})} then the negative log likelihood loss on this token is − log ⁡ P r ( ice | I like to eat ) {\\displaystyle -\\log Pr({\text{ice}}|{\text{I like to eat}})}.\n",
      "\n",
      "During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. There are also many more evaluation criteria than just negative log likelihood. See the section below for details.\n",
      "Training dataset size\n",
      "\n",
      "The earliest LLMs were trained on corpora having on the order of billions of words.\n",
      "\n",
      "GPT-1, the first model in OpenAI's numbered series of generative pre-trained transformer models, was trained in 2018 on BookCorpus, consisting of 985 million words.[8] In the same year, BERT was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words.[7] Since then, training corpora for LLMs have increased by orders of magnitude, reaching up to trillions of tokens.[7]\n",
      "Training cost\n",
      "\n",
      "LLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (2 orders of magnitude smaller than the state of the art at the time) at anywhere from $80 thousand to $1.6 million.[9][10] Advances in software and hardware have brought the cost substantially down, with a 2023 paper reporting a cost of 72,300 A100-GPU-hours to train a 12 billion parameter model.[11]\n",
      "\n",
      "For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[12]\n",
      "\n",
      "Companies in the 2020s invested large sums into increasingly large LLMs. GPT-2 (1.5 billion parameters, 2019) cost $50,000 to train, while Google PaLM (540 billion parameters, 2022) cost $8 million to train.[13]\n",
      "Application to downstream tasks\n",
      "\n",
      "Between 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to fine tune the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as GPT-3 can solve tasks without additional training via \"prompting\" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.[5]\n",
      "Fine-tuning\n",
      "Main article: Fine-tuning (machine learning)\n",
      "\n",
      "Fine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. sentiment analysis, named-entity recognition, or part-of-speech tagging). It is a form of transfer learning. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[7]\n",
      "Prompting\n",
      "See also: Prompt engineering and Few-shot learning (natural language processing)\n",
      "\n",
      "In the prompting paradigm, popularized by GPT-3,[14] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[5] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[14]\n",
      "\n",
      "Review: This movie stinks.\n",
      "Sentiment: negative\n",
      "\n",
      "Review: This movie is fantastic!\n",
      "Sentiment:\n",
      "\n",
      "If the model outputs \"positive\", then it has correctly solved the task. In zero-shot prompting, no solved examples are provided.[9][15] An example of a zero-shot prompt for the same sentiment analysis task would be \"The sentiment associated with the movie review 'This movie is fantastic!' is\".[16]\n",
      "\n",
      "Few-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are translation, question answering, cloze tasks, unscrambling words, and using a novel word in a sentence.[15] The creation and optimisation of such prompts is called prompt engineering.\n",
      "Instruction tuning\n",
      "\n",
      "Often, instruction tuning is necessary because otherwise an artificial neural network, in response to user 's instruction \"Write an essay about the main themes represented in Hamlet,\" may generate a response such as \"If you submit the essay after March 17th, your grade will be reduced by 10% for each day of delay\" based on the frequency of this textual sequence in the corpus. It is only through instruction tuning that the model learns what the response should actually contain for specific instructions.\n",
      "\n",
      "Answer the following question\n",
      "\n",
      "How are LLMs pretrained?\n",
      "\n",
      "LLMs can be trained either autoregressively (GPT-style) or masked (BERT-style). Autoregressive models predict the next token given a segment of text like “I like to [MASK]”. Masked models predict the missing tokens from a sentence like “I like to [MASK] cream” where the model must fill in the blanks with its predictions.\n"
     ]
    }
   ],
   "source": [
    "output = generator.generate_simple(prompt, max_new_tokens = 200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QT2QXOJkCALk",
    "outputId": "c4bf2cbb-a497-4a59-89d2-9e3cbdc43d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "A large language model (LLM) is a computerized language model consisting of an artificial neural network with many parameters (tens of millions to billions), trained on large quantities of unlabeled text containing up to trillions of tokens, using self-supervised learning or semi-supervised learning achieved by parallel computing.[1] With advent of transformers, notable for requiring less training time compared to older long short-term memory (LSTM) models,[2] thus enabling large (language) datasets, such as the Wikipedia Corpus and Common Crawl, to be used for training due to parallelization,[3] LLMs emerged around 2018. They are general purpose models, excelling at a wide range of tasks. The level of proficiency exhibited by language models in various tasks, as well as the breadth of tasks they can handle, rely less on the model's design and more on the size of the training corpus, the quantity of parameters, and the computational power achieved by parallel computing.[4] This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.[5]\n",
      "\n",
      "Because the training corpus and number of parameters were so extensive, despite being trained on simple tasks such as predicting the next word in a sentence, they have along the way (implicitly) learned syntax and semantics of human language. Although they acquired general \"knowledge\" about the world, inherent in the statements contained in the training corpus, they also acquired the inaccuracies and biases from those statements.[5]\n",
      "Training\n",
      "\n",
      "Most LLM are pre-trained such that given a training dataset of text tokens, the model predicts the tokens in the dataset. There are two general styles of such pretraining:[6]\n",
      "\n",
      "    autoregressive (GPT-style, \"predict the next word\"): Given a segment of text like \"I like to eat\" the model predicts the next tokens, like \"ice cream\".\n",
      "    masked (\"BERT-style\",[7] \"cloze test\"): Given a segment of text like \"I like to [MASK] [MASK] cream\" the model predicts the masked tokens, like \"eat ice\".\n",
      "\n",
      "LLMs may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[7]\n",
      "\n",
      "Usually, LLMs are trained to minimize a specific loss function: the average negative log likelihood per token (also called cross-entropy loss).[citation needed] For example, if an autoregressive model, given \"I like to eat\", predicts a probability distribution P r ( ⋅ | I like to eat ) {\\displaystyle Pr(\\cdot |{\text{I like to eat}})} then the negative log likelihood loss on this token is − log ⁡ P r ( ice | I like to eat ) {\\displaystyle -\\log Pr({\text{ice}}|{\text{I like to eat}})}.\n",
      "\n",
      "During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. There are also many more evaluation criteria than just negative log likelihood. See the section below for details.\n",
      "Training dataset size\n",
      "\n",
      "The earliest LLMs were trained on corpora having on the order of billions of words.\n",
      "\n",
      "GPT-1, the first model in OpenAI's numbered series of generative pre-trained transformer models, was trained in 2018 on BookCorpus, consisting of 985 million words.[8] In the same year, BERT was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words.[7] Since then, training corpora for LLMs have increased by orders of magnitude, reaching up to trillions of tokens.[7]\n",
      "Training cost\n",
      "\n",
      "LLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (2 orders of magnitude smaller than the state of the art at the time) at anywhere from $80 thousand to $1.6 million.[9][10] Advances in software and hardware have brought the cost substantially down, with a 2023 paper reporting a cost of 72,300 A100-GPU-hours to train a 12 billion parameter model.[11]\n",
      "\n",
      "For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[12]\n",
      "\n",
      "Companies in the 2020s invested large sums into increasingly large LLMs. GPT-2 (1.5 billion parameters, 2019) cost $50,000 to train, while Google PaLM (540 billion parameters, 2022) cost $8 million to train.[13]\n",
      "Application to downstream tasks\n",
      "\n",
      "Between 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to fine tune the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as GPT-3 can solve tasks without additional training via \"prompting\" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.[5]\n",
      "Fine-tuning\n",
      "Main article: Fine-tuning (machine learning)\n",
      "\n",
      "Fine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. sentiment analysis, named-entity recognition, or part-of-speech tagging). It is a form of transfer learning. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[7]\n",
      "Prompting\n",
      "See also: Prompt engineering and Few-shot learning (natural language processing)\n",
      "\n",
      "In the prompting paradigm, popularized by GPT-3,[14] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[5] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[14]\n",
      "\n",
      "Review: This movie stinks.\n",
      "Sentiment: negative\n",
      "\n",
      "Review: This movie is fantastic!\n",
      "Sentiment:\n",
      "\n",
      "If the model outputs \"positive\", then it has correctly solved the task. In zero-shot prompting, no solved examples are provided.[9][15] An example of a zero-shot prompt for the same sentiment analysis task would be \"The sentiment associated with the movie review 'This movie is fantastic!' is\".[16]\n",
      "\n",
      "Few-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are translation, question answering, cloze tasks, unscrambling words, and using a novel word in a sentence.[15] The creation and optimisation of such prompts is called prompt engineering.\n",
      "Instruction tuning\n",
      "\n",
      "Often, instruction tuning is necessary because otherwise an artificial neural network, in response to user 's instruction \"Write an essay about the main themes represented in Hamlet,\" may generate a response such as \"If you submit the essay after March 17th, your grade will be reduced by 10% for each day of delay\" based on the frequency of this textual sequence in the corpus. It is only through instruction tuning that the model learns what the response should actually contain for specific instructions.\n",
      "\n",
      "Answer the following question\n",
      "What was the estimate of the 2020 study?\n",
      "\n",
      "Answer: The estimated cost of training GPT-3 (1.5 billion parameters) was $8 million.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"From Wikipedia, the free encyclopedia\n",
    "\n",
    "A large language model (LLM) is a computerized language model consisting of an artificial neural network with many parameters (tens of millions to billions), trained on large quantities of unlabeled text containing up to trillions of tokens, using self-supervised learning or semi-supervised learning achieved by parallel computing.[1] With advent of transformers, notable for requiring less training time compared to older long short-term memory (LSTM) models,[2] thus enabling large (language) datasets, such as the Wikipedia Corpus and Common Crawl, to be used for training due to parallelization,[3] LLMs emerged around 2018. They are general purpose models, excelling at a wide range of tasks. The level of proficiency exhibited by language models in various tasks, as well as the breadth of tasks they can handle, rely less on the model's design and more on the size of the training corpus, the quantity of parameters, and the computational power achieved by parallel computing.[4] This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.[5]\n",
    "\n",
    "Because the training corpus and number of parameters were so extensive, despite being trained on simple tasks such as predicting the next word in a sentence, they have along the way (implicitly) learned syntax and semantics of human language. Although they acquired general \"knowledge\" about the world, inherent in the statements contained in the training corpus, they also acquired the inaccuracies and biases from those statements.[5]\n",
    "Training\n",
    "\n",
    "Most LLM are pre-trained such that given a training dataset of text tokens, the model predicts the tokens in the dataset. There are two general styles of such pretraining:[6]\n",
    "\n",
    "    autoregressive (GPT-style, \"predict the next word\"): Given a segment of text like \"I like to eat\" the model predicts the next tokens, like \"ice cream\".\n",
    "    masked (\"BERT-style\",[7] \"cloze test\"): Given a segment of text like \"I like to [MASK] [MASK] cream\" the model predicts the masked tokens, like \"eat ice\".\n",
    "\n",
    "LLMs may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[7]\n",
    "\n",
    "Usually, LLMs are trained to minimize a specific loss function: the average negative log likelihood per token (also called cross-entropy loss).[citation needed] For example, if an autoregressive model, given \"I like to eat\", predicts a probability distribution P r ( ⋅ | I like to eat ) {\\displaystyle Pr(\\cdot |{\\text{I like to eat}})} then the negative log likelihood loss on this token is − log ⁡ P r ( ice | I like to eat ) {\\displaystyle -\\log Pr({\\text{ice}}|{\\text{I like to eat}})}.\n",
    "\n",
    "During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. There are also many more evaluation criteria than just negative log likelihood. See the section below for details.\n",
    "Training dataset size\n",
    "\n",
    "The earliest LLMs were trained on corpora having on the order of billions of words.\n",
    "\n",
    "GPT-1, the first model in OpenAI's numbered series of generative pre-trained transformer models, was trained in 2018 on BookCorpus, consisting of 985 million words.[8] In the same year, BERT was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words.[7] Since then, training corpora for LLMs have increased by orders of magnitude, reaching up to trillions of tokens.[7]\n",
    "Training cost\n",
    "\n",
    "LLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (2 orders of magnitude smaller than the state of the art at the time) at anywhere from $80 thousand to $1.6 million.[9][10] Advances in software and hardware have brought the cost substantially down, with a 2023 paper reporting a cost of 72,300 A100-GPU-hours to train a 12 billion parameter model.[11]\n",
    "\n",
    "For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[12]\n",
    "\n",
    "Companies in the 2020s invested large sums into increasingly large LLMs. GPT-2 (1.5 billion parameters, 2019) cost $50,000 to train, while Google PaLM (540 billion parameters, 2022) cost $8 million to train.[13]\n",
    "Application to downstream tasks\n",
    "\n",
    "Between 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to fine tune the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as GPT-3 can solve tasks without additional training via \"prompting\" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.[5]\n",
    "Fine-tuning\n",
    "Main article: Fine-tuning (machine learning)\n",
    "\n",
    "Fine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. sentiment analysis, named-entity recognition, or part-of-speech tagging). It is a form of transfer learning. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[7]\n",
    "Prompting\n",
    "See also: Prompt engineering and Few-shot learning (natural language processing)\n",
    "\n",
    "In the prompting paradigm, popularized by GPT-3,[14] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[5] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[14]\n",
    "\n",
    "Review: This movie stinks.\n",
    "Sentiment: negative\n",
    "\n",
    "Review: This movie is fantastic!\n",
    "Sentiment:\n",
    "\n",
    "If the model outputs \"positive\", then it has correctly solved the task. In zero-shot prompting, no solved examples are provided.[9][15] An example of a zero-shot prompt for the same sentiment analysis task would be \"The sentiment associated with the movie review 'This movie is fantastic!' is\".[16]\n",
    "\n",
    "Few-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are translation, question answering, cloze tasks, unscrambling words, and using a novel word in a sentence.[15] The creation and optimisation of such prompts is called prompt engineering.\n",
    "Instruction tuning\n",
    "\n",
    "Often, instruction tuning is necessary because otherwise an artificial neural network, in response to user 's instruction \"Write an essay about the main themes represented in Hamlet,\" may generate a response such as \"If you submit the essay after March 17th, your grade will be reduced by 10% for each day of delay\" based on the frequency of this textual sequence in the corpus. It is only through instruction tuning that the model learns what the response should actually contain for specific instructions.\n",
    "\n",
    "Answer the following question\n",
    "What was the estimate of the 2020 study\"\"\"\n",
    "\n",
    "output = generator.generate_simple(prompt, max_new_tokens = 200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdjzh7UE_KKl"
   },
   "outputs": [],
   "source": [
    "# Generate without LoRA\n",
    "\n",
    "print(\"\")\n",
    "print(\" --- No LoRA -------------- \")\n",
    "print(\"\")\n",
    "\n",
    "generator.lora = None\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "output = generator.generate_simple(prompt, max_new_tokens = 200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bE5b0VBG_pkP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdMpzMaY_pfR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrmFTv-O_pdC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m94CUtyY_pal"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsr-Mqhr_pX_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
