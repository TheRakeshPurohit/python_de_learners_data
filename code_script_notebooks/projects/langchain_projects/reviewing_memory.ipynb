{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4Tv_I7s3fgS"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY']=''"
      ],
      "metadata": {
        "id": "npdgyI2V3sIX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_user_message(\"You Human!\")\n",
        "\n",
        "history.add_ai_message(\"I am more Human?\")\n",
        "\n",
        "#which exposes convienence methods for saving \n",
        "#Human messages, AI messages, and then fetching them all"
      ],
      "metadata": {
        "id": "4T14oazi3sPH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"whats up?\")"
      ],
      "metadata": {
        "id": "YLtIj9Ai3sf6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"hello Genius\"}, \n",
        "                    {\"ouput\": \"I am Genius..\"})"
      ],
      "metadata": {
        "id": "RghidTwmQCiU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRJCczV15A3V",
        "outputId": "fa62f418-7060-4fd8-8c77-631f56433555"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='hi!', additional_kwargs={}),\n",
              "  AIMessage(content='whats up?', additional_kwargs={}),\n",
              "  HumanMessage(content='hello Genius', additional_kwargs={}),\n",
              "  AIMessage(content='I am Genius..', additional_kwargs={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "#Adding memory to chain\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True, \n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "sZ4xHwRc5-Va"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer memory seems to be more versatile\n",
        "### Conversation Buffer memory restricts the memory to window size\n",
        "\n",
        "Both are help in just storage of memory"
      ],
      "metadata": {
        "id": "5KYrwh5BQY4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='How are you?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "XobqAYxW5-T2",
        "outputId": "b6dec944-a660-47f3-9758-7772593a3e85"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: How are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm doing great! I'm feeling very productive today. I've been working on a few projects and I'm excited to see the results. How about you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='Remember, my name is King Kong')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "epLN-rys6cB-",
        "outputId": "186cf459-0864-43c5-8cb7-e85a372b4b70"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: How are you?\n",
            "AI:  I'm doing great! I'm feeling very productive today. I've been working on a few projects and I'm excited to see the results. How about you?\n",
            "Human: Remember, my name is King Kong\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi King Kong! It's nice to meet you. I'm glad to know your name.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='What did I tell you first?')"
      ],
      "metadata": {
        "id": "GHr3BDr-6lfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGv68LGk5-Rq",
        "outputId": "0cbd61f8-b77c-453c-df3d-76edf8e5fc08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: How are you?\\nAI:  I'm doing great! I'm feeling very productive today. I've been working on a few projects and I'm excited to see the results. How about you?\\nHuman: Remember, my name is King Kong\\nAI:  Hi King Kong! It's nice to meet you. I'm glad to know your name.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationEntityMemory"
      ],
      "metadata": {
        "id": "Xcw51plB9Lgz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#memories have their own llm!!!\n",
        "memory = ConversationEntityMemory(llm=llm)\n",
        "\n",
        "_input = {\"input\": \"Nona & Kie are working on a Clay house\"}\n",
        "\n",
        "memory.load_memory_variables(_input)\n",
        "\n",
        "memory.save_context(\n",
        "    _input,\n",
        "    {\"ouput\": \" Play with clay is fun, right?\"}\n",
        ")"
      ],
      "metadata": {
        "id": "7O3j7qDk5-PX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({\"input\":\"who is Kie?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ7i7dgc5-Nh",
        "outputId": "b0735302-4c4a-4d64-99dd-dedaf6cff36b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Nona & Kie are working on a Clay house\\nAI:  Play with clay is fun, right?',\n",
              " 'entities': {'Kie': 'Kie is working on a Clay house with Nona.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationEntityMemory\n",
        "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "G8cFVwj05-Ks"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting entities using special prompt\n",
        "#Note there is llm for the chain and seperately for memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True,\n",
        "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
        "    memory=ConversationEntityMemory(llm=llm)\n",
        ")"
      ],
      "metadata": {
        "id": "W0q_Nxhh5-H7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Kie and Nona are working with clay\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "VrEIHTTv5-Bz",
        "outputId": "9ad2f466-6311-4433-d0f1-d567b3d262d2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Kie': '', 'Nona': ''}\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Last line:\n",
            "Human: Kie and Nona are working with clay\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' That sounds like a fun activity! What kind of clay are they using?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9JZZIPW-KUM",
        "outputId": "cecaaee8-87d4-4d12-88c9-55563ec004a8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Kie': 'Kie is working with clay.', 'Nona': 'Nona is working with clay.'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVqplcyW-KRC",
        "outputId": "0151a5c0-f1c2-46dd-ede3-bd48aafa0b57"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['entities', 'history', 'input'], output_parser=None, partial_variables={}, template='You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{entities}\\n\\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Z0G-mzk-KO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4opOCOk-KKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDgdg07i-KIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhnVlHgy-KDP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}