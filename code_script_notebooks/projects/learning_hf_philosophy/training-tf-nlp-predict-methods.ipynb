{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "**Motivation:** \n",
    "\n",
    "During the practice session with tokenizers, and various Models of AutoModel Instances, I got stuck when trying to convert the raw model output using the tokenizer.decode(). Which made me realize that, depending on the model head, decoding strategy will differ.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForQuestionAnswering, \n",
    "    AutoModelForMultipleChoice\n",
    ")\n",
    "import torch\n",
    "# The above classes provide different heads to the underlying model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "task_arch = {\n",
    "    \"text_classification\":{\n",
    "        \"architectures\":['ALBERT','BART',\n",
    " 'BERT',\n",
    " 'BigBird',\n",
    " 'BigBird-Pegasus',\n",
    " 'BioGpt',\n",
    " 'BLOOM',\n",
    " 'CamemBERT',\n",
    " 'CANINE',\n",
    " 'CodeLlama',\n",
    " 'ConvBERT',\n",
    " 'CTRL',\n",
    " 'Data2VecText',\n",
    " 'DeBERTa',\n",
    " 'DeBERTa-v2',\n",
    " 'DistilBERT',\n",
    " 'ELECTRA',\n",
    " 'ERNIE',\n",
    " 'ErnieM',\n",
    " 'ESM',\n",
    " 'Falcon',\n",
    " 'FlauBERT',\n",
    " 'FNet',\n",
    " 'Funnel Transformer',\n",
    " 'GPT-Sw3',\n",
    " 'OpenAI GPT-2',\n",
    " 'GPTBigCode',\n",
    " 'GPT Neo',\n",
    " 'GPT NeoX',\n",
    " 'GPT-J',\n",
    " 'I-BERT',\n",
    " 'LayoutLM',\n",
    " 'LayoutLMv2',\n",
    " 'LayoutLMv3',\n",
    " 'LED',\n",
    " 'LiLT',\n",
    " 'LLaMA',\n",
    " 'Longformer',\n",
    " 'LUKE',\n",
    " 'MarkupLM',\n",
    " 'mBART',\n",
    " 'MEGA',\n",
    " 'Megatron-BERT',\n",
    " 'Mistral',\n",
    " 'Mixtral',\n",
    " 'MobileBERT',\n",
    " 'MPNet',\n",
    " 'MPT',\n",
    " 'MRA',\n",
    " 'MT5',\n",
    " 'MVP',\n",
    " 'Nezha',\n",
    " 'Nyströmformer',\n",
    " 'OpenLlama',\n",
    " 'OpenAI GPT',\n",
    " 'OPT',\n",
    " 'Perceiver',\n",
    " 'Persimmon',\n",
    " 'Phi',\n",
    " 'PLBart',\n",
    " 'QDQBert',\n",
    " 'Qwen2',\n",
    " 'Reformer',\n",
    " 'RemBERT',\n",
    " 'RoBERTa',\n",
    " 'RoBERTa-PreLayerNorm',\n",
    " 'RoCBert',\n",
    " 'RoFormer',\n",
    " 'SqueezeBERT',\n",
    " 'T5',\n",
    " 'TAPAS',\n",
    " 'Transformer-XL',\n",
    " 'UMT5',\n",
    " 'XLM',\n",
    " 'XLM-RoBERTa',\n",
    " 'XLM-RoBERTa-XL',\n",
    " 'XLNet',\n",
    " 'X-MOD',\n",
    " 'YOSO'],\n",
    "        \"AutoModelClass\": \"AutoModelForSequenceClassification\",\n",
    "        \"dataset\": \"imdb\",\n",
    "        \"model_used\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "    \"token_classification\":{\n",
    "        \"architectures\":['ALBERT',\n",
    " 'BERT',\n",
    " 'BigBird',\n",
    " 'BioGpt',\n",
    " 'BLOOM',\n",
    " 'BROS',\n",
    " 'CamemBERT',\n",
    " 'CANINE',\n",
    " 'ConvBERT',\n",
    " 'Data2VecText',\n",
    " 'DeBERTa',\n",
    " 'DeBERTa-v2',\n",
    " 'DistilBERT',\n",
    " 'ELECTRA',\n",
    " 'ERNIE',\n",
    " 'ErnieM',\n",
    " 'ESM',\n",
    " 'Falcon',\n",
    " 'FlauBERT',\n",
    " 'FNet',\n",
    " 'Funnel Transformer',\n",
    " 'GPT-Sw3',\n",
    " 'OpenAI GPT-2',\n",
    " 'GPTBigCode',\n",
    " 'GPT Neo',\n",
    " 'GPT NeoX',\n",
    " 'I-BERT',\n",
    " 'LayoutLM',\n",
    " 'LayoutLMv2',\n",
    " 'LayoutLMv3',\n",
    " 'LiLT',\n",
    " 'Longformer',\n",
    " 'LUKE',\n",
    " 'MarkupLM',\n",
    " 'MEGA',\n",
    " 'Megatron-BERT',\n",
    " 'MobileBERT',\n",
    " 'MPNet',\n",
    " 'MPT',\n",
    " 'MRA',\n",
    " 'MT5',\n",
    " 'Nezha',\n",
    " 'Nyströmformer',\n",
    " 'Phi',\n",
    " 'QDQBert',\n",
    " 'RemBERT',\n",
    " 'RoBERTa',\n",
    " 'RoBERTa-PreLayerNorm',\n",
    " 'RoCBert',\n",
    " 'RoFormer',\n",
    " 'SqueezeBERT',\n",
    " 'T5',\n",
    " 'UMT5',\n",
    " 'XLM',\n",
    " 'XLM-RoBERTa',\n",
    " 'XLM-RoBERTa-XL',\n",
    " 'XLNet',\n",
    " 'X-MOD',\n",
    " 'YOSO'],\n",
    "        \"AutoModelClass\": \"AutoModelForTokenClassification\",\n",
    "        \"dataset\": \"wnut_17\",\n",
    "        \"model_used\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "    \"question_answering\":{\n",
    "        \"architectures\":['ALBERT',\n",
    " 'BART',\n",
    " 'BERT',\n",
    " 'BigBird',\n",
    " 'BigBird-Pegasus',\n",
    " 'BLOOM',\n",
    " 'CamemBERT',\n",
    " 'CANINE',\n",
    " 'ConvBERT',\n",
    " 'Data2VecText',\n",
    " 'DeBERTa',\n",
    " 'DeBERTa-v2',\n",
    " 'DistilBERT',\n",
    " 'ELECTRA',\n",
    " 'ERNIE',\n",
    " 'ErnieM',\n",
    " 'Falcon',\n",
    " 'FlauBERT',\n",
    " 'FNet',\n",
    " 'Funnel Transformer',\n",
    " 'OpenAI GPT-2',\n",
    " 'GPT Neo',\n",
    " 'GPT NeoX',\n",
    " 'GPT-J',\n",
    " 'I-BERT',\n",
    " 'LayoutLMv2',\n",
    " 'LayoutLMv3',\n",
    " 'LED',\n",
    " 'LiLT',\n",
    " 'LLaMA',\n",
    " 'Longformer',\n",
    " 'LUKE',\n",
    " 'LXMERT',\n",
    " 'MarkupLM',\n",
    " 'mBART',\n",
    " 'MEGA',\n",
    " 'Megatron-BERT',\n",
    " 'MobileBERT',\n",
    " 'MPNet',\n",
    " 'MPT',\n",
    " 'MRA',\n",
    " 'MT5',\n",
    " 'MVP',\n",
    " 'Nezha',\n",
    " 'Nyströmformer',\n",
    " 'OPT',\n",
    " 'QDQBert',\n",
    " 'Reformer',\n",
    " 'RemBERT',\n",
    " 'RoBERTa',\n",
    " 'RoBERTa-PreLayerNorm',\n",
    " 'RoCBert',\n",
    " 'RoFormer',\n",
    " 'Splinter',\n",
    " 'SqueezeBERT',\n",
    " 'T5',\n",
    " 'UMT5',\n",
    " 'XLM',\n",
    " 'XLM-RoBERTa',\n",
    " 'XLM-RoBERTa-XL',\n",
    " 'XLNet',\n",
    " 'X-MOD',\n",
    " 'YOSO'],\n",
    "        \"AutoModelClass\": \"AutoModelForQuestionAnswering\",\n",
    "        \"dataset\": \"squad\",\n",
    "        \"model_used\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "    \"causal_lm\":{\n",
    "        \"architectures\": ['BART',\n",
    " 'BERT',\n",
    " 'Bert Generation',\n",
    " 'BigBird',\n",
    " 'BigBird-Pegasus',\n",
    " 'BioGpt',\n",
    " 'Blenderbot',\n",
    " 'BlenderbotSmall',\n",
    " 'BLOOM',\n",
    " 'CamemBERT',\n",
    " 'CodeLlama',\n",
    " 'CodeGen',\n",
    " 'CPM-Ant',\n",
    " 'CTRL',\n",
    " 'Data2VecText',\n",
    " 'ELECTRA',\n",
    " 'ERNIE',\n",
    " 'Falcon',\n",
    " 'Fuyu',\n",
    " 'GIT',\n",
    " 'GPT-Sw3',\n",
    " 'OpenAI GPT-2',\n",
    " 'GPTBigCode',\n",
    " 'GPT Neo',\n",
    " 'GPT NeoX',\n",
    " 'GPT NeoX Japanese',\n",
    " 'GPT-J',\n",
    " 'LLaMA',\n",
    " 'Marian',\n",
    " 'mBART',\n",
    " 'MEGA',\n",
    " 'Megatron-BERT',\n",
    " 'Mistral',\n",
    " 'Mixtral',\n",
    " 'MPT',\n",
    " 'MusicGen',\n",
    " 'MVP',\n",
    " 'OpenLlama',\n",
    " 'OpenAI GPT',\n",
    " 'OPT',\n",
    " 'Pegasus',\n",
    " 'Persimmon',\n",
    " 'Phi',\n",
    " 'PLBart',\n",
    " 'ProphetNet',\n",
    " 'QDQBert',\n",
    " 'Qwen2',\n",
    " 'Reformer',\n",
    " 'RemBERT',\n",
    " 'RoBERTa',\n",
    " 'RoBERTa-PreLayerNorm',\n",
    " 'RoCBert',\n",
    " 'RoFormer',\n",
    " 'RWKV',\n",
    " 'Speech2Text2',\n",
    " 'Transformer-XL',\n",
    " 'TrOCR',\n",
    " 'Whisper',\n",
    " 'XGLM',\n",
    " 'XLM',\n",
    " 'XLM-ProphetNet',\n",
    " 'XLM-RoBERTa',\n",
    " 'XLM-RoBERTa-XL',\n",
    " 'XLNet',\n",
    " 'X-MOD'],\n",
    "        \"AutoModelClass\":\"AutoModelForCausalLM\",\n",
    "        \"dataset\": \"eli5_category\",\n",
    "        \"model_used\": \"distilgpt2\",\n",
    "    },\n",
    "    \"masked_lm\":{\n",
    "        \"architectures\": ['ALBERT',\n",
    " 'BART',\n",
    " 'BERT',\n",
    " 'BigBird',\n",
    " 'CamemBERT',\n",
    " 'ConvBERT',\n",
    " 'Data2VecText',\n",
    " 'DeBERTa',\n",
    " 'DeBERTa-v2',\n",
    " 'DistilBERT',\n",
    " 'ELECTRA',\n",
    " 'ERNIE',\n",
    " 'ESM',\n",
    " 'FlauBERT',\n",
    " 'FNet',\n",
    " 'Funnel Transformer',\n",
    " 'I-BERT',\n",
    " 'LayoutLM',\n",
    " 'Longformer',\n",
    " 'LUKE',\n",
    " 'mBART',\n",
    " 'MEGA',\n",
    " 'Megatron-BERT',\n",
    " 'MobileBERT',\n",
    " 'MPNet',\n",
    " 'MRA',\n",
    " 'MVP',\n",
    " 'Nezha',\n",
    " 'Nyströmformer',\n",
    " 'Perceiver',\n",
    " 'QDQBert',\n",
    " 'Reformer',\n",
    " 'RemBERT',\n",
    " 'RoBERTa',\n",
    " 'RoBERTa-PreLayerNorm',\n",
    " 'RoCBert',\n",
    " 'RoFormer',\n",
    " 'SqueezeBERT',\n",
    " 'TAPAS',\n",
    " 'Wav2Vec2',\n",
    " 'XLM',\n",
    " 'XLM-RoBERTa',\n",
    " 'XLM-RoBERTa-XL',\n",
    " 'X-MOD',\n",
    " 'YOSO'],\n",
    "        \"AutoModelClass\": \"AutoModelForMaskedLM\",\n",
    "        \"dataset\": \"eli-5\",\n",
    "        \"model_used\": \"distilroberta-base\",\n",
    "    },\n",
    "    \"translation\":{\n",
    "        \"architectures\": ['BART',\n",
    " 'BigBird-Pegasus',\n",
    " 'Blenderbot',\n",
    " 'BlenderbotSmall',\n",
    " 'Encoder decoder',\n",
    " 'FairSeq Machine-Translation',\n",
    " 'GPTSAN-japanese',\n",
    " 'LED',\n",
    " 'LongT5',\n",
    " 'M2M100',\n",
    " 'Marian',\n",
    " 'mBART',\n",
    " 'MT5',\n",
    " 'MVP',\n",
    " 'NLLB',\n",
    " 'NLLB-MOE',\n",
    " 'Pegasus',\n",
    " 'PEGASUS-X',\n",
    " 'PLBart',\n",
    " 'ProphetNet',\n",
    " 'SeamlessM4T',\n",
    " 'SeamlessM4Tv2',\n",
    " 'SwitchTransformers',\n",
    " 'T5',\n",
    " 'UMT5',\n",
    " 'XLM-ProphetNet'],\n",
    "        \"AutoModelClass\": \"AutoModelForSeq2SeqLM\",\n",
    "        \"dataset\": \"opus_books\",\n",
    "        \"model_used\": \"t5-small\",\n",
    "    },\n",
    "    \"summarization\":{\n",
    "        \"architectures\": ['BART',\n",
    " 'BigBird-Pegasus',\n",
    " 'Blenderbot',\n",
    " 'BlenderbotSmall',\n",
    " 'Encoder decoder',\n",
    " 'FairSeq Machine-Translation',\n",
    " 'GPTSAN-japanese',\n",
    " 'LED',\n",
    " 'LongT5',\n",
    " 'M2M100',\n",
    " 'Marian',\n",
    " 'mBART',\n",
    " 'MT5',\n",
    " 'MVP',\n",
    " 'NLLB',\n",
    " 'NLLB-MOE',\n",
    " 'Pegasus',\n",
    " 'PEGASUS-X',\n",
    " 'PLBart',\n",
    " 'ProphetNet',\n",
    " 'SeamlessM4T',\n",
    " 'SeamlessM4Tv2',\n",
    " 'SwitchTransformers',\n",
    " 'T5',\n",
    " 'UMT5',\n",
    " 'XLM-ProphetNet'],\n",
    "        \"AutoModelClass\": \"AutoModelForSeq2SeqLM\",\n",
    "        \"dataset\": \"billsum\",\n",
    "        \"model_used\": \"t5-small\",\n",
    "    },\n",
    "    \"multiple_choice\":{\n",
    "        \"architectures\": ['ALBERT',\n",
    " 'BERT',\n",
    " 'BigBird',\n",
    " 'CamemBERT',\n",
    " 'CANINE',\n",
    " 'ConvBERT',\n",
    " 'Data2VecText',\n",
    " 'DeBERTa-v2',\n",
    " 'DistilBERT',\n",
    " 'ELECTRA',\n",
    " 'ERNIE',\n",
    " 'ErnieM',\n",
    " 'FlauBERT',\n",
    " 'FNet',\n",
    " 'Funnel Transformer',\n",
    " 'I-BERT',\n",
    " 'Longformer',\n",
    " 'LUKE',\n",
    " 'MEGA',\n",
    " 'Megatron-BERT',\n",
    " 'MobileBERT',\n",
    " 'MPNet',\n",
    " 'MRA',\n",
    " 'Nezha',\n",
    " 'Nyströmformer',\n",
    " 'QDQBert',\n",
    " 'RemBERT',\n",
    " 'RoBERTa',\n",
    " 'RoBERTa-PreLayerNorm',\n",
    " 'RoCBert',\n",
    " 'RoFormer',\n",
    " 'SqueezeBERT',\n",
    " 'XLM',\n",
    " 'XLM-RoBERTa',\n",
    " 'XLM-RoBERTa-XL',\n",
    " 'XLNet',\n",
    " 'X-MOD',\n",
    " 'YOSO'],\n",
    "        \"AutoModelClass\": \"AutoModelForMultipleChoice\",\n",
    "        \"dataset\": \"swag\",\n",
    "        \"model_used\": \"bert-base-uncased\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased is used for text_classification\n",
      "distilbert-base-uncased is used for token_classification\n",
      "distilbert-base-uncased is used for question_answering\n",
      "distilgpt2 is used for causal_lm\n",
      "distilroberta-base is used for masked_lm\n",
      "t5-small is used for translation\n",
      "t5-small is used for summarization\n",
      "bert-base-uncased is used for multiple_choice\n"
     ]
    }
   ],
   "source": [
    "for k, v in task_arch.items():\n",
    "    print(f\"{v['model_used']} is used for {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb is used for text_classification\n",
      "Dataset wnut_17 is used for token_classification\n",
      "Dataset squad is used for question_answering\n",
      "Dataset eli5_category is used for causal_lm\n",
      "Dataset eli-5 is used for masked_lm\n",
      "Dataset opus_books is used for translation\n",
      "Dataset billsum is used for summarization\n",
      "Dataset swag is used for multiple_choice\n"
     ]
    }
   ],
   "source": [
    "for k, v in task_arch.items():\n",
    "    print(f\"Dataset {v['dataset']} is used for {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T05:42:25.975091Z",
     "iopub.status.busy": "2024-02-08T05:42:25.974173Z",
     "iopub.status.idle": "2024-02-08T05:42:25.982386Z",
     "shell.execute_reply": "2024-02-08T05:42:25.980823Z",
     "shell.execute_reply.started": "2024-02-08T05:42:25.975047Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Collecting the model architectures according to the tasks\n",
    "with open('task_arch.json', 'w') as mod:\n",
    "    json.dump(task_arch, mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_cls_path = \"distilbert-base-uncased\"\n",
    "distilbert_local_path = \"/home/kamal/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(distilbert_cls_path)\n",
    "tokenizer_local = AutoTokenizer.from_pretrained(distilbert_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4516bfa42c64ddd92599eaaf5cdf93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_model = AutoModelForSequenceClassification.from_pretrained(distilbert_cls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_local_model = AutoModelForSequenceClassification.from_pretrained(distilbert_cls_path,\n",
    "                                                                           device_map='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_text = \"This is a very nice way of getting things done\"\n",
    "text_tokened = tokenizer(classify_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2Label = {0:'negative', 1:'positive'}\n",
    "label2id = {'negative':0, 'positive':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_logits = distilbert_model(**text_tokened).logits\n",
    "predicted_class_id = classification_logits.argmax().item()\n",
    "id2Label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4f5e27ba644af1aa1da6d1cab7ebc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43c763ade744e7aa984cbdc43facad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wnut_17/wnut_17 to /home/kamal/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ba30315bc948de91c467227ceaee2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c9f2b70ee7440b997524c51b613e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf900932a2147079b073660180138bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d36aef1475a4f3bad897cbdbd82f9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ee0f8d2af74deaa469f195d94650ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wnut_17 downloaded and prepared to /home/kamal/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819f8e8db9a04d2ea58e14784422286b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# token classification labels are part of the datasets\n",
    "from datasets import load_dataset\n",
    "wnut = load_dataset('wnut_17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = wnut['train'].features['ner_tags'].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilbert_tokencls_path = \"distilbert-base-uncased\"\n",
    "distilbert_tokencls_path = distilbert_local_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(distilbert_tokencls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T04:06:34.174145Z",
     "iopub.status.busy": "2024-02-08T04:06:34.173048Z",
     "iopub.status.idle": "2024-02-08T04:06:34.184294Z",
     "shell.execute_reply": "2024-02-08T04:06:34.182841Z",
     "shell.execute_reply.started": "2024-02-08T04:06:34.174086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@paulwalk',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'Empire',\n",
       " 'State',\n",
       " 'Building',\n",
       " '=',\n",
       " 'ESB',\n",
       " '.',\n",
       " 'Pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut['train'][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'paul',\n",
       " '##walk',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'empire',\n",
       " 'state',\n",
       " 'building',\n",
       " '=',\n",
       " 'es',\n",
       " '##b',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = wnut['train'][0]\n",
    "tokened_input = tokenizer(example['tokens'], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokened_input['input_ids'])\n",
    "tokens  # additional tokens added in beginning, and at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokened_input.word_ids(batch_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at /home/kamal/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokencls_model = AutoModelForTokenClassification.from_pretrained(distilbert_tokencls_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = tokencls_model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-corporation',\n",
       " 'O',\n",
       " 'B-corporation',\n",
       " 'B-corporation',\n",
       " 'B-corporation',\n",
       " 'B-corporation',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-corporation',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-corporation',\n",
       " 'O',\n",
       " 'B-corporation',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [id2label[t.item()] for t in predictions[0]]\n",
    "predicted_token_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at /home/kamal/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_qa_path = \"distilbert-base-uncased\"\n",
    "distilbert_qa_path = distilbert_local_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(distilbert_qa_path)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(distilbert_qa_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"\"\"BLOOM has 176 billion parameters and can \n",
    "generate text in 46 languages natural languages and 13 programming languages.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors='pt')  \n",
    "# question , context are a single tensor after tokenisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = qa_model(**inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2129,  2116,  4730,  4155,  2515, 13426,  2490,  1029,   102,\n",
       "        13426,  2038, 18561,  4551, 11709,  1998,  2064,  9699,  3793,  1999,\n",
       "         4805,  4155,  3019,  4155,  1998,  2410,  4730,  4155,  1012,   102])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] how many programming languages does bloom'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation >> Uses the generate followed by tokenizer.decode\n",
    "\n",
    "# model_tg = 'gpt2'\n",
    "model_tg = 'distilgpt2'\n",
    "model_tg_local = \"/home/kamal/.cache/huggingface/hub/models--distilgpt2/snapshots/38cc92ec43315abd5136313225e95acc5986876c/\"\n",
    "# model_tokenizer = AutoTokenizer.from_pretrained(model_tg)\n",
    "causal_gpt = AutoModelForCausalLM.from_pretrained(model_tg, resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "50256\n"
     ]
    }
   ],
   "source": [
    "# model_tokenizer.add_special_tokens({'pad_token': '[PAD]'}) \n",
    "# can't do this, as it will raise when decoding the model output\n",
    "print(model_tokenizer.pad_token_id)\n",
    "print(model_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokenizer.pad_token_id = model_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a seed sentence for enormous experiments that will be successful\"\n",
    "# tokened = model_tokenizer(sentence, padding='max_length', max_length=20, return_tensors='pt')\n",
    "tokened = model_tokenizer(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1212,   318,   257,  9403,  6827,   329,  9812, 10256,   326,   481,\n",
       "           307,  4388]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_gpt.can_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_logits = causal_gpt.generate(tokened['input_ids'],\n",
    "                                  max_new_tokens=100, do_sample=True,\n",
    "                                  top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a seed sentence for enormous experiments that will be successful on this task in the next couple of years.\\n\\nThe seed sentences below are designed to reflect an idea on a student\\'s work. A single sentence will only describe the concept of how this work can be executed if not to determine what it will be like.\\nA few ideas to be considered before you write the sentence:\\nIf you don\\'t like the word \"a\", your best idea is to write a single sentence, and then to use the word \"a\", or even a specific'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generation output decoded \n",
    "text_output = model_tokenizer.decode(model_logits[0], skip_special_tokens=True)\n",
    "text_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f4c910d35649d682c47a4c7ece919a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c3e3c535664f67bf3bd774741e8c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49158269bea847d796ab2a4bf6c9a06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e5c832d67e4b4a845f01ccb4146e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbb9682c1ba4f329c7fffb4e10e0ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_masked_path = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_masked_path)\n",
    "model_masked = AutoModelForMaskedLM.from_pretrained(model_masked_path,\n",
    "                                                    resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Milky Way is a <mask> galaxy.\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_masked(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0420, -3.1702,  3.7433,  ..., -1.2430, -2.4243,  1.4616]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Milky Way is a  spiral galaxy.\n",
      "The Milky Way is a  dwarf galaxy.\n",
      "The Milky Way is a  massive galaxy.\n"
     ]
    }
   ],
   "source": [
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "for tokens in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([tokens])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_translation_path = 't5-small'\n",
    "model_t5_local_path = \"/home/kamal/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_translation_path)\n",
    "model_translation = AutoModelForSeq2SeqLM.from_pretrained(model_translation_path, resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model_translation.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Legumes échange leurs ressources en matière d'azote avec les bactéries qui fixent de l'azote.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summarisation_path = 't5-small'\n",
    "model_summarisation_path = model_t5_local_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_summarisation_path)\n",
    "model_summarisation = AutoModelForSeq2SeqLM.from_pretrained(model_summarisation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model_summarisation.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at /home/kamal/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_multi_path = \"bert-base-uncased\"\n",
    "model_bert_local_path = \"/home/kamal/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/\"\n",
    "model_multi_path = model_bert_local_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_multi_path)\n",
    "model_multi = AutoModelForMultipleChoice.from_pretrained(model_multi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"France has a bread law, Le Décret Pain, with strict rules on what is allowed in a traditional baguette.\"\n",
    "candidate1 = \"The law does not apply to croissants and brioche.\"\n",
    "candidate2 = \"The law applies to baguettes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([[prompt, candidate1],\n",
    "                    [prompt, candidate2]], \n",
    "                   return_tensors=\"pt\", padding=True)\n",
    "\n",
    "labels = torch.tensor(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model_multi(**{k: v.unsqueeze(0) for k, v in inputs.items()},\n",
    "                      labels=labels)\n",
    "logits = outputs.logits\n",
    "predicted_class = logits.argmax().item()\n",
    "predicted_class"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
