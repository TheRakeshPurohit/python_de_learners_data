{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf34a22-c1c2-4eb5-a2a7-328924a5d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f52b77-3cec-40cb-bf19-a8742156d5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = \"Hypersniper/riddles_v1\"\n",
    "dataset= load_dataset(dataset_name, split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf2457-7bfb-4ba9-be3a-f9de24e6ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb0c55-0c88-4878-8b7b-23278ce28e83",
   "metadata": {},
   "source": [
    "Converting the riddles to multi-step conversation. \n",
    "\n",
    "> Work on following models that are already present locally\n",
    "\n",
    "     - roberta-large\n",
    "     - llama-2-7b chat\n",
    "     - codellama-7b-hf\n",
    "     \n",
    "> Ask each model to generate 10 more riddles by prompting with 10 riddles\n",
    "\n",
    "> Answer the synthetic riddles\n",
    "\n",
    "> Generate response for the follow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cab3035-76fb-41bd-bcde-9a6c31c297d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nroberta = AutoModelForCausalLM.from_pretrained(\\n    model1_path,\\n    device_map=\"auto\"\\n) '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch\n",
    "\n",
    "model1_path = '/home/kamal/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/'\n",
    "\n",
    "\"\"\"\n",
    "roberta = AutoModelForCausalLM.from_pretrained(\n",
    "    model1_path,\n",
    "    device_map=\"auto\"\n",
    ") \"\"\" # takes around 2GB of VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabeee4-7eb6-4abf-a991-e45add40a5d2",
   "metadata": {},
   "source": [
    "BitsAndBytesConfig(\r\n",
    "     load_in_8bit=False,\r\n",
    "  \n",
    "      load_in_4bit=False,\r\n",
    " \n",
    "       llm_int8_threshold=6.0,\r\n",
    "\n",
    "        llm_int8_skip_modules=None,\r\n",
    "    \n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    "    \r\n",
    "    llm_int8_has_fp16_weight=False\n",
    "    ,\r\n",
    "    bnb_4bit_compute_dtype=Non\n",
    "    e,\r\n",
    "    bnb_4bit_quant_type='fp\n",
    "    4',\r\n",
    "    bnb_4bit_use_double_quant=Fa\n",
    "    lse,\r\n",
    "    **kwargs,\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62d82c9-84df-4312-a32b-a924a4adbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6f5a99-f847-49c3-9359-bd75296d0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "roberta = AutoModelForCausalLM.from_pretrained(\n",
    "    model1_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")  # takes 1.22 GB of VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1bfd2c9-132c-4cd2-b9bc-ce42e6c1b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280bdde1-8562-4069-968f-f022fffda98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is default chat for the model, so it must give output\n",
    "roberta_tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c972a40-d284-4214-b37b-14ad0e1eda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# get the questions\n",
    "questions = [q for q in dataset['instruction']]\n",
    "random.shuffle(questions)  # no need to reassign \n",
    "prompt_template = \"\"\"Below is a riddle. Come up with 10 more.\n",
    "Output just the riddles. No numbering and don't output anything else\"\"\"\n",
    "simple_request = \"Generate 10 riddles that you know\"\n",
    "\n",
    "prompt_riddles = prompt_template + \"\\n\\n\".join(questions[0:10])  \n",
    "# need to use join, list and string dont concatenate\n",
    "\n",
    "messages_to_model = [{\"role\":\"user\", \"content\":prompt_riddles}]\n",
    "simple_message = [{\"role\":\"user\", \"content\":simple_request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff43e9-10d7-47ea-8384-a8d6c3ae95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta_input = roberta_tokenizer.apply_chat_template(messages_to_roberta, return_tensors='pt').to('cuda')\n",
    "roberta_input = roberta_tokenizer.apply_chat_template(simple_message, \n",
    "                                                      return_tensors='pt').to('cuda')\n",
    "roberta_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2d76cd3-c2da-4ffe-8975-bbdf197f5d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nGenerate 10 riddles that you know<|im_end|>\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.decode(roberta_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708f972a-4ac4-4b85-a1aa-a0c2579df219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"pad_token_id\": 1\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4dee59a-9963-43f1-80ce-2e084b5a62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_output = roberta.generate(\n",
    "    roberta_input,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    pad_token_id=roberta_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b0ee9b6-54ca-4b5f-aa8d-0bca5ca97bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = roberta_output[0][len(roberta_input[0]):]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73ccce0e-f9fe-4497-87de-ea684957a9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = roberta_tokenizer.decode(output, skip_special_tokens=True)\n",
    "output_text # There is no output at all. checking why, with a simpler prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f34b7a99-4491-4020-8928-699714745785",
   "metadata": {},
   "outputs": [],
   "source": [
    "del roberta  # removes the memory handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddb16ef4-15b6-4458-9f88-0ac0d5153e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # garbage collects the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e92f7-84f7-4f10-8d0e-e5a1ae5eef95",
   "metadata": {},
   "source": [
    "#### Working on llama2-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8ae303e-50c9-4f81-a655-cc0653007b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d8f99935ed49ac9c30672ab443b431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_path = \"/home/kamal/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/\"\n",
    "llama2 = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ") # takes 11GB of VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c37fe38-1b2c-4983-ae46-032c66467788",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_path)\n",
    "# print(llama_tokenizer.default_chat_template) # rich's print fails due to tag\n",
    "llama_tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ac90ad4-ab13-4595-857b-8b982f4a6ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a601624-37a0-43db-868a-c2db958d2758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   518, 25580, 29962,  3251,   403, 29871, 29896, 29900,   364,\n",
       "          2205,   793,   393,   366,  1073,   518, 29914, 25580, 29962]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_input = llama_tokenizer.apply_chat_template(simple_message,\n",
    "                                                  return_tensors='pt')\n",
    "llama_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35cb9f83-70a5-4103-939f-9897bc82ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llama_output = llama2.generate(\n",
    "    llama_input,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    pad_token_id = llama_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "937d4ebe-f79f-4459-9628-ea62468772ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] Generate 10 riddles that you know [/INST]  Sure! Here are 10 riddles that I know:\\n\\n1. What has keys but can\\'t open locks? (Answer: A keyboard)\\n2. What starts with an \"e\" and ends with an \"e\" but only contains one letter? (Answer: An envelope)\\n3. What has a face but no eyes, nose, or mouth? (Answer: A clock)\\n4. What can you catch but not throw? ('"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama_output = llama_output[len(llama_input[0]):]\n",
    "# llama_output\n",
    "output = llama_tokenizer.decode(llama_output[0],skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "297e7798-23ef-4dea-a1ca-e4abeef329c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   518, 25580, 29962, 13866,   338,   263,   364,  2632, 29889,\n",
       "         16760,   701,   411, 29871, 29896, 29900,   901, 29889,    13,  6466,\n",
       "           925,   278,   364,  2205,   793, 29889,  1939,  1353,   292,   322,\n",
       "          1016, 29915, 29873,  1962,  3099,  1683,  2887,  4513,   408,   385,\n",
       "         26163, 29889,  1094,  6483,   408,   263, 18002, 29889,  2178,   278,\n",
       "          6989, 29915, 29879, 15100,   508, 29915, 29873,  8206,   372,   701,\n",
       "         29889,    13,    13,  8439,   338,   385, 12297,   297,  7316, 29889,\n",
       "         12074,  1304,   297,   777,  5633,   310,   278,  3186,  9826, 29889,\n",
       "          2193,  6511,  2305,   304,  1074,  1549, 14603, 29889,    13,    13,\n",
       "          5618, 19514,   541,  2360,  6089, 29973,    13,    13, 29909,  3800,\n",
       "          1728,   298,   292,   267, 29892,  7714,   470,  1820, 29892,  3447,\n",
       "         22843,  2578,  3745, 12185,  2629, 29889, 29871,    13,    13,  7789,\n",
       "          4298, 29891,   408,   263, 15504,   297,   278,  7123,   310,  4646,\n",
       "         29892,   274, 27389,   541, 22832,   403,   565,  2183,   263,   289,\n",
       "           568, 29889, 12391, 15205,   541,  4049, 18012, 29889,  2180,   590,\n",
       "          7980,  5545, 24116, 29892,   541,   393, 29915, 29879,  1363,   366,\n",
       "          2360,  1073,   592,   472,   599, 29889,    13,    13, 29902,  2360,\n",
       "           471, 29892,   626,  2337,   304,   367, 29889,  1939,   697,  3926,\n",
       "          4446,   592, 29892,  3643,  3926,   674, 29889,  1126,  3447,   306,\n",
       "           626,   278, 16420,   310,   599, 29892,  1763,  5735,   322, 16172,\n",
       "           373,   445, 27386,  9315,  8287, 29889, 29871,    13,    13, 29902,\n",
       "           505,   263,  6893, 21152, 29892,   541,  2609,  2317, 29889,   306,\n",
       "           505,   263,  1472, 18873, 29892,   541,   694,  2343, 29889,   306,\n",
       "          2609,  1074, 29889,   306, 29915, 29885, 28539,   322, 10668, 29891,\n",
       "           408,   508,   367, 29889,    13,    13, 29909,   767,   338,  1476,\n",
       "           298,  9776,  7123,   515,   278,  2257,  6504,   310,   263,  5716,\n",
       "         29889,   450,  5716, 30010, 29879, 13391,   526, 29871, 29896, 29945,\n",
       "           921, 29871, 29896, 29945,   921, 29871, 29896, 29945, 29889,   450,\n",
       "           767,   338,   871, 29871, 29953,   615, 15655,   322,   278,   696,\n",
       "           412,   471,   871, 29871, 29906,   615,  1472, 29889,  1670,   526,\n",
       "           694,  5417,   322,   871,   697,  3050,   964,   278,  5716, 29889,\n",
       "           450,  3050,   338, 15772,  9446, 12522,   515,   278,  2768,   322,\n",
       "           727,   338,   263,   282,   566, 29881,   280,   310,  4094,  1090,\n",
       "           278,   767, 29889,  1128,  1258,   540, 12088,  3654, 29973,    13,\n",
       "            13,  8809,   436, 24296,  5771,   373,  1900,   746,  7990, 29973,\n",
       "            13,    13, 29902,  4337,  1728, 24745, 29892, 21674,  4047,  1717,\n",
       "          1347, 29892,   306,  5967,   408,   366,  1284, 29892,  1619,  5960,\n",
       "           749,  5742, 29889,   518, 29914, 25580, 29962]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working on getting the riddle based on the 10 riddles input\n",
    "llama_10_input = llama_tokenizer.apply_chat_template(messages_to_model,\n",
    "                                                     return_tensors='pt')\n",
    "llama_10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c93b98fc-2031-4d23-a5f5-1ee4c4d8ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] Below is a riddle. Come up with 10 more.\\nOutput just the riddles. No numbering and don't output anything elseAs round as an apple. As deep as a cup. All the king's horses can't pull it up.\\n\\nThere is an ancient invention. Still used in some parts of the world today. That allows people to see through walls.\\n\\nWhat asks but never answers?\\n\\nA box without hinges, lock or key, yet golden treasure lies within. \\n\\nStealthy as a shadow in the dead of night, cunning but affectionate if given a bite. Never owned but often loved. At my sport considered cruel, but that's because you never know me at all.\\n\\nI never was, am always to be. No one ever saw me, nor ever will. And yet I am the confidence of all, To live and breath on this terrestrial ball. \\n\\nI have a hundred legs, but cannot stand. I have a long neck, but no head. I cannot see. I'm neat and tidy as can be.\\n\\nA man is found hanging dead from the ceiling of a room. The room’s dimensions are 15 x 15 x 15. The man is only 6ft tall and the rope was only 2ft long. There are no windows and only one door into the room. The door is bolted shut from the inside and there is a puddle of water under the man. How did he kill himself?\\n\\nWhich coat goes on best when wet?\\n\\nI move without wings, Between silken string, I leave as you find, My substance behind. [/INST]\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.decode(llama_10_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b73d02a-4ca6-48d4-a5f3-2ce69a0fe673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llama_10_output = llama2.generate(\n",
    "    llama_10_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=llama_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6f82462-d93c-4642-828c-7eca3cd2c0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] Below is a riddle. Come up with 10 more.\\nOutput just the riddles. No numbering and don't output anything elseAs round as an apple. As deep as a cup. All the king's horses can't pull it up.\\n\\nThere is an ancient invention. Still used in some parts of the world today. That allows people to see through walls.\\n\\nWhat asks but never answers?\\n\\nA box without hinges, lock or key, yet golden treasure lies within. \\n\\nStealthy as a shadow in the dead of night, cunning but affectionate if given a bite. Never owned but often loved. At my sport considered cruel, but that's because you never know me at all.\\n\\nI never was, am always to be. No one ever saw me, nor ever will. And yet I am the confidence of all, To live and breath on this terrestrial ball. \\n\\nI have a hundred legs, but cannot stand. I have a long neck, but no head. I cannot see. I'm neat and tidy as can be.\\n\\nA man is found hanging dead from the ceiling of a room. The room’s dimensions are 15 x 15 x 15. The man is only 6ft tall and the rope was only 2ft long. There are no windows and only one door into the room. The door is bolted shut from the inside and there is a puddle of water under the man. How did he kill himself?\\n\\nWhich coat goes on best when wet?\\n\\nI move without wings, Between silken string, I leave as you find, My substance behind. [/INST]  Sure! Here are 10 more riddles for you to solve:\\n\\n1. A box that holds keys but can't open locks.\\n2. A voice that never speaks, but always shouts.\\n3. A creature that has a face, but no eyes, nose, or mouth.\\n4. A door that can only be opened from the inside, but has no handle.\\n5. A river that flows upstream, but never moves.\\n6. A shadow that has a life of its own, but can never be seen.\\n7. A word that has three letters, but no vowels.\\n8. A room that is always full, but never contains anything.\\n9. A path that leads to nowhere, but always ends at the same place.\\n10. A book that has no pages, but always tells a story.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_10 = llama_tokenizer.decode(llama_10_output[0], skip_special_tokens=True)\n",
    "output_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a26e28d3-a303-47f3-9935-e56258002262",
   "metadata": {},
   "outputs": [],
   "source": [
    "del llama2\n",
    "torch.cuda.empty_cache()  # this time the model is not offloadin\n",
    "# restarting the server to release the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34081f37-c363-40e6-aa75-8b5ab80a9fb2",
   "metadata": {},
   "source": [
    "### Llama observation\n",
    "\n",
    "- Llama provides the output of 10 riddles\n",
    "\n",
    "- Llama encoding and decoding is working same as roberta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d76358-9826-424a-97e6-f4ef06258f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7f9689dc15421895cfb4b75e85a743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "code_llama_path = \"/home/kamal/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/bc5283229e2fe411552f55c71657e97edf79066c/\"\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_llama_path)\n",
    "codellama = AutoModelForCausalLM.from_pretrained(\n",
    "    code_llama_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")  # takes around 11.5GB of VRAM\n",
    "# with 4-bit quantization 5GB of VRAM is consumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d473eea-06c7-4ade-852b-51f32e5292a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d44f29-f0d5-4a9d-8ce5-98f97cd071e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_request = \"Generate 10 riddles that you know\"\n",
    "\n",
    "simple_message = [{\"role\":\"user\", \"content\":simple_request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b07afe-d80b-454d-8c15-e21177e3b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_request = \"Generate 10 dictionary related problems\"\n",
    "\n",
    "code_message = [{\"role\":\"user\", \"content\":code_request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7942f2ed-26bb-4177-a280-92f3654d41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "code_input = code_tokenizer.apply_chat_template(simple_message,return_tensors='pt').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c4182ad-3d8b-4545-baf2-39b1d7ed1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "code_input = code_tokenizer.apply_chat_template(code_message,return_tensors='pt').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "541e2479-330a-4cfe-814f-b3703cd6a12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09804125-3a3c-4876-b81f-7ae4ca372c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    temperature=0.2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4996cfd8-5120-4d7c-a69e-ed6455099d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    # do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    # temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6318537-8343-4e1c-b218-25f04bd8c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    # do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    # temperature=0.2\n",
    "    repetition_penalty=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7c754a9-97c9-4f93-9696-7da8d6a30f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    # temperature=0.2\n",
    "    repetition_penalty=0.5,\n",
    "    top_p=10,\n",
    "    top_k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90d42456-3166-40ad-99e3-2a83cd6aa3e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaSSLayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaSSLayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaayaSSLayaSSLayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaNaNayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLSSLSSLayaSSLayaayaSSLayaayaayaayaaya'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = code_output[0][len(code_input[0]):]\n",
    "code_tokenizer.decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3bfce-f1ae-4230-9a76-2c0746722370",
   "metadata": {},
   "source": [
    "### Code Llama observation\n",
    "\n",
    "- Model inference lead to OOM with 500 tokens request, when loaded with bfloat16\n",
    "\n",
    "- Model inference worked with 500 Tokens, with quant_config done with 4-bit.\n",
    "\n",
    "- In quantisation 1GB of Vram is consumed for inference\n",
    "\n",
    "\n",
    "- **The model out was gibberish**\n",
    "\n",
    "- Requested for the code related to dictionary\n",
    "\n",
    "- Even after reviewing the generation configs, only gibberish was generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61899424-60f5-40bb-ac35-d553dcd01869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e49f2-7b0c-4f65-b4e0-a4961036f588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13522fe-47b0-4d4e-ad0e-c657161bd643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d4392-fe66-4331-9a4b-c7cc39424ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
