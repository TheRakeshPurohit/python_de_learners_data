{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers library model falls into one of the below categories\n",
    "\n",
    "- Autoregressive-models : Correspond to the decoder of the transformer model\n",
    "\n",
    "- Autoencoding-models : Same as the encoder in transformer model\n",
    "\n",
    "**Same arch** can be used as Autoregressive and AutoEncoding model\n",
    "\n",
    "- seq-to-seq models : Use both encoder and decoder of the original transformer \n",
    "\n",
    "- multimodal models\n",
    "\n",
    "- retrieval-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models_tasks = {\n",
    "    \"autoregressive\": [{\"model\":\"originalGPT\",\n",
    "                        \"datset\":\"Book Corpus\",\n",
    "                        \"tasks\":[\"language modelling\", \"multitask LM\", \"multichoice classification\"],\n",
    "                        \"special_change\": None,\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/gpt\",\n",
    "                        }, \n",
    "                        {\"model\":\"GPT2\",\n",
    "                        \"dataset\": \"WebText\",\n",
    "                        \"tasks\":[\"language modelling\", \"multitask LM\", \"multichoice classification\"],\n",
    "                        \"special_change\": None,\n",
    "                        \"link\": \"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/gpt2\",\n",
    "                        }, \n",
    "                        {\"model\": \"CTRL\",\n",
    "                        \"dataset\": \"WebText\",\n",
    "                        \"tasks\": [\"language modelling\"],\n",
    "                        \"special_change\": [\"Introduces Control Code\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/ctrl\",\n",
    "                        },\n",
    "                        {\"model\": \"TransformerXL\",\n",
    "                        \"dataset\": \"BookCorpus\",\n",
    "                        \"tasks\": [\"language modelling\"]\n",
    "                        \"special_change\": [\"Recurrence Mechanism\", \"Positional Relative Embedding\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/transformerxl\",\n",
    "                        },\n",
    "                        {\"model\": \"Reformer\",\n",
    "                        \"dataset\": \"Unknown\",\n",
    "                        \"tasks\":[\"language modelling\"]\n",
    "                        \"special_change\":[\"Axial position embedding\",\n",
    "                        \"local sensitive hashing attention\",\n",
    "                        \"Feed forward by chunks, not batches\",\n",
    "                        \"Intermediate results not Stored\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/reformer\",\n",
    "                        },\n",
    "                        {\"model\":\"XLNet\",\n",
    "                        \"dataset\": \"Unknown\",\n",
    "                        \"tasks\":[\"language modelling\", \"token classification\", \n",
    "                        \"sentence classification\", \"multi-choice classification\", \n",
    "                        \"question answering\"],\n",
    "                        \"special_change\":[\"not traditional AR model\", \"builds on training strategy\",],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/xlnet\",\n",
    "                        }],\n",
    "    \"autoencoding\": [{\"model\":\"BERT(bidirectional encoding representation from Transformers)\",\n",
    "                        \"dataset\":[\"brown corpus\", \"wikipedia\"],\n",
    "                        \"tasks\":[\"language modelling\", \"mask LM\", \"multichoice classification\",\n",
    "                        \"sentence classification\", \"token classification\", \"question answering\",\n",
    "                        \"next sentence prediction\"],\n",
    "                        \"special_change\": None,\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/bert\",\n",
    "                        },\n",
    "                      {\"model\":\"ALBERT\",\n",
    "                        \"dataset\":[\"brown corpus\", \"wikipedia\"],\n",
    "                        \"tasks\":[\"language modelling\", \"mask LM\", \"multichoice classification\",\n",
    "                        \"sentence classification\", \"token classification\", \"question answering\",\n",
    "                        \"next sentence prediction\"],\n",
    "                        \"special_change\": [\"Embedding size is different\", \"next sentence pred is replaced with sentence ordering prediction\",\n",
    "                        \"Layers are split in groups\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/albert\",\n",
    "                        },\n",
    "                        {\"model\":\"RoBERTa\",\n",
    "                        \"dataset\":[\"brown corpus\", \"wikipedia\"],\n",
    "                        \"tasks\":[\"mask LM\", \"multichoice classification\",\n",
    "                        \"sentence classification\", \"token classification\", \"question answering\",],\n",
    "                        \"special_change\":[\"Dynamic Masking\", \"no NSP\", \"larger train batches\", \"BPE\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/roberta\",\n",
    "                        },\n",
    "                        {\"model\":\"DistilBert\",\n",
    "                        \"dataset\":[\"brown corpus\", \"wikipedia\"],\n",
    "                        \"tasks\":[\"mask LM\",\"sentence classification\", \"token classification\", \"question answering\",],\n",
    "                        \"special_change\": [\"trained by distillation of BERT\", \"predicting masked token\",\n",
    "                                        \"cos similarity between student and teacher model\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/distilbert\",\n",
    "                        },\n",
    "                        {\"model\":\"ConvBERT\",\n",
    "                        \"datset\":\"Unknown\",\n",
    "                        \"tasks\":[\"multitask LM\", \"token classification\", \"sentence classification\"],\n",
    "                        \"special_change\": [\"attention heads replaced with span-based dynamic conv\", \n",
    "                                            \"helps to get local dependencies\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/convbert\", \n",
    "                        },\n",
    "                        {\"model\":\"cross-ling language model XLM\",\n",
    "                        \"datset\":\"Unknown\",\n",
    "                        \"tasks\":[\"language modelling\", \"multitask LM\", \"multichoice classification\"],\n",
    "                        \"special_change\": [\"Causal LM AutoRegressive training\", \"MLM with 256 tokens\",\n",
    "                                            \"MLM and TLM with random masking\", \"bi-lingual\"]\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/xlm\",\n",
    "                        },\n",
    "                        {\"model\":\"XLM-Roberta\",\n",
    "                        \"datset\":\"Unknown\",\n",
    "                        \"tasks\":[\"multitask LM\", \"token classification\", \"sentence classification\", \n",
    "                                \"multi-choice QA\", \"QA\"],\n",
    "                        \"special_change\": [\"Using RoBERTa trick on XLM\",\"no TLM objective\", \"100 langs trained\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/xlmroberta\",\n",
    "                        },\n",
    "                        {\"model\":\"FlauBERT\",\n",
    "                        \"datset\":[\"book corpus\", \"wiki pedia\"],\n",
    "                        \"tasks\":[\"language modelling\", \"sentence classification\"],\n",
    "                        \"special_change\": [\"no sentence order prediction\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/flaubert\",\n",
    "                        },\n",
    "                        {\"model\":\"ELECTRA\",\n",
    "                        \"datset\":\"inputs from another model\",\n",
    "                        \"tasks\":[\"masked language modelling\", \"sentence classification\",\"token classification\"],\n",
    "                        \"special_change\": [\"training is similar to GAN\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/electra\",\n",
    "                        },\n",
    "                         {\"model\":\"Funnel Transformer\",\n",
    "                        \"datset\":\"unknown\",\n",
    "                        \"tasks\":[\"masked language modelling\", \"sentence classification\",\n",
    "                                \"token classification\",\"multi QA\", \"QA\"],\n",
    "                        \"special_change\": [\"transformer with pooling\", \"layers in blocks\",\n",
    "                                            \"final seq 1/4th of original\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/funnel\",\n",
    "                        },\n",
    "                        {\"model\":\"LongFormer\",\n",
    "                        \"datset\":['book corpus', 'wikipedia'],\n",
    "                        \"tasks\":[\"mask LM\", \"multichoice classification\",\n",
    "                        \"sentence classification\", \"token classification\", \"question answering\",],\n",
    "                        \"special_change\": [\"replace attn mat with sparse mat to go faster\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/longformer\",\n",
    "                        },],\n",
    "    \"seq-2-seq\":[{\"model\":\"BART\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"conditional generation\", \"sequence classification\"],\n",
    "                        \"special_change\": [\"Encoder Decoder model\",\"multiple transformation involved\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/bart\",\n",
    "                        },\n",
    "                  {\"model\":\"Pegasus\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"conditional generation\", \"summarisation\"],\n",
    "                        \"special_change\": [\"pretrained for Gap Seq Gen\",\n",
    "                                        \"pretrained for masked language modelling\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/pegasus\",\n",
    "                        },      \n",
    "                  {\"model\":\"MarianMT in C++\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"conditional generation\"],\n",
    "                        \"special_change\": [\"Modeled in C++\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/marian\",\n",
    "                        },\n",
    "                  {\"model\":\"T5\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"conditional generation\"],\n",
    "                        \"special_change\": [\"supervised & self supervised training\",\n",
    "                                            \"Glue / SuperGLUE tasks are converted to text to text\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/t5\",\n",
    "                        },\n",
    "                  {\"model\":\"MT5\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"conditional generation\"],\n",
    "                        \"special_change\": [\"self supervised training\",\n",
    "                                            \"Trained on 101 langs\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/mt5\",\n",
    "                        },\n",
    "                 {\"model\":\"MBART\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"conditional generation\"],\n",
    "                        \"special_change\": [\"same as BART\",\n",
    "                                            \"Trained on 25 langs\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/mbart\",\n",
    "                        },\n",
    "                 {\"model\":\"prophetnet\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"conditional generation\", \"summarization\"],\n",
    "                        \"special_change\": [\"novel seq2seq pretraining\",\n",
    "                                            \"predicts next n tokens based on prev n tokens\",\n",
    "                                            \"main and n-stream self attention\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/prophetnet\",\n",
    "                        },\n",
    "                 {\"model\":\"xlm-prophetnet\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"multi-lingual conditional generation\", \"question generation\", \"headline generation\"],\n",
    "                        \"special_change\": [\"same as prophetnet\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/xlmprophetnet\",\n",
    "                        },],\n",
    "    \"MultiModal\":[{\"model\":\"MMBT\",\n",
    "                   \"dataset\":\"Unknown\",\n",
    "                   \"tasks\":[\"classification\"],\n",
    "                   \"special_change\":[\"tokenized text + final activation of resnet image\",\n",
    "                                    \"segment embedding to tell text and image difference\"],\n",
    "                    \"link\":\"https://arxiv.org/abs/1909.02950\"\n",
    "                    }],\n",
    "    \"Retrieval Based\":[\n",
    "                  {\"model\":\"Dense Passage Retrieval\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"Not implemented\"],\n",
    "                        \"special_change\": [\"Question Encoder\", \"Context Encoder\", \"reader\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/dpr\",\n",
    "                        },\n",
    "                        {\"model\":\"Retrieval Aug Gen for Knowledge Intensive Tasks\",\n",
    "                        \"dataset\":\"unknown\",\n",
    "                        \"tasks\":[\"generation\", \"summarization\"],\n",
    "                        \"special_change\": [\"RAG extracts the text\",\n",
    "                        \"seq2Seq model reads and generates output\",\n",
    "                        \"RAG-Token and RAG-Sequence\"],\n",
    "                        \"link\":\"https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/rag\",\n",
    "                        },]                       \n",
    "},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
