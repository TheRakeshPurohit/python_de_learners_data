{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30784d0d-4786-4667-a5bb-6c28116d4cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.38.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb605c6-2cfa-4e9b-877e-8c9dcf837fed",
   "metadata": {},
   "source": [
    "##### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf912d66-27aa-4bb9-919f-beadf1d3b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Dataset,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PromptEncoderConfig,\n",
    "    PrefixTuningConfig,\n",
    "    IA3Config,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PromptTuningInit,\n",
    "    PromptTuningConfig,\n",
    "    prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c57ef6-80c0-4f58-a084-230e4cd4bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# model_path = \"google/gemma-2b-it\"\n",
    "# model_path = \"t5-large\"  # is a seq2seqlm\n",
    "# resorting to below model, since gemma_2B was creating issues \n",
    "# with lora_config. \n",
    "model_path = \"mistralai/Mistral-7B-v0.1\" \n",
    "# model_path = \"bigscience/bloomz-560m\" \n",
    "lr = 1e-3\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "steps = 150\n",
    "\n",
    "save_location = \"/home/aicoder/training/multi_mistral_models\"\n",
    "# loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8209f7-093d-49a6-ad32-bb2f0bb2600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model no-quantisation, device_map=\"auto\" takes 10.25GB VRAM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791de8df-14f2-4ce2-a452-9f5d29f6b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating BnB confing for 8-bit loading\n",
    "bnb_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "bnb_4bit = BitsAndBytesConfig(load_in_4bit=True)\n",
    "bnb_4bit_nested = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                    bnb_4bit_use_double_quant=True)\n",
    "bnb_4bit_nested_nf4 = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_quant_type='nf4',\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39aae186-9202-44bb-bbf4-8ef4a543d113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd7ebe8e5504cef94988e47e95ed6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading model 8-bit quantisation device_map=auto takes 3.62 GB VRAM\n",
    "# loading mistral model 8-bit quantisation device_map=auto takes 8 GB VRAM\n",
    "# training fails due to multiple issues, scroll down\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             # load_in_8bit=True, depricated,\n",
    "                                             quantization_config=bnb_8bit,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c72097-d1ca-4727-b0cd-3ce23b6c7d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225aaf6dd19f43fba5b67936e0ab9d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading model 4-bit quantisation device_map=auto takes 2.80 GB VRAM\n",
    "# loading mistral model 4-bit quantisation device_map=auto takes 5.2 GB VRAM\n",
    "# training doesn't works with mistral_7B model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             # load_in_4bit=True, depricated,\n",
    "                                             quantization_config=bnb_4bit,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4543ea22-e0a3-4f5f-a1ac-99cae63ef68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa0b7c356c04141acee51cc3401f87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading model 4-bit quantisation with nesting, device_map=auto takes 2.70 GB VRAM\n",
    "# did not try with mistral\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             # load_in_4bit=True, depricated,\n",
    "                                             quantization_config=bnb_4bit_nested,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3d8cb1-3c96-4585-9f43-11023791d9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efbb1ae58d84d618f06543fadeeacf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading model 4-bit quantisation with nesting & nf4, device_map=auto takes 2.70 GB VRAM\n",
    "# loading mistral model 4-bit quantisation device_map=auto takes 4.7 GB VRAM\n",
    "# training throws trying to unscale fp16 error, change to bf16=True in t_args\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             # load_in_4bit=True, depricated,\n",
    "                                             quantization_config=bnb_4bit_nested_nf4,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8c239f-3d13-4144-a71c-e73f236fdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt(prompt, your_model):\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt')\n",
    "    logits = your_model.generate(**input_ids.to(device),\n",
    "                           max_new_tokens=50)\n",
    "    return tokenizer.decode(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef41cd43-24cc-43fc-997f-1f778c3d0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the zodiac signs of the month of May?\n",
      "\n",
      "The zodiac signs of the month of May are Taurus, Gemini, Cancer, Leo, Virgo, Libra, Scorpio, Sagittarius, Capricorn, Aquarius\n"
     ]
    }
   ],
   "source": [
    "# Testing the loaded model\n",
    "# DONT RUN THIS WHEN PLANNING TO TRAIN\n",
    "input_text = \"What is the zodiac signs\"\n",
    "print(test_prompt(input_text,model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4093fe0-6dc7-4b3d-b947-d0c3ad084c81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Moving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719017fa-ea96-4c7c-93a0-283558966284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try moving the model back to cpu \n",
    "# will still keep the gpu memory\n",
    "# trying this with quantised model will throw error\n",
    "cpu_model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852459c6-351f-43c2-9d60-48af2b29499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try deleting quantised model & releasing memory will also not work\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b3b97e-adb4-4c9a-9fed-32bedfe1e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the memory will be released from VRAM.\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60e55f-d107-4b3e-8c68-6337e77d6e10",
   "metadata": {},
   "source": [
    "##### Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e129007b-0015-4b89-89fc-db82bf9c8c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fe1e08e84c4a68ba4ae990b2081c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5871\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1468\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preping the dataset for training.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('/home/aicoder/gitfolder/python_de_learners_data/code_script_notebooks/projects/learning_hf_philosophy/databricks-dolly-15k.jsonl') as file:\n",
    "    for line in file:\n",
    "        features = json.loads(line)\n",
    "        # Filter out examples with context, to keep it simple.\n",
    "        if features[\"context\"]:\n",
    "            continue\n",
    "        # Format the entire example as a single string.\n",
    "        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "        data.append(template.format(**features))\n",
    "\n",
    "data_500 = [x for x in data if len(x) <= 500]\n",
    "json_ds = Dataset.from_pandas(pd.DataFrame(data=data_500))\n",
    "\n",
    "# Add tokens <|im_start|> and <|im_end|>, latter is special eos token \n",
    "# tokenizer.pad_token = \"</s>\"\n",
    "# tokenizer.add_tokens([\"<|im_start|>\"])\n",
    "# tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "def tokenize_instruction(row):\n",
    "    # print(row)\n",
    "    input_token = tokenizer(row['0'], padding=True,\n",
    "                            truncation=True, max_length=512)\n",
    "    input_token['labels'] = input_token['input_ids'].copy()\n",
    "    return input_token\n",
    "\n",
    "dataset_tokenized = json_ds.map(tokenize_instruction,\n",
    "                                remove_columns=[\"0\"],\n",
    "                                batched=True)\n",
    "\n",
    "process_ds = dataset_tokenized.train_test_split(test_size=0.2)\n",
    "\n",
    "process_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4073ac0-804f-49ea-839c-596d632be267",
   "metadata": {},
   "source": [
    "##### Model size details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da622d09-4426-4efb-b9ad-96f65f7e261c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4551360512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviewing model footprint, use this for all models\n",
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3c82f5-5ae0-428d-a3d0-e6ca6639a337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd4d7c-47f0-4bb9-abbc-17fe441edd9b",
   "metadata": {},
   "source": [
    "### Run below cells when doing PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db736d6-29f2-4c92-b4aa-6a2437d3a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Low Ranking Adapters and checking trainable parameters\n",
    "lora_config = config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=16, \n",
    "    target_modules = ['q_proj', 'k_proj',  'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'],\n",
    "    # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj',],\n",
    "    # target_modules = ['q_proj', 'k_proj',],\n",
    "    # target_modules = ['self_attn',],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\t\t# needed because we added new tokens to tokenizer/model\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRA decomposes the weight update matrix into two smaller matrices. \n",
    "# The size of these low-rank matrices is determined by its rank or r. \n",
    "# A higher rank means the model has more parameters to train, but it also means the model has \n",
    "# more learning capacity. \n",
    "# Specify the target_modules which determine where the smaller matrices are inserted. \n",
    "# Other important parameters to set are lora_alpha (scaling factor), \n",
    "# bias (whether none, all or only the LoRA bias parameters should be trained), \n",
    "# and modules_to_save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976a817f-6fbb-4289-a253-4eb44283ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model = prepare_model_for_int8_training(model) # deprecated\n",
    "# will take 2.0 GB of VRAM\n",
    "# don't use this\n",
    "# https://huggingface.co/docs/peft/v0.8.2/en/package_reference/peft_model#peft.prepare_model_for_kbit_training\n",
    "prep_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce9cdce-440c-4d4a-9fd1-9b5104bc8108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes > 7.0 GB of VRAM & with r = 16 throws OOM when using 8-bit prepped model\n",
    "# Takes ~ 4.0 GB of VRAM & with r = 16 throws when using 8-bit no_prep model\n",
    "# Takes ~ 7.0 GB of VRAM & with r = 16 works when using 4-bit prep model\n",
    "# Takes ~ 3.0 GB of VRAM & with r = 16 works when using 4-bit no-prep model\n",
    "# lora_model = get_peft_model(prep_model, lora_config)\n",
    "lora_model_noprep = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb52f06-f854-4363-b790-7e5aa5166a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 304,087,040 || all params: 7,545,819,136 || trainable%: 4.029874484391565\n"
     ]
    }
   ],
   "source": [
    "# lora_model.print_trainable_parameters()\n",
    "lora_model_noprep.print_trainable_parameters()  # trainable is 1.Mil params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeca6e6-040b-4866-accb-fa67a14bfc32",
   "metadata": {},
   "source": [
    "Errors:\n",
    "\n",
    "Value Error: Attempting to Unscale FP Gradients\n",
    "\n",
    "- issue seems to be due to float16 https://github.com/huggingface/transformers/issues/23165\n",
    "\n",
    "- https://discuss.pytorch.org/t/valueerror-attemting-to-unscale-fp16-gradients/81372/18\n",
    "\n",
    "Solution was to use the plain 4-bit quantisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effbb7b0-a19d-4311-91ba-5df7bd7415bc",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d56c297-c2db-4ef0-9793-6f0cbcd9c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training_args\n",
    "args = TrainingArguments(\n",
    "    output_dir=save_location,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    eval_steps=steps,\n",
    "    save_steps=steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    #learning_rate=lr,\n",
    "    lr_scheduler_type='constant',\n",
    "    optim='paged_adamw_32bit',\n",
    "    bf16=True,\n",
    ")\n",
    "# We have to use bf16 in the args above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31c4c367-cb19-4d4f-bae5-32f86cf19f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e68b783c-08bc-4d76-bc88-9ad6669f777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    # model=model,\n",
    "    model=lora_model_noprep,\n",
    "    # model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=process_ds['train'],\n",
    "    eval_dataset=process_ds['test'],\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f7bff-e641-403e-a95c-0491058afe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model w/o quantisation will throw OOM Error with 11.9GB VRAM Usage\n",
    "# when using lora_unprep_8bit model with bfloat16, throws Attempting to Unscale FP16 grad\n",
    "# when using lora_prep_4bit model, throws OOM error\n",
    "# using the model quantised with 4-bit and compute_type of bf_16 it\n",
    "# started training.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd44e46-29f2-46d1-9e80-f1650e56ddab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b399d-38ea-49f0-9a73-32988c09339f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac6cc5-1903-4d29-867c-e32831bd4424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
