{
  "text_classification": {
    "architectures": [
      "ALBERT",
      "BART",
      "BERT",
      "BigBird",
      "BigBird-Pegasus",
      "BioGpt",
      "BLOOM",
      "CamemBERT",
      "CANINE",
      "CodeLlama",
      "ConvBERT",
      "CTRL",
      "Data2VecText",
      "DeBERTa",
      "DeBERTa-v2",
      "DistilBERT",
      "ELECTRA",
      "ERNIE",
      "ErnieM",
      "ESM",
      "Falcon",
      "FlauBERT",
      "FNet",
      "Funnel Transformer",
      "GPT-Sw3",
      "OpenAI GPT-2",
      "GPTBigCode",
      "GPT Neo",
      "GPT NeoX",
      "GPT-J",
      "I-BERT",
      "LayoutLM",
      "LayoutLMv2",
      "LayoutLMv3",
      "LED",
      "LiLT",
      "LLaMA",
      "Longformer",
      "LUKE",
      "MarkupLM",
      "mBART",
      "MEGA",
      "Megatron-BERT",
      "Mistral",
      "Mixtral",
      "MobileBERT",
      "MPNet",
      "MPT",
      "MRA",
      "MT5",
      "MVP",
      "Nezha",
      "Nystr\u00f6mformer",
      "OpenLlama",
      "OpenAI GPT",
      "OPT",
      "Perceiver",
      "Persimmon",
      "Phi",
      "PLBart",
      "QDQBert",
      "Qwen2",
      "Reformer",
      "RemBERT",
      "RoBERTa",
      "RoBERTa-PreLayerNorm",
      "RoCBert",
      "RoFormer",
      "SqueezeBERT",
      "T5",
      "TAPAS",
      "Transformer-XL",
      "UMT5",
      "XLM",
      "XLM-RoBERTa",
      "XLM-RoBERTa-XL",
      "XLNet",
      "X-MOD",
      "YOSO"
    ],
    "AutoModelClass": "AutoModelForSequenceClassification",
    "dataset": "imdb",
    "model_used": "distilbert-base-uncased"
  },
  "token_classification": {
    "architectures": [
      "ALBERT",
      "BERT",
      "BigBird",
      "BioGpt",
      "BLOOM",
      "BROS",
      "CamemBERT",
      "CANINE",
      "ConvBERT",
      "Data2VecText",
      "DeBERTa",
      "DeBERTa-v2",
      "DistilBERT",
      "ELECTRA",
      "ERNIE",
      "ErnieM",
      "ESM",
      "Falcon",
      "FlauBERT",
      "FNet",
      "Funnel Transformer",
      "GPT-Sw3",
      "OpenAI GPT-2",
      "GPTBigCode",
      "GPT Neo",
      "GPT NeoX",
      "I-BERT",
      "LayoutLM",
      "LayoutLMv2",
      "LayoutLMv3",
      "LiLT",
      "Longformer",
      "LUKE",
      "MarkupLM",
      "MEGA",
      "Megatron-BERT",
      "MobileBERT",
      "MPNet",
      "MPT",
      "MRA",
      "MT5",
      "Nezha",
      "Nystr\u00f6mformer",
      "Phi",
      "QDQBert",
      "RemBERT",
      "RoBERTa",
      "RoBERTa-PreLayerNorm",
      "RoCBert",
      "RoFormer",
      "SqueezeBERT",
      "T5",
      "UMT5",
      "XLM",
      "XLM-RoBERTa",
      "XLM-RoBERTa-XL",
      "XLNet",
      "X-MOD",
      "YOSO"
    ],
    "AutoModelClass": "AutoModelForTokenClassification",
    "dataset": "wnut_17",
    "model_used": "distilbert-base-uncased"
  },
  "question_answering": {
    "architectures": [
      "ALBERT",
      "BART",
      "BERT",
      "BigBird",
      "BigBird-Pegasus",
      "BLOOM",
      "CamemBERT",
      "CANINE",
      "ConvBERT",
      "Data2VecText",
      "DeBERTa",
      "DeBERTa-v2",
      "DistilBERT",
      "ELECTRA",
      "ERNIE",
      "ErnieM",
      "Falcon",
      "FlauBERT",
      "FNet",
      "Funnel Transformer",
      "OpenAI GPT-2",
      "GPT Neo",
      "GPT NeoX",
      "GPT-J",
      "I-BERT",
      "LayoutLMv2",
      "LayoutLMv3",
      "LED",
      "LiLT",
      "LLaMA",
      "Longformer",
      "LUKE",
      "LXMERT",
      "MarkupLM",
      "mBART",
      "MEGA",
      "Megatron-BERT",
      "MobileBERT",
      "MPNet",
      "MPT",
      "MRA",
      "MT5",
      "MVP",
      "Nezha",
      "Nystr\u00f6mformer",
      "OPT",
      "QDQBert",
      "Reformer",
      "RemBERT",
      "RoBERTa",
      "RoBERTa-PreLayerNorm",
      "RoCBert",
      "RoFormer",
      "Splinter",
      "SqueezeBERT",
      "T5",
      "UMT5",
      "XLM",
      "XLM-RoBERTa",
      "XLM-RoBERTa-XL",
      "XLNet",
      "X-MOD",
      "YOSO"
    ],
    "AutoModelClass": "AutoModelForQuestionAnswering",
    "dataset": "squad",
    "model_used": "distilbert-base-uncased"
  },
  "causal_lm": {
    "architectures": [
      "BART",
      "BERT",
      "Bert Generation",
      "BigBird",
      "BigBird-Pegasus",
      "BioGpt",
      "Blenderbot",
      "BlenderbotSmall",
      "BLOOM",
      "CamemBERT",
      "CodeLlama",
      "CodeGen",
      "CPM-Ant",
      "CTRL",
      "Data2VecText",
      "ELECTRA",
      "ERNIE",
      "Falcon",
      "Fuyu",
      "GIT",
      "GPT-Sw3",
      "OpenAI GPT-2",
      "GPTBigCode",
      "GPT Neo",
      "GPT NeoX",
      "GPT NeoX Japanese",
      "GPT-J",
      "LLaMA",
      "Marian",
      "mBART",
      "MEGA",
      "Megatron-BERT",
      "Mistral",
      "Mixtral",
      "MPT",
      "MusicGen",
      "MVP",
      "OpenLlama",
      "OpenAI GPT",
      "OPT",
      "Pegasus",
      "Persimmon",
      "Phi",
      "PLBart",
      "ProphetNet",
      "QDQBert",
      "Qwen2",
      "Reformer",
      "RemBERT",
      "RoBERTa",
      "RoBERTa-PreLayerNorm",
      "RoCBert",
      "RoFormer",
      "RWKV",
      "Speech2Text2",
      "Transformer-XL",
      "TrOCR",
      "Whisper",
      "XGLM",
      "XLM",
      "XLM-ProphetNet",
      "XLM-RoBERTa",
      "XLM-RoBERTa-XL",
      "XLNet",
      "X-MOD"
    ],
    "AutoModelClass": "AutoModelForCausalLM",
    "dataset": "eli5_category",
    "model_used": "distilgpt2"
  },
  "masked_lm": {
    "architectures": [
      "ALBERT",
      "BART",
      "BERT",
      "BigBird",
      "CamemBERT",
      "ConvBERT",
      "Data2VecText",
      "DeBERTa",
      "DeBERTa-v2",
      "DistilBERT",
      "ELECTRA",
      "ERNIE",
      "ESM",
      "FlauBERT",
      "FNet",
      "Funnel Transformer",
      "I-BERT",
      "LayoutLM",
      "Longformer",
      "LUKE",
      "mBART",
      "MEGA",
      "Megatron-BERT",
      "MobileBERT",
      "MPNet",
      "MRA",
      "MVP",
      "Nezha",
      "Nystr\u00f6mformer",
      "Perceiver",
      "QDQBert",
      "Reformer",
      "RemBERT",
      "RoBERTa",
      "RoBERTa-PreLayerNorm",
      "RoCBert",
      "RoFormer",
      "SqueezeBERT",
      "TAPAS",
      "Wav2Vec2",
      "XLM",
      "XLM-RoBERTa",
      "XLM-RoBERTa-XL",
      "X-MOD",
      "YOSO"
    ],
    "AutoModelClass": "AutoModelForMaskedLM",
    "dataset": "eli-5",
    "model_used": "distilroberta-base"
  },
  "translation": {
    "architectures": [
      "BART",
      "BigBird-Pegasus",
      "Blenderbot",
      "BlenderbotSmall",
      "Encoder decoder",
      "FairSeq Machine-Translation",
      "GPTSAN-japanese",
      "LED",
      "LongT5",
      "M2M100",
      "Marian",
      "mBART",
      "MT5",
      "MVP",
      "NLLB",
      "NLLB-MOE",
      "Pegasus",
      "PEGASUS-X",
      "PLBart",
      "ProphetNet",
      "SeamlessM4T",
      "SeamlessM4Tv2",
      "SwitchTransformers",
      "T5",
      "UMT5",
      "XLM-ProphetNet"
    ],
    "AutoModelClass": "AutoModelForSeq2SeqLM",
    "dataset": "opus_books",
    "model_used": "t5-small"
  },
  "summarization": {
    "architectures": [
      "BART",
      "BigBird-Pegasus",
      "Blenderbot",
      "BlenderbotSmall",
      "Encoder decoder",
      "FairSeq Machine-Translation",
      "GPTSAN-japanese",
      "LED",
      "LongT5",
      "M2M100",
      "Marian",
      "mBART",
      "MT5",
      "MVP",
      "NLLB",
      "NLLB-MOE",
      "Pegasus",
      "PEGASUS-X",
      "PLBart",
      "ProphetNet",
      "SeamlessM4T",
      "SeamlessM4Tv2",
      "SwitchTransformers",
      "T5",
      "UMT5",
      "XLM-ProphetNet"
    ],
    "AutoModelClass": "AutoModelForSeq2SeqLM",
    "dataset": "billsum",
    "model_used": "t5-small"
  },
  "multiple_choice": {
    "architectures": [
      "ALBERT",
      "BERT",
      "BigBird",
      "CamemBERT",
      "CANINE",
      "ConvBERT",
      "Data2VecText",
      "DeBERTa-v2",
      "DistilBERT",
      "ELECTRA",
      "ERNIE",
      "ErnieM",
      "FlauBERT",
      "FNet",
      "Funnel Transformer",
      "I-BERT",
      "Longformer",
      "LUKE",
      "MEGA",
      "Megatron-BERT",
      "MobileBERT",
      "MPNet",
      "MRA",
      "Nezha",
      "Nystr\u00f6mformer",
      "QDQBert",
      "RemBERT",
      "RoBERTa",
      "RoBERTa-PreLayerNorm",
      "RoCBert",
      "RoFormer",
      "SqueezeBERT",
      "XLM",
      "XLM-RoBERTa",
      "XLM-RoBERTa-XL",
      "XLNet",
      "X-MOD",
      "YOSO"
    ],
    "AutoModelClass": "AutoModelForMultipleChoice",
    "dataset": "swag",
    "model_used": "bert-base-uncased"
  }
}
